import asyncio
from sqlalchemy import text
from app.core.database import AsyncSessionLocal, engine, Base

# [ì¤‘ìš”] í…Œì´ë¸” ìƒì„±ì„ ìœ„í•´ ëª¨ë¸ì„ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.
# ì‚¬ìš©ìê°€ ì•Œë ¤ì¤€ íŒŒì¼ëª…(regulation_model.py)ì— ë§ê²Œ ì„í¬íŠ¸
import app.core.models.regulation_model 

from app.crawler.usa_fda import USAFDACrawler
from app.services.regulation_service import RegulationService

async def main():
    print("ğŸ› ï¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

    # 1. DB í…Œì´ë¸” ìƒì„± (í…Œì´ë¸”ì´ ì—†ì„ ê²½ìš° ìë™ ìƒì„±)
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    # 2. ê¸°ì´ˆ ë°ì´í„°(Seed Data) ìƒì„± - FK ì—ëŸ¬ ë°©ì§€ìš©
    # ê·œì œë¥¼ ì €ì¥í•˜ë ¤ë©´ 'êµ­ê°€ ì½”ë“œ(US)'ì™€ 'ë°ì´í„° ì†ŒìŠ¤ ID(1)'ê°€ ë¯¸ë¦¬ DBì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    async with AsyncSessionLocal() as db:
        try:
            # 2-1. êµ­ê°€ ì½”ë“œ 'US' í™•ì¸ ë° ìƒì„±
            result = await db.execute(text("SELECT 1 FROM countries WHERE country_code = 'US'"))
            if not result.scalar():
                print("âš™ï¸ ê¸°ì´ˆ ë°ì´í„° ìƒì„±: Country (US)")
                await db.execute(text("INSERT INTO countries (country_code, country_name) VALUES ('US', 'United States')"))

            # 2-2. ë°ì´í„° ì†ŒìŠ¤ 'ID=1' í™•ì¸ ë° ìƒì„±
            result = await db.execute(text("SELECT 1 FROM data_sources WHERE source_id = 1"))
            if not result.scalar():
                print("âš™ï¸ ê¸°ì´ˆ ë°ì´í„° ìƒì„±: DataSource (ID=1)")
                await db.execute(text("INSERT INTO data_sources (source_id, source_name, url, source_type) VALUES (1, 'US FDA', 'https://www.fda.gov', 'html')"))
            
            await db.commit()
        except Exception as e:
            print(f"âš ï¸ ê¸°ì´ˆ ë°ì´í„° ìƒì„± ì¤‘ ê²½ê³ : {e}")
            await db.rollback()

    # 3. í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ë° ì‹¤í–‰
    crawler = USAFDACrawler()
    print("ğŸš€ í¬ë¡¤ë§ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    try:
        # ë°ì´í„° ìˆ˜ì§‘ (ë©”íƒ€ë°ì´í„°ë§Œ ë¨¼ì € ê°€ì ¸ì˜´)
        data_list = await crawler.run()
        print(f"ğŸ“¦ ìˆ˜ì§‘ëœ ë©”íƒ€ ë°ì´í„°: {len(data_list)}ê±´")

        if not data_list:
            print("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        # 4. ì„œë¹„ìŠ¤ ë ˆì´ì–´ ì‹¤í–‰ (DB ì €ì¥ + íŒŒì¼ ë‹¤ìš´ë¡œë“œ + ì „ì²˜ë¦¬)
        async with AsyncSessionLocal() as db:
            service = RegulationService(db)
            
            print("ğŸ’¾ [DB ì €ì¥] ë° [íŒŒì¼ ë‹¤ìš´ë¡œë“œ] íŒŒì´í”„ë¼ì¸ ê°€ë™...")
            
            success_count = 0
            skip_count = 0
            error_count = 0

            for data in data_list:
                try:
                    # [í•µì‹¬] crawler ì¸ìŠ¤í„´ìŠ¤ë¥¼ í•¨ê»˜ ë„˜ê²¨ì„œ, í•„ìš” ì‹œ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê²Œ í•¨
                    result = await service.process_crawled_data(data, crawler)
                    
                    if result == "skipped":
                        skip_count += 1
                    else:
                        success_count += 1
                        
                except Exception as e:
                    print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨ ({data.get('title')}): {e}")
                    error_count += 1
                    await db.rollback() # ì—ëŸ¬ ë‚œ íŠ¸ëœì­ì…˜ë§Œ ë¡¤ë°±í•˜ê³  ê³„ì† ì§„í–‰

            print("\n" + "="*40)
            print(f"âœ… ì‘ì—… ì™„ë£Œ ë¦¬í¬íŠ¸")
            print(f"âœ¨ ì²˜ë¦¬ ì„±ê³µ(ì‹ ê·œ/ì—…ë°ì´íŠ¸): {success_count}ê±´")
            print(f"â­ï¸ ë³€ê²½ ì—†ìŒ(Skip): {skip_count}ê±´")
            print(f"âŒ ì—ëŸ¬: {error_count}ê±´")
            print("="*40)

    except Exception as e:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        await crawler.close()
        await engine.dispose()

if __name__ == "__main__":
    asyncio.run(main())