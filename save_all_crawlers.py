import asyncio
import yaml
import os
import sys
from sqlalchemy import text
from dotenv import load_dotenv  # [ì¶”ê°€] .env ë¡œë“œìš©

from app.core.database import AsyncSessionLocal, engine, Base
from app.crawler.discovery_agent import DiscoveryAgent

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼)
load_dotenv()

def load_config():
    """
    ì„¤ì • íŒŒì¼ ë¡œë“œ
    ìš°ì„ ìˆœìœ„: app/config/config.yaml -> (ì—†ìœ¼ë©´) í˜„ì¬ í´ë”ì˜ config.yaml
    """
    base_dir = os.getcwd() # í˜„ì¬ ì‹¤í–‰ ê²½ë¡œ
    
    # 1ìˆœìœ„: app/config/config.yaml (ê¶Œì¥)
    config_path = os.path.join(base_dir, "app", "config", "config.yaml")
    
    if not os.path.exists(config_path):
        print(f"âš ï¸ 'app/config/config.yaml'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        # ë¹„ìƒìš©: ë£¨íŠ¸ì— ìˆëŠ” ê²½ìš° ì²´í¬
        if os.path.exists("config.yaml"):
             config_path = "config.yaml"
        else:
            return {"targets": []}
    
    print(f"âš™ï¸ ì„¤ì • íŒŒì¼ ë¡œë“œ: {config_path}")
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

async def init_seed_data(db, targets):
    """ê¸°ì´ˆ ë°ì´í„°(êµ­ê°€, ì†ŒìŠ¤) ë™ê¸°í™”"""
    print("âš™ï¸ ê¸°ì´ˆ ë°ì´í„°(Seed) ë™ê¸°í™” ì¤‘...")
    try:
        # 1. êµ­ê°€ ì½”ë“œ ìë™ ë“±ë¡
        for target in targets:
            code = target.get("code")
            name = target.get("country")
            if code and name:
                exists = (await db.execute(text(f"SELECT 1 FROM countries WHERE country_code = '{code}'"))).scalar()
                if not exists:
                    print(f" ğŸ³ï¸ ì‹ ê·œ êµ­ê°€ ë“±ë¡: {name} ({code})")
                    await db.execute(text(f"INSERT INTO countries (country_code, country_name) VALUES ('{code}', '{name}')"))
        
        # 2. Discovery Agentìš© ì†ŒìŠ¤ ë“±ë¡ (ID 99)
        if not (await db.execute(text("SELECT 1 FROM data_sources WHERE source_id = 99"))).scalar():
             await db.execute(text("INSERT INTO data_sources (source_id, source_name, url, source_type) VALUES (99, 'Tavily Discovery', 'https://tavily.com', 'ai_search')"))

        await db.commit()
        print("âœ… ê¸°ì´ˆ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ ê¸°ì´ˆ ë°ì´í„° ë™ê¸°í™” ì‹¤íŒ¨: {e}")
        await db.rollback()

async def main():
    # 1. DB ì´ˆê¸°í™”
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    # 2. Config ë° API Key ë¡œë“œ
    config = load_config()
    targets = config.get("targets", [])
    
    # [ìˆ˜ì •] .envì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    tavily_key = os.getenv("TAVILY_API_KEY")

    if not tavily_key:
        print("ğŸš¨ ì˜¤ë¥˜: .env íŒŒì¼ì— 'TAVILY_API_KEY'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   -> .env íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return # í‚¤ê°€ ì—†ìœ¼ë©´ ì‹¤í–‰ ì¤‘ë‹¨ (Mock ëª¨ë“œ ì›í•˜ë©´ ì´ ì¤„ ì£¼ì„ ì²˜ë¦¬)

    # 3. Seed ë°ì´í„° ì¤€ë¹„
    async with AsyncSessionLocal() as db:
        await init_seed_data(db, targets)

    print("\n" + "="*60)
    print("ğŸŒ [Global Regulation Monitor] Tavily ê¸°ë°˜ ê°ì‹œ ì‹œì‘")
    print(f"ğŸ¯ ê°ì‹œ ëŒ€ìƒ: {len([t for t in targets if t['enabled']])}ê°œêµ­")
    print("="*60)

    # 4. ì—ì´ì „íŠ¸ ì‹¤í–‰
    async with AsyncSessionLocal() as db_session:
        # Agent ì´ˆê¸°í™” (DBì„¸ì…˜ê³¼ APIí‚¤ ì£¼ì…)
        agent = DiscoveryAgent(db_session, tavily_api_key=tavily_key)

        for target in targets:
            if not target.get("enabled", False):
                continue
            
            country = target["country"]
            keywords = target["keywords"]
             # [ì¶”ê°€] config.yamlì—ì„œ category ì½ê¸° (ê¸°ë³¸ê°’ regulation)
            category = target.get("category", "regulation")

            print(f"\nğŸ“¡ [{country}] íƒìƒ‰ ì‹œì‘ ({category})")
            
            # run í•¨ìˆ˜ì— category ì „ë‹¬
            await agent.run(country, keywords, category=category)
            
            await asyncio.sleep(2)

            # print(f"\nğŸ“¡ [{country}] ê·œì œ íƒìƒ‰ ì‹œì‘ (Keywords: {len(keywords)}ê°œ)")
            
            # # Agentê°€ ì•Œì•„ì„œ ê²€ìƒ‰ -> íŒë‹¨ -> ë‹¤ìš´ë¡œë“œ -> ì €ì¥ ìˆ˜í–‰
            # await agent.run(country, keywords)
            
            # # API í˜¸ì¶œ ì†ë„ ì¡°ì ˆ (ë¬´ë£Œ í‹°ì–´ ë³´í˜¸)
            # await asyncio.sleep(2)

    print("\n" + "="*60)
    print("ğŸ‰ ëª¨ë“  ëª¨ë‹ˆí„°ë§ ì‘ì—… ì™„ë£Œ!")
    print("="*60)
    await engine.dispose()

if __name__ == "__main__":
    # ì‹¤í–‰ ì‹œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ pathì— ì¶”ê°€
    sys.path.append(os.getcwd())
    asyncio.run(main())

# import asyncio
# from sqlalchemy import text
# from app.core.database import AsyncSessionLocal, engine, Base
# import app.core.models.regulation_model 

# # [Import] í¬ë¡¤ëŸ¬ë“¤
# from app.crawler.crawling_regulation.usa_fda import USAFDACrawler
# from app.crawler.crawling_regulation.california_law import CaliforniaLawCrawler
# from app.crawler.crawling_regulation.sf_bos_selenium import SFBOSSeleniumCrawler
# from app.crawler.crawling_regulation.ecfr_api import ECFRAPICrawler
# from app.crawler.crawling_regulation.russia_eec import RussiaEECCrawler  # [ì‹ ê·œ ì¶”ê°€]

# from app.core.repositories.crawl_repository import CrawlRepository

# async def init_seed_data(db):
#     print("âš™ï¸ ê¸°ì´ˆ ë°ì´í„°(Seed) ì ê²€ ì¤‘...")
#     try:
#         # êµ­ê°€ ì½”ë“œ ì¶”ê°€ (RU)
#         for code, name in [('US', 'United States'), ('RU', 'Russia')]:
#             if not (await db.execute(text(f"SELECT 1 FROM countries WHERE country_code = '{code}'"))).scalar():
#                 await db.execute(text(f"INSERT INTO countries (country_code, country_name) VALUES ('{code}', '{name}')"))
        
#         # ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€ (ID 5: Russia EEC)
#         sources = [
#             (1, 'US FDA', 'https://www.fda.gov', 'html'),
#             (2, 'CA Legislature', 'https://leginfo.legislature.ca.gov', 'html'),
#             (3, 'San Francisco BOS', 'https://sfbos.org', 'html'),
#             (4, 'eCFR API', 'https://www.ecfr.gov', 'api'),
#             (5, 'Eurasian Economic Commission', 'https://eec.eaeunion.org', 'html') # [ì‹ ê·œ]
#         ]
        
#         for s_id, s_name, s_url, s_type in sources:
#             if not (await db.execute(text(f"SELECT 1 FROM data_sources WHERE source_id = {s_id}"))).scalar():
#                 print(f"   + ì†ŒìŠ¤ ì¶”ê°€: {s_name}")
#                 await db.execute(text(f"INSERT INTO data_sources (source_id, source_name, url, source_type) VALUES ({s_id}, '{s_name}', '{s_url}', '{s_type}')"))
        
#         await db.commit()
#         print("âœ… ê¸°ì´ˆ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
#     except Exception as e:
#         print(f"âš ï¸ ê¸°ì´ˆ ë°ì´í„° ìƒì„± ì¤‘ ê²½ê³ : {e}")
#         await db.rollback()

# async def run_single_crawler(crawler_instance, source_name):
#     print(f"\nğŸš€ [{source_name}] í¬ë¡¤ë§ ì‹œì‘...")
#     crawler = crawler_instance
#     try:
#         data_list = await crawler.run()
#         print(f"ğŸ“¦ [{source_name}] ìˆ˜ì§‘ëœ ë°ì´í„°: {len(data_list)}ê±´")

#         if not data_list:
#             print(f"âš ï¸ [{source_name}] ë°ì´í„° ì—†ìŒ. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
#             return

#         async with AsyncSessionLocal() as db:
#             service = CrawlRepository(db)
#             success, skipped, errors = 0, 0, 0

#             print(f"ğŸ’¾ [{source_name}] ì €ì¥ ë° ë¶„ì„ ì¤‘...")
#             for data in data_list:
#                 try:
#                     # [eCFR ì˜ˆì™¸ ì²˜ë¦¬] eCFRì€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œ 404 ì´ìŠˆê°€ ìˆìœ¼ë¯€ë¡œ í¬ë¡¤ëŸ¬ ì¸ìë¥¼ ë„˜ê¸°ì§€ ì•ŠìŒ (ë©”íƒ€ë°ì´í„°ë§Œ ì €ì¥)
#                     if "eCFR" in source_name:
#                         result = await service.process_crawled_data(data, crawler=None) # íŒŒì¼ ë‹¤ìš´ë¡œë“œ Skip
#                     else:
#                         result = await service.process_crawled_data(data, crawler) # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì§„í–‰
                    
#                     if result == "skipped": skipped += 1
#                     else: success += 1
#                 except Exception as e:
#                     print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
#                     errors += 1
#                     await db.rollback()
            
#             print(f"ğŸ“Š [{source_name}] ê²°ê³¼: ì„±ê³µ {success} / ìŠ¤í‚µ {skipped} / ì—ëŸ¬ {errors}")

#     except Exception as e:
#         print(f"âŒ [{source_name}] í¬ë¡¤ëŸ¬ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
#     finally:
#         await crawler.close()

# async def main():
#     async with engine.begin() as conn:
#         await conn.run_sync(Base.metadata.create_all)

#     async with AsyncSessionLocal() as db:
#         await init_seed_data(db)

#     # [ì‹¤í–‰ ëª©ë¡]
#     crawlers_to_run = [
#         # (USAFDACrawler(), "US FDA"),
#         # (CaliforniaLawCrawler(), "California Law"),
#         # (SFBOSSeleniumCrawler(), "SF Board of Supervisors"),
#         # (ECFRAPICrawler(title_number="21", query="tobacco"), "eCFR Title 21 (FDA)"),
        
#         # [ì‹ ê·œ] ëŸ¬ì‹œì•„ í¬ë¡¤ëŸ¬ ë‹¨ë… í…ŒìŠ¤íŠ¸
#         (RussiaEECCrawler(), "Russia EAEU TR"),
#     ]

#     print("\n" + "="*50)
#     print("ğŸŒ ê¸€ë¡œë²Œ ê·œì œ í†µí•© ìˆ˜ì§‘ ì‹œì‘")
#     print("="*50)

#     for crawler_instance, name in crawlers_to_run:
#         await run_single_crawler(crawler_instance, name)
#         await asyncio.sleep(2)

#     print("\n" + "="*50)
#     print("ğŸ‰ ëª¨ë“  í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ!")
#     print("="*50)
#     await engine.dispose()

# if __name__ == "__main__":
#     asyncio.run(main())


# import asyncio
# from sqlalchemy import text
# from app.core.database import AsyncSessionLocal, engine, Base
# import app.core.models.regulation_model 

# # [1] ëª¨ë“  í¬ë¡¤ëŸ¬ ì„í¬íŠ¸
# from app.crawler.crawling_regulation.usa_fda import USAFDACrawler
# from app.crawler.crawling_regulation.california_law import CaliforniaLawCrawler
# from app.crawler.crawling_regulation.sf_bos_selenium import SFBOSSeleniumCrawler # Selenium ë²„ì „ ì‚¬ìš©
# from app.crawler.crawling_regulation.ecfr_api import ECFRAPICrawler # [ì¶”ê°€]

# from app.core.repositories.crawl_repository import CrawlRepository

# async def init_seed_data(db):
#     """êµ­ê°€ ë° ë°ì´í„° ì†ŒìŠ¤ ê¸°ì´ˆ ë°ì´í„° ìƒì„±"""
#     print("âš™ï¸ ê¸°ì´ˆ ë°ì´í„°(Seed) ì ê²€ ì¤‘...")
#     try:
#         # 1. êµ­ê°€ ì½”ë“œ (US)
#         if not (await db.execute(text("SELECT 1 FROM countries WHERE country_code = 'US'"))).scalar():
#             await db.execute(text("INSERT INTO countries (country_code, country_name) VALUES ('US', 'United States')"))
        
#         # 2. ë°ì´í„° ì†ŒìŠ¤ (ID 1: FDA, 2: CA, 3: SF, 4: eCFR)
#         sources = [
#             (1, 'US FDA', 'https://www.fda.gov', 'html'),
#             (2, 'CA Legislature', 'https://leginfo.legislature.ca.gov', 'html'),
#             (3, 'San Francisco BOS', 'https://sfbos.org', 'html'),
#             (4, 'eCFR API', 'https://www.ecfr.gov', 'api') # ID 4ë²ˆ ì¶”ê°€
#         ]
        
#         for s_id, s_name, s_url, s_type in sources:
#             if not (await db.execute(text(f"SELECT 1 FROM data_sources WHERE source_id = {s_id}"))).scalar():
#                 print(f"   + ì†ŒìŠ¤ ì¶”ê°€: {s_name}")
#                 await db.execute(text(f"INSERT INTO data_sources (source_id, source_name, url, source_type) VALUES ({s_id}, '{s_name}', '{s_url}', '{s_type}')"))
        
#         await db.commit()
#         print("âœ… ê¸°ì´ˆ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
#     except Exception as e:
#         print(f"âš ï¸ ê¸°ì´ˆ ë°ì´í„° ìƒì„± ì¤‘ ê²½ê³ : {e}")
#         await db.rollback()

# # [ìˆ˜ì •ë¨] crawler_clsê°€ ì•„ë‹ˆë¼ crawler_instanceë¥¼ ë°›ë„ë¡ ë³€ê²½
# async def run_single_crawler(crawler_instance, source_name):
#     """ë‹¨ì¼ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ë° ì €ì¥ ë¡œì§"""
#     print(f"\nğŸš€ [{source_name}] í¬ë¡¤ë§ ì‹œì‘...")
    
#     # [ìˆ˜ì •ë¨] ì´ë¯¸ ë°–ì—ì„œ ìƒì„±ëœ ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
#     crawler = crawler_instance 
    
#     try:
#         # 1. ë°ì´í„° ìˆ˜ì§‘
#         data_list = await crawler.run()
#         print(f"ğŸ“¦ [{source_name}] ìˆ˜ì§‘ëœ ë°ì´í„°: {len(data_list)}ê±´")

#         if not data_list:
#             print(f"âš ï¸ [{source_name}] ë°ì´í„° ì—†ìŒ. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
#             return

#         # 2. DB ì €ì¥ ë° ì²˜ë¦¬
#         async with AsyncSessionLocal() as db:
#             service = CrawlRepository(db)
            
#             success = 0
#             skipped = 0
#             errors = 0

#             print(f"ğŸ’¾ [{source_name}] ì €ì¥ ë° ë¶„ì„ ì¤‘...")
#             for data in data_list:
#                 try:
#                     result = await service.process_crawled_data(data, crawler)
#                     if result == "skipped": skipped += 1
#                     else: success += 1
#                 except Exception as e:
#                     print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
#                     errors += 1
#                     await db.rollback()
            
#             print(f"ğŸ“Š [{source_name}] ê²°ê³¼: ì„±ê³µ {success} / ìŠ¤í‚µ {skipped} / ì—ëŸ¬ {errors}")

#     except Exception as e:
#         print(f"âŒ [{source_name}] í¬ë¡¤ëŸ¬ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
#     finally:
#         # [ì¤‘ìš”] ì„¸ì…˜ ì¢…ë£ŒëŠ” ì—¬ê¸°ì„œ ìˆ˜í–‰
#         await crawler.close()

# async def main():
#     # 1. í…Œì´ë¸” ìƒì„±
#     async with engine.begin() as conn:
#         await conn.run_sync(Base.metadata.create_all)

#     # 2. ê¸°ì´ˆ ë°ì´í„° ì´ˆê¸°í™”
#     async with AsyncSessionLocal() as db:
#         await init_seed_data(db)

#     # [3] ì‹¤í–‰í•  í¬ë¡¤ëŸ¬ ëª©ë¡ ì •ì˜ (ëª¨ë‘ ì¸ìŠ¤í„´ìŠ¤ë¡œ ìƒì„±í•´ì„œ ë„£ìŒ)
#     crawlers_to_run = [
#         (USAFDACrawler(), "US FDA"), # ê´„í˜¸ () ì¶”ê°€!
#         (CaliforniaLawCrawler(), "California Law"), # ê´„í˜¸ () ì¶”ê°€!
#         (SFBOSSeleniumCrawler(), "SF Board of Supervisors"), # ê´„í˜¸ () ì¶”ê°€!
#         # eCFRì€ íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•˜ë¯€ë¡œ ì´ë¯¸ ì¸ìŠ¤í„´ìŠ¤ ìƒíƒœì„
#         (ECFRAPICrawler(title_number="21", query="tobacco"), "eCFR Title 21 (FDA)"),
#         (ECFRAPICrawler(title_number="27", query="tobacco"), "eCFR Title 27 (ATF)"),
#     ]

#     # 4. ìˆœì°¨ ì‹¤í–‰
#     print("\n" + "="*50)
#     print("ğŸŒ ê¸€ë¡œë²Œ ê·œì œ í†µí•© ìˆ˜ì§‘ ì‹œì‘")
#     print("="*50)

#     for crawler_instance, name in crawlers_to_run:
#         await run_single_crawler(crawler_instance, name)
#         # ë‹¤ìŒ ì‚¬ì´íŠ¸ ë„˜ì–´ê°€ê¸° ì „ ì ê¹ ëŒ€ê¸°
#         await asyncio.sleep(2) 

#     print("\n" + "="*50)
#     print("ğŸ‰ ëª¨ë“  í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ!")
#     print("="*50)
#     await engine.dispose()

# if __name__ == "__main__":
#     asyncio.run(main())



# import asyncio
# from sqlalchemy import text
# from app.core.database import AsyncSessionLocal, engine, Base
# import app.core.models.regulation_model 

# # [1] ëª¨ë“  í¬ë¡¤ëŸ¬ ì„í¬íŠ¸
# from app.crawler.crawling_regulation.usa_fda import USAFDACrawler
# from app.crawler.crawling_regulation.california_law import CaliforniaLawCrawler
# from app.crawler.crawling_regulation.sf_bos_selenium import SFBOSSeleniumCrawler # Selenium ë²„ì „ ì‚¬ìš©
# from app.crawler.crawling_regulation.ecfr_api import ECFRAPICrawler # [ì¶”ê°€]


# from app.core.repositories.crawl_repository import CrawlRepository

# async def init_seed_data(db):
#     """êµ­ê°€ ë° ë°ì´í„° ì†ŒìŠ¤ ê¸°ì´ˆ ë°ì´í„° ìƒì„±"""
#     print("âš™ï¸ ê¸°ì´ˆ ë°ì´í„°(Seed) ì ê²€ ì¤‘...")
#     try:
#         # 1. êµ­ê°€ ì½”ë“œ (US)
#         if not (await db.execute(text("SELECT 1 FROM countries WHERE country_code = 'US'"))).scalar():
#             await db.execute(text("INSERT INTO countries (country_code, country_name) VALUES ('US', 'United States')"))
        
#         # 2. ë°ì´í„° ì†ŒìŠ¤ (ID 1: FDA, 2: CA, 3: SF)
#         sources = [
#             (1, 'US FDA', 'https://www.fda.gov', 'html'),
#             (2, 'CA Legislature', 'https://leginfo.legislature.ca.gov', 'html'),
#             (3, 'San Francisco BOS', 'https://sfbos.org', 'html'),
#             # 4. eCFR API ì†ŒìŠ¤ ì¶”ê°€
#             (4, 'eCFR API (Title 21)', 'https://www.ecfr.gov', 'api')
#         ]
        
#         for s_id, s_name, s_url, s_type in sources:
#             if not (await db.execute(text(f"SELECT 1 FROM data_sources WHERE source_id = {s_id}"))).scalar():
#                 print(f"   + ì†ŒìŠ¤ ì¶”ê°€: {s_name}")
#                 await db.execute(text(f"INSERT INTO data_sources (source_id, source_name, url, source_type) VALUES ({s_id}, '{s_name}', '{s_url}', '{s_type}')"))
        
#         await db.commit()
#         print("âœ… ê¸°ì´ˆ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
#     except Exception as e:
#         print(f"âš ï¸ ê¸°ì´ˆ ë°ì´í„° ìƒì„± ì¤‘ ê²½ê³ : {e}")
#         await db.rollback()

# async def run_single_crawler(crawler_cls, source_name):
#     """ë‹¨ì¼ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ë° ì €ì¥ ë¡œì§"""
#     print(f"\nğŸš€ [{source_name}] í¬ë¡¤ë§ ì‹œì‘...")
    
#     crawler = crawler_cls() # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    
#     try:
#         # 1. ë°ì´í„° ìˆ˜ì§‘
#         data_list = await crawler.run()
#         print(f"ğŸ“¦ [{source_name}] ìˆ˜ì§‘ëœ ë°ì´í„°: {len(data_list)}ê±´")

#         if not data_list:
#             print(f"âš ï¸ [{source_name}] ë°ì´í„° ì—†ìŒ. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
#             return

#         # 2. DB ì €ì¥ ë° ì²˜ë¦¬
#         async with AsyncSessionLocal() as db:
#             service = CrawlRepository(db)
            
#             success = 0
#             skipped = 0
#             errors = 0

#             print(f"ğŸ’¾ [{source_name}] ì €ì¥ ë° ë¶„ì„ ì¤‘...")
#             for data in data_list:
#                 try:
#                     # CrawlRepositoryê°€ ì•Œì•„ì„œ urlë¡œ ì¤‘ë³µ ì²´í¬ ë° ë‹¤ìš´ë¡œë“œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
#                     result = await service.process_crawled_data(data, crawler)
#                     if result == "skipped": skipped += 1
#                     else: success += 1
#                 except Exception as e:
#                     print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
#                     errors += 1
#                     await db.rollback()
            
#             print(f"ğŸ“Š [{source_name}] ê²°ê³¼: ì„±ê³µ {success} / ìŠ¤í‚µ {skipped} / ì—ëŸ¬ {errors}")

#     except Exception as e:
#         print(f"âŒ [{source_name}] í¬ë¡¤ëŸ¬ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
#     finally:
#         await crawler.close()

# async def main():
#     # 1. í…Œì´ë¸” ìƒì„±
#     async with engine.begin() as conn:
#         await conn.run_sync(Base.metadata.create_all)

#     # 2. ê¸°ì´ˆ ë°ì´í„° ì´ˆê¸°í™”
#     async with AsyncSessionLocal() as db:
#         await init_seed_data(db)

#     # [3] ì‹¤í–‰í•  í¬ë¡¤ëŸ¬ ëª©ë¡ ì •ì˜
#     # (í´ë˜ìŠ¤ëª…, í‘œì‹œì´ë¦„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
#     crawlers_to_run = [
#         (USAFDACrawler, "US FDA"),
#         (CaliforniaLawCrawler, "California Law"),
#         (SFBOSSeleniumCrawler, "SF Board of Supervisors"),
#         # [ì‹ ê·œ] eCFR API í¬ë¡¤ëŸ¬ (Title 21 - FDA ê´€ë ¨)
#         (ECFRAPICrawler(title_number="21", query="tobacco"), "eCFR Title 21 (FDA)"),
        
#         # [ì‹ ê·œ] eCFR API í¬ë¡¤ëŸ¬ (Title 27 - ATF/TTB ê´€ë ¨)
#         (ECFRAPICrawler(title_number="27", query="tobacco"), "eCFR Title 27 (ATF)"),
#     ]

#     # 4. ìˆœì°¨ ì‹¤í–‰
#     print("\n" + "="*50)
#     print("ğŸŒ ê¸€ë¡œë²Œ ê·œì œ í†µí•© ìˆ˜ì§‘ ì‹œì‘")
#     print("="*50)

#     for crawler_cls, name in crawlers_to_run:
#         await run_single_crawler(crawler_cls, name)
#         # ë‹¤ìŒ ì‚¬ì´íŠ¸ ë„˜ì–´ê°€ê¸° ì „ ì ê¹ ëŒ€ê¸° (ì„ íƒì‚¬í•­)
#         await asyncio.sleep(2) 

#     print("\n" + "="*50)
#     print("ğŸ‰ ëª¨ë“  í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ!")
#     print("="*50)
#     await engine.dispose()

# if __name__ == "__main__":
#     asyncio.run(main())