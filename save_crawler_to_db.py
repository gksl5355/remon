# save_crawler_to_db.py

import asyncio
from sqlalchemy import text
from app.core.database import AsyncSessionLocal, engine, Base
import app.core.models.regulation_model 

# [ë³€ê²½] SFBOS í¬ë¡¤ëŸ¬ ì„í¬íŠ¸
from app.crawler.crawling_regulation.sf_bos_selenium import SFBOSSeleniumCrawler
from app.services.regulation_service import RegulationService

async def main():
    # 1. í…Œì´ë¸” ìƒì„±
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    # 2. ê¸°ì´ˆ ë°ì´í„° ìƒì„± (SF BOS ì¶”ê°€)
    async with AsyncSessionLocal() as db:
        try:
            # 2-1. êµ­ê°€ ì½”ë“œ 'US' í™•ì¸
            result = await db.execute(text("SELECT 1 FROM countries WHERE country_code = 'US'"))
            if not result.scalar():
                print("âš™ï¸ ê¸°ì´ˆ ë°ì´í„° ìƒì„±: Country (US)")
                await db.execute(text("INSERT INTO countries (country_code, country_name) VALUES ('US', 'United States')"))

            # 2-2. ë°ì´í„° ì†ŒìŠ¤ 'ID=3' (SF BOS) í™•ì¸ ë° ìƒì„±
            # (ID 1=FDA, 2=CA Law, 3=SF BOSë¡œ ê°€ì •)
            result = await db.execute(text("SELECT 1 FROM data_sources WHERE source_id = 3"))
            if not result.scalar():
                print("âš™ï¸ ê¸°ì´ˆ ë°ì´í„° ìƒì„±: DataSource (ID=3)")
                await db.execute(text("INSERT INTO data_sources (source_id, source_name, url, source_type) VALUES (3, 'San Francisco BOS', 'https://sfbos.org', 'html')"))
            
            await db.commit()
        except Exception as e:
            print(f"âš ï¸ ê¸°ì´ˆ ë°ì´í„° ìƒì„± ì¤‘ ê²½ê³ : {e}")
            await db.rollback()

    # 3. í¬ë¡¤ëŸ¬ ì‹¤í–‰ (SF BOS)
    crawler = SFBOSSeleniumCrawler() 
    print("ğŸš€ í¬ë¡¤ë§ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    try:
        data_list = await crawler.run()
        print(f"ğŸ“¦ ìˆ˜ì§‘ëœ ë©”íƒ€ ë°ì´í„°: {len(data_list)}ê±´")

        if not data_list:
            print("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (êµ¬ê¸€ ê²€ìƒ‰ ì—”ì§„ì¼ ê°€ëŠ¥ì„± ìˆìŒ)")
            return

        # 4. ì„œë¹„ìŠ¤ ì‹¤í–‰
        async with AsyncSessionLocal() as db:
            service = RegulationService(db)
            print("ğŸ’¾ [DB ì €ì¥] ë° [íŒŒì¼ ë‹¤ìš´ë¡œë“œ] íŒŒì´í”„ë¼ì¸ ê°€ë™...")
            
            success_count = 0
            skip_count = 0
            error_count = 0

            for data in data_list:
                try:
                    # source_idë¥¼ 3ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•˜ë¯€ë¡œ, 
                    # RegulationService ë‚´ë¶€ì—ì„œ ê³ ì •ëœ source_id=1ì„ ì“°ë©´ ì•ˆë©ë‹ˆë‹¤.
                    # í•˜ì§€ë§Œ í˜„ì¬ Service ì½”ë“œëŠ” source_id=1ë¡œ ê³ ì •ë˜ì–´ ìˆìœ¼ë‹ˆ
                    # ì´ ë¶€ë¶„ì€ Service ì½”ë“œ ìˆ˜ì • ì—†ì´ ì¼ë‹¨ ì§„í–‰í•©ë‹ˆë‹¤.
                    # (ì •ì„ì€ service.process_crawled_dataì— source_idë¥¼ ì¸ìë¡œ ë„˜ê¸°ëŠ” ê²ƒì…ë‹ˆë‹¤)
                    
                    result = await service.process_crawled_data(data, crawler)
                    
                    if result == "skipped":
                        skip_count += 1
                    else:
                        success_count += 1
                        
                except Exception as e:
                    print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨ ({data.get('title')}): {e}")
                    error_count += 1
                    await db.rollback()

            print("\n" + "="*40)
            print(f"âœ… ì‘ì—… ì™„ë£Œ ë¦¬í¬íŠ¸")
            print(f"âœ¨ ì²˜ë¦¬ ì„±ê³µ: {success_count}ê±´")
            print(f"â­ï¸ ë³€ê²½ ì—†ìŒ: {skip_count}ê±´")
            print(f"âŒ ì—ëŸ¬: {error_count}ê±´")
            print("="*40)

    except Exception as e:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        await crawler.close()
        await engine.dispose()

if __name__ == "__main__":
    asyncio.run(main())