#!/usr/bin/env python
"""
RAG ì±—ë´‡ (ê°„ì†Œí™” ë²„ì „ - ëª¨ë¸ ì¬ì‚¬ìš©)
updated: 2025-01-19
"""

import os
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv(project_root / ".env")

from openai import OpenAI

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ìºì‹±
_embedder = None
_vector_client = None


def get_embedder():
    """ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì‹±ê¸€í†¤"""
    global _embedder
    if _embedder is None:
        print("ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        from app.ai_pipeline.preprocess.embedding_pipeline import EmbeddingPipeline
        _embedder = EmbeddingPipeline(use_sparse=True)
        print("âœ… ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
    return _embedder


def get_vector_client():
    """VectorClient ì‹±ê¸€í†¤"""
    global _vector_client
    if _vector_client is None:
        from app.vectorstore.vector_client import VectorClient
        _vector_client = VectorClient(
            collection_name="skala-2.4.17-regulation",
            use_local=False
        )
    return _vector_client


def search_regulations(query: str, top_k: int = 3):
    """ê·œì œ ë¬¸ì„œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
    print(f"\nğŸ” ê²€ìƒ‰: '{query}'")
    
    # ì„ë² ë”© ìƒì„±
    embedder = get_embedder()
    print("  â³ ì„ë² ë”© ìƒì„± ì¤‘...")
    embeddings = embedder.embed_single_text(query)
    print("  âœ… ì„ë² ë”© ì™„ë£Œ")
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    vector_client = get_vector_client()
    print("  â³ Qdrant ê²€ìƒ‰ ì¤‘...")
    results = vector_client.search(
        query_dense=embeddings["dense"],
        query_sparse=embeddings.get("sparse"),
        top_k=top_k,
        hybrid_alpha=0.7
    )
    print(f"  âœ… {len(results['documents'])}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ\n")
    
    # ê²°ê³¼ ì¶œë ¥
    for i, (doc, meta, score) in enumerate(zip(
        results["documents"], 
        results["metadatas"], 
        results["scores"]
    ), 1):
        dense_score = meta.get('_dense_score') or 0.0
        sparse_score = meta.get('_sparse_score') or 0.0
        
        print(f"ğŸ“„ ê²°ê³¼ {i} (RRF: {score:.3f})")
        print(f"   ğŸ”¢ Dense: {dense_score:.3f} | Sparse: {sparse_score:.3f}")
        print(f"   ğŸŒ êµ­ê°€: {meta.get('country', 'N/A')}")
        print(f"   ğŸ“‹ ì œëª©: {meta.get('title', 'N/A')[:60]}...")
        print(f"   ğŸ“„ í˜ì´ì§€: {meta.get('page_num', 'N/A')}")
        print(f"   ğŸ“ ë‚´ìš©: {doc[:150]}...")
        print()
    
    return results


def generate_answer(query: str, context_docs: list[str]):
    """LLM ë‹µë³€ ìƒì„±"""
    print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n")
    
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    context = "\n\n".join([f"[ë¬¸ì„œ {i+1}]\n{doc}" for i, doc in enumerate(context_docs)])
    
    prompt = f"""ë‹¤ìŒ ê·œì œ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ê·œì œ ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€ (í•œêµ­ì–´ë¡œ, ê°„ê²°í•˜ê²Œ):"""
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ê·œì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,
        max_tokens=500
    )
    
    answer = response.choices[0].message.content
    print(f"ğŸ’¬ ë‹µë³€:\n{answer}\n")
    
    return answer


def main():
    print("=" * 60)
    print("ğŸ¤– RAG ì±—ë´‡ (ê°„ì†Œí™” ë²„ì „)")
    print("=" * 60)
    print("ëª…ë ¹ì–´: 'quit' ë˜ëŠ” 'exit' - ì¢…ë£Œ")
    print("=" * 60)
    
    # ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
    get_embedder()
    get_vector_client()
    
    print("\nâœ… ì¤€ë¹„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.\n")
    
    while True:
        try:
            query = input("ğŸ’¬ ì§ˆë¬¸: ").strip()
            
            if not query:
                continue
            
            if query.lower() in ["quit", "exit", "ì¢…ë£Œ"]:
                print("\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            # ê²€ìƒ‰ + ë‹µë³€
            results = search_regulations(query, top_k=3)
            generate_answer(query, results["documents"])
            
            print("-" * 60)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜: {e}\n")


if __name__ == "__main__":
    main()
