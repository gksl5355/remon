#!/usr/bin/env python
"""
Vision-Centric Preprocessing Pipeline ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

Usage:
    uv run python scripts/run_vision_pipeline.py
    uv run python scripts/run_vision_pipeline.py --pdf demo/1.pdf --enable-graph
"""

import asyncio
import logging
import argparse
from pathlib import Path
import sys
from datetime import datetime
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# .env íŒŒì¼ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv(project_root / ".env")

from app.ai_pipeline.preprocess.config import PreprocessConfig
from app.ai_pipeline.preprocess.vision_orchestrator import VisionOrchestrator

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
)
logger = logging.getLogger(__name__)

# ì¶œë ¥ ì €ì¥ ë””ë ‰í† ë¦¬
OUTPUT_DIR = project_root / "data" / "vision_outputs"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


def parse_args():
    parser = argparse.ArgumentParser(description="Vision Pipeline ì‹¤í–‰")
    parser.add_argument(
        "--pdf",
        type=str,
        help="ì²˜ë¦¬í•  PDF íŒŒì¼ ê²½ë¡œ (ì§€ì • ì•ˆí•˜ë©´ demo í´ë” ì „ì²´)",
    )
    parser.add_argument(
        "--folder",
        type=str,
        default="app/ai_pipeline/preprocess/demo",
        help="ì²˜ë¦¬í•  PDF í´ë” ê²½ë¡œ (ê¸°ë³¸: demo)",
    )
    parser.add_argument(
        "--enable-graph", action="store_true", help="ì§€ì‹ ê·¸ë˜í”„ ì¶”ì¶œ í™œì„±í™”"
    )
    parser.add_argument(
        "--disable-langsmith", action="store_true", help="LangSmith ì¶”ì  ë¹„í™œì„±í™”"
    )
    parser.add_argument(
        "--save-outputs",
        action="store_true",
        default=True,
        help="LLM ì¶œë ¥ì„ .txtë¡œ ì €ì¥ (ê¸°ë³¸: True)",
    )
    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
    parser.add_argument(
        "--max-concurrency",
        type=int,
        default=3,
        help="ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜ (ê¸°ë³¸ê°’: 3)",
    )
    parser.add_argument(
        "--token-budget",
        type=int,
        default=None,
        help="í† í° ì˜ˆì‚° (ê¸°ë³¸ê°’: None, ì œí•œ ì—†ìŒ)",
    )
    parser.add_argument(
        "--request-timeout",
        type=int,
        default=120,
        help="API ìš”ì²­ íƒ€ì„ì•„ì›ƒ ì´ˆ (ê¸°ë³¸ê°’: 120)",
    )
    parser.add_argument(
        "--retry-max-attempts",
        type=int,
        default=2,
        help="ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 2)",
    )
    parser.add_argument(
        "--retry-backoff-seconds",
        type=float,
        default=1.0,
        help="ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ ì´ˆ (ê¸°ë³¸ê°’: 1.0)",
    )
    parser.add_argument(
        "--no-parallel",
        action="store_true",
        help="ë³‘ë ¬ ì²˜ë¦¬ ë¹„í™œì„±í™” (ìˆœì°¨ ì²˜ë¦¬)",
    )
    parser.add_argument(
        "--skip-indexing",
        action="store_true",
        help="Qdrant ì €ì¥ ê±´ë„ˆë›°ê¸° (ì½˜ì†” ì¶œë ¥ë§Œ)",
    )
    parser.add_argument(
        "--collection",
        type=str,
        default=None,
        help="Qdrant ì»¬ë ‰ì…˜ëª… (ê¸°ë³¸ê°’: .envì˜ QDRANT_COLLECTION)",
    )
    return parser.parse_args()


def save_llm_outputs(result: dict, pdf_name: str, timestamp: str) -> None:
    """LLM ì¶œë ¥ì„ .txt íŒŒì¼ë¡œ ì €ì¥."""
    if result["status"] != "success":
        return

    vision_results = result.get("vision_extraction_result", [])
    
    # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ (80ê¸€ì)
    safe_pdf_name = pdf_name[:80] if len(pdf_name) > 80 else pdf_name

    for page_result in vision_results:
        page_num = page_result["page_num"]
        structure = page_result["structure"]
        markdown_content = structure["markdown_content"]
        model_used = page_result["model_used"]

        # íŒŒì¼ëª…: {pdf_name}_page{num}_{model}_{timestamp}.txt
        filename = f"{safe_pdf_name}_page{page_num:03d}_{model_used}_{timestamp}.txt"
        output_path = OUTPUT_DIR / filename

        # ë©”íƒ€ë°ì´í„° í¬í•¨ ì €ì¥
        content = f"""# Vision LLM Output
# PDF: {pdf_name}
# Page: {page_num}
# Model: {model_used}
# Complexity: {page_result['complexity_score']:.2f}
# Has Table: {page_result['has_table']}
# Tokens Used: {page_result.get('tokens_used', 0)}
# Timestamp: {timestamp}

{markdown_content}
"""

        output_path.write_text(content, encoding="utf-8")

    logger.info(f"ğŸ’¾ LLM ì¶œë ¥ ì €ì¥ ì™„ë£Œ: {OUTPUT_DIR} ({len(vision_results)}ê°œ íŒŒì¼)")


async def process_single_pdf(pdf_path: Path, args, orchestrator) -> dict:
    """ë‹¨ì¼ PDF ì²˜ë¦¬."""
    pdf_name = pdf_path.stem
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    logger.info("=" * 60)
    logger.info(f"ğŸš€ ì²˜ë¦¬ ì‹œì‘: {pdf_path.name}")
    logger.info("=" * 60)

    # ë³‘ë ¬ ì²˜ë¦¬ ì—¬ë¶€ ì„¤ì •
    use_parallel = not args.no_parallel
    result = await asyncio.to_thread(orchestrator.process_pdf, str(pdf_path), use_parallel)

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì½˜ì†”ì— ìƒì„¸ ê²°ê³¼ ì¶œë ¥
    if args.skip_indexing and result["status"] == "success":
        _print_detailed_results(result)

    # LLM ì¶œë ¥ ì €ì¥
    if args.save_outputs and result["status"] == "success":
        save_llm_outputs(result, pdf_name, timestamp)

    # ê²°ê³¼ ì¶œë ¥
    if result["status"] == "success":
        vision_results = result.get("vision_extraction_result", [])
        index_summary = result.get("dual_index_summary", {})
        
        gpt4o_count = sum(1 for p in vision_results if p.get("model_used") == "gpt-4o")
        total_tokens = sum(p.get("tokens_used", 0) for p in vision_results)

        if args.skip_indexing:
            logger.info(f"âœ… ì™„ë£Œ: {len(vision_results)}í˜ì´ì§€, {total_tokens:,}í† í° (Qdrant ì €ì¥ ê±´ë„ˆëœ€)")
        else:
            logger.info(f"âœ… ì™„ë£Œ: {len(vision_results)}í˜ì´ì§€, {index_summary.get('qdrant_chunks', 0)}ì²­í¬, {total_tokens:,}í† í°")
    else:
        logger.error(f"âŒ ì‹¤íŒ¨: {result.get('error')}")
    
    return result


def _print_detailed_results(result: dict) -> None:
    """í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì½˜ì†”ì— ìƒì„¸ ê²°ê³¼ ì¶œë ¥."""
    vision_results = result.get("vision_extraction_result", [])
    processing_results = result.get("processing_results", {})
    chunks = processing_results.get("chunks", [])
    
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“„ Vision ì¶”ì¶œ ê²°ê³¼ ìƒì„¸")
    logger.info("=" * 60)
    
    for page_result in vision_results:
        page_num = page_result["page_num"]
        model = page_result["model_used"]
        complexity = page_result["complexity_score"]
        tokens = page_result.get("tokens_used", 0)
        structure = page_result["structure"]
        markdown = structure.get("markdown_content", "")
        
        logger.info(f"\n[í˜ì´ì§€ {page_num}]")
        logger.info(f"  ëª¨ë¸: {model}")
        logger.info(f"  ë³µì¡ë„: {complexity:.2f}")
        logger.info(f"  í† í°: {tokens:,}")
        logger.info(f"  í‘œ í¬í•¨: {page_result.get('has_table', False)}")
        logger.info(f"  ë‚´ìš©:\n{markdown}")
        logger.info("-" * 60)
    
    if chunks:
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“¦ ì²­í‚¹ ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 60)
        for i, chunk in enumerate(chunks[:10], 1):  # ì²˜ìŒ 10ê°œë§Œ
            chunk_text = chunk.get("text", chunk.get("content", ""))
            logger.info(f"\n[ì²­í¬ {i}]")
            logger.info(f"  í˜ì´ì§€: {chunk.get('page_num', 'N/A')}")
            logger.info(f"  ì„¹ì…˜: {chunk.get('section', 'N/A')}")
            logger.info(f"  ë‚´ìš©: {chunk_text[:200]}...")
        if len(chunks) > 10:
            logger.info(f"\n... ì™¸ {len(chunks) - 10}ê°œ ì²­í¬")


async def main():
    args = parse_args()

    # LangSmith ì„¤ì •
    if not args.disable_langsmith:
        PreprocessConfig.setup_langsmith()

    # PDF ëª©ë¡ ìˆ˜ì§‘
    pdf_files = []
    
    if args.pdf:
        # ë‹¨ì¼ íŒŒì¼ ì§€ì •
        pdf_path = Path(args.pdf)
        if not pdf_path.is_absolute():
            pdf_path = project_root / pdf_path
        if pdf_path.exists():
            pdf_files = [pdf_path]
        else:
            logger.error(f"âŒ PDF íŒŒì¼ ì—†ìŒ: {pdf_path}")
            return
    else:
        # í´ë” ì „ì²´ ì²˜ë¦¬
        folder_path = Path(args.folder)
        if not folder_path.is_absolute():
            folder_path = project_root / folder_path
        
        if not folder_path.exists():
            logger.error(f"âŒ í´ë” ì—†ìŒ: {folder_path}")
            return
        
        pdf_files = sorted(folder_path.glob("*.pdf"))
        pdf_files = [p for p in pdf_files if not p.name.startswith(".")]
    
    if not pdf_files:
        logger.error("âŒ ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        return

    logger.info(f"ğŸ“š ì´ {len(pdf_files)}ê°œ PDF íŒŒì¼ ë°œê²¬")
    logger.info(f"ğŸ” ì§€ì‹ ê·¸ë˜í”„: {'í™œì„±í™”' if args.enable_graph else 'ë¹„í™œì„±í™”'}")
    logger.info(f"ğŸ’¾ ì¶œë ¥ ì €ì¥: {'í™œì„±í™”' if args.save_outputs else 'ë¹„í™œì„±í™”'}")
    logger.info(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: {'ë¹„í™œì„±í™”' if args.no_parallel else f'í™œì„±í™” (max_concurrency={args.max_concurrency})'}")
    logger.info(f"ğŸ—„ï¸  Qdrant ì €ì¥: {'ê±´ë„ˆë›°ê¸° (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)' if args.skip_indexing else 'í™œì„±í™”'}")

    # Orchestrator ìƒì„± (ìƒì„±ì ì¸ìë¡œ ì„¤ì • ì „ë‹¬)
    orchestrator = VisionOrchestrator(
        max_concurrency=args.max_concurrency,
        token_budget=args.token_budget,
        request_timeout=args.request_timeout,
        retry_max_attempts=args.retry_max_attempts,
        retry_backoff_seconds=args.retry_backoff_seconds,
        enable_graph=args.enable_graph,
    )
    
    # ì»¬ë ‰ì…˜ëª… ì„¤ì •
    if args.collection:
        from app.ai_pipeline.preprocess.semantic_processing import DualIndexer
        orchestrator.dual_indexer = DualIndexer(collection_name=args.collection)
        logger.info(f"ğŸ—„ï¸  Qdrant ì»¬ë ‰ì…˜: {args.collection}")
    else:
        logger.info(f"ğŸ—„ï¸  Qdrant ì»¬ë ‰ì…˜: {os.getenv('QDRANT_COLLECTION', 'remon_regulations')}")
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: Qdrant ì €ì¥ ê±´ë„ˆë›°ê¸° (ìŠ¤í¬ë¦½íŠ¸ ë ˆë²¨ì—ì„œë§Œ ì²˜ë¦¬)
    if args.skip_indexing:
        from unittest.mock import MagicMock
        # DualIndexerë¥¼ Mockìœ¼ë¡œ êµì²´
        orchestrator.dual_indexer = MagicMock()
        orchestrator.dual_indexer.index = lambda chunks, graph_data, source_file: {
            "status": "skipped",
            "qdrant_chunks": 0,
            "graph_nodes": len(graph_data.get("nodes", [])),
            "graph_edges": len(graph_data.get("edges", [])),
            "collection_name": "test_mode",
            "processed_at": "test_mode",
            "message": "Indexing skipped for testing"
        }

    # ìˆœì°¨ ì²˜ë¦¬
    results = []
    for idx, pdf_path in enumerate(pdf_files, 1):
        logger.info(f"\n[{idx}/{len(pdf_files)}] {pdf_path.name}")
        result = await process_single_pdf(pdf_path, args, orchestrator)
        results.append({"file": pdf_path.name, "status": result["status"]})

    # ì „ì²´ ìš”ì•½
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ")
    logger.info("=" * 60)
    
    success_count = sum(1 for r in results if r["status"] == "success")
    logger.info(f"ì„±ê³µ: {success_count}/{len(results)}")
    
    if success_count < len(results):
        logger.info("\nì‹¤íŒ¨ íŒŒì¼:")
        for r in results:
            if r["status"] != "success":
                logger.info(f"  - {r['file']}")
    
    if args.save_outputs:
        logger.info(f"\nğŸ“ ì¶œë ¥ ìœ„ì¹˜: {OUTPUT_DIR}")


if __name__ == "__main__":
    asyncio.run(main())
