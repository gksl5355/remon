#!/usr/bin/env python
"""
Vision Pipeline + Change Detection í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

Usage:
    # Vision Pipelineë§Œ ì‹¤í–‰
    uv run python scripts/run_vision_pipeline\ copy.py --pdf demo/1.pdf

    # Vision Pipeline + Change Detection ì‹¤í–‰
    uv run python scripts/run_vision_pipeline\ copy.py --pdf demo/new_regulation.pdf --enable-change-detection --legacy-id FDA-2024-001

    # ì»¬ë ‰ì…˜ ì´ˆê¸°í™” í›„ ì‹¤í–‰
    uv run python scripts/run_vision_pipeline\ copy.py --pdf demo/1.pdf --reset-collection
"""

import asyncio
import logging
import argparse
from pathlib import Path
import sys
from datetime import datetime
import os
from typing import Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# .env íŒŒì¼ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv(project_root / ".env")

from app.ai_pipeline.preprocess.config import PreprocessConfig
from app.ai_pipeline.preprocess.vision_orchestrator import VisionOrchestrator

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
)
logger = logging.getLogger(__name__)

# ì¶œë ¥ ì €ì¥ ë””ë ‰í† ë¦¬
OUTPUT_DIR = project_root / "data" / "vision_outputs"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


def parse_args():
    parser = argparse.ArgumentParser(description="Vision Pipeline ì‹¤í–‰")
    parser.add_argument(
        "--pdf",
        type=str,
        help="ì²˜ë¦¬í•  PDF íŒŒì¼ ê²½ë¡œ (ì§€ì • ì•ˆí•˜ë©´ demo í´ë” ì „ì²´)",
    )
    parser.add_argument(
        "--folder",
        type=str,
        default="app/ai_pipeline/preprocess/demo",
        help="ì²˜ë¦¬í•  PDF í´ë” ê²½ë¡œ (ê¸°ë³¸: demo)",
    )
    parser.add_argument(
        "--enable-graph", action="store_true", help="ì§€ì‹ ê·¸ë˜í”„ ì¶”ì¶œ í™œì„±í™”"
    )
    parser.add_argument(
        "--disable-langsmith", action="store_true", help="LangSmith ì¶”ì  ë¹„í™œì„±í™”"
    )
    parser.add_argument(
        "--save-outputs",
        action="store_true",
        default=True,
        help="LLM ì¶œë ¥ì„ .txtë¡œ ì €ì¥ (ê¸°ë³¸: True)",
    )
    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
    parser.add_argument(
        "--max-concurrency",
        type=int,
        default=30,
        help="ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜ (ê¸°ë³¸ê°’: 30)",
    )
    parser.add_argument(
        "--token-budget",
        type=int,
        default=None,
        help="í† í° ì˜ˆì‚° (ê¸°ë³¸ê°’: None, ì œí•œ ì—†ìŒ)",
    )
    parser.add_argument(
        "--request-timeout",
        type=int,
        default=120,
        help="API ìš”ì²­ íƒ€ì„ì•„ì›ƒ ì´ˆ (ê¸°ë³¸ê°’: 120)",
    )
    parser.add_argument(
        "--retry-max-attempts",
        type=int,
        default=2,
        help="ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 2)",
    )
    parser.add_argument(
        "--retry-backoff-seconds",
        type=float,
        default=1.0,
        help="ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ ì´ˆ (ê¸°ë³¸ê°’: 1.0)",
    )
    parser.add_argument(
        "--no-parallel",
        action="store_true",
        help="ë³‘ë ¬ ì²˜ë¦¬ ë¹„í™œì„±í™” (ìˆœì°¨ ì²˜ë¦¬)",
    )
    parser.add_argument(
        "--skip-indexing",
        action="store_true",
        help="Qdrant ì €ì¥ ê±´ë„ˆë›°ê¸° (ì½˜ì†” ì¶œë ¥ë§Œ)",
    )
    parser.add_argument(
        "--collection",
        type=str,
        default=None,
        help="Qdrant ì»¬ë ‰ì…˜ëª… (ê¸°ë³¸ê°’: .envì˜ QDRANT_COLLECTION)",
    )
    parser.add_argument(
        "--reset-collection",
        action="store_true",
        help="ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„±",
    )
    parser.add_argument(
        "--enable-change-detection",
        action="store_true",
        help="ë³€ê²½ ê°ì§€ í™œì„±í™”",
    )
    parser.add_argument(
        "--legacy-id",
        type=str,
        default=None,
        help="Legacy ê·œì œ ID (ë³€ê²½ ê°ì§€ ì‹œ í•„ìš”, ì˜ˆ: FDA-2024-001)",
    )
    parser.add_argument(
        "--legacy-file",
        type=str,
        default=None,
        help="Legacy ì „ì²˜ë¦¬ JSON íŒŒì¼ ê²½ë¡œ (íŒŒì¼ ê¸°ë°˜ ë¹„êµìš©)",
    )
    parser.add_argument(
        "--compare-jsons",
        action="store_true",
        help="ê¸°ì¡´ JSON íŒŒì¼ 2ê°œë¥¼ ì§ì ‘ ë¹„êµ (Vision Pipeline ìƒëµ)",
    )
    parser.add_argument(
        "--new-json",
        type=str,
        default=None,
        help="ì‹ ê·œ ê·œì œ JSON íŒŒì¼ ê²½ë¡œ (--compare-jsons ì‚¬ìš© ì‹œ)",
    )
    parser.add_argument(
        "--legacy-json",
        type=str,
        default=None,
        help="Legacy ê·œì œ JSON íŒŒì¼ ê²½ë¡œ (--compare-jsons ì‚¬ìš© ì‹œ)",
    )
    parser.add_argument(
        "--save-preprocessed",
        action="store_true",
        help="ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥ (demo í´ë”)",
    )
    return parser.parse_args()


def save_llm_outputs(result: dict, pdf_name: str, timestamp: str) -> None:
    """LLM ì¶œë ¥ì„ .txt íŒŒì¼ë¡œ ì €ì¥."""
    if result["status"] != "success":
        return

    vision_results = result.get("vision_extraction_result", [])
    # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ (80ê¸€ì)
    safe_pdf_name = pdf_name[:80] if len(pdf_name) > 80 else pdf_name

    for page_result in vision_results:
        page_num = page_result["page_num"]
        structure = page_result["structure"]
        markdown_content = structure["markdown_content"]
        model_used = page_result["model_used"]

        # íŒŒì¼ëª…: {pdf_name}_page{num}_{model}_{timestamp}.txt
        filename = f"{safe_pdf_name}_page{page_num:03d}_{model_used}_{timestamp}.txt"
        output_path = OUTPUT_DIR / filename

        # ë©”íƒ€ë°ì´í„° í¬í•¨ ì €ì¥
        content = f"""# Vision LLM Output
# PDF: {pdf_name}
# Page: {page_num}
# Model: {model_used}
# Complexity: {page_result['complexity_score']:.2f}
# Has Table: {page_result['has_table']}
# Tokens Used: {page_result.get('tokens_used', 0)}
# Timestamp: {timestamp}

{markdown_content}
"""

        output_path.write_text(content, encoding="utf-8")

    logger.info(f"ğŸ’¾ LLM ì¶œë ¥ ì €ì¥ ì™„ë£Œ: {OUTPUT_DIR} ({len(vision_results)}ê°œ íŒŒì¼)")


def save_preprocessed_data(result: dict, pdf_path: Path, output_dir: Path) -> Optional[Path]:
    """ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥ (ë³€ê²½ ê°ì§€ìš©)."""
    import json

    if result["status"] != "success":
        return None

    output_dir.mkdir(parents=True, exist_ok=True)

    # íŒŒì¼ëª…: {pdf_name}_preprocessed.json
    output_file = output_dir / f"{pdf_path.stem}_preprocessed.json"

    # ì €ì¥í•  ë°ì´í„° êµ¬ì„±
    preprocessed_data = {
        "source_pdf": pdf_path.name,
        "processed_at": datetime.now().isoformat(),
        "vision_extraction_result": result.get("vision_extraction_result", []),
        "graph_data": result.get("graph_data", {}),
        "dual_index_summary": result.get("dual_index_summary", {}),
    }

    output_file.write_text(
        json.dumps(preprocessed_data, indent=2, ensure_ascii=False), encoding="utf-8"
    )
    logger.info(f"ğŸ’¾ ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥: {output_file}")

    return output_file


async def process_single_pdf(pdf_path: Path, args, orchestrator) -> dict:
    """ë‹¨ì¼ PDF ì²˜ë¦¬ (Vision Pipeline + S3 ì—…ë¡œë“œ + ì„ íƒì  Change Detection)."""
    pdf_name = pdf_path.stem
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    logger.info("=" * 60)
    logger.info(f"ğŸš€ ì²˜ë¦¬ ì‹œì‘: {pdf_path.name}")
    logger.info("=" * 60)

    # ì „ì²˜ë¦¬ JSON íŒŒì¼ í™•ì¸
    import json
    demo_dir = project_root / "app" / "ai_pipeline" / "preprocess" / "demo"
    preprocessed_file = demo_dir / f"{pdf_path.stem}_preprocessed.json"
    
    # S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    from app.utils.s3_client import S3Client
    s3_client = S3Client()
    
    if preprocessed_file.exists():
        logger.info(f"ğŸ“‚ ì „ì²˜ë¦¬ JSON íŒŒì¼ ë°œê²¬: {preprocessed_file.name}")
        logger.info("â© Vision Pipeline ìƒëµ, JSONì—ì„œ ë¡œë“œ")
        
        preprocessed_data = json.loads(preprocessed_file.read_text(encoding="utf-8"))
        result = {
            "status": "success",
            "vision_extraction_result": preprocessed_data.get("vision_extraction_result", []),
            "graph_data": preprocessed_data.get("graph_data", {}),
            "dual_index_summary": preprocessed_data.get("dual_index_summary", {})
        }
        logger.info(f"âœ… ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(result['vision_extraction_result'])}í˜ì´ì§€")
    else:
        logger.info("ğŸ”„ Vision Pipeline ì‹¤í–‰")
        
        # Phase 1: Vision Pipeline ì‹¤í–‰ (Prompt Cachingì„ ìœ„í•´ ìˆœì°¨ ì²˜ë¦¬ ê°•ì œ)
        use_parallel = not args.no_parallel
        if use_parallel:
            logger.warning("âš ï¸  Prompt Caching í™œì„±í™”ë¥¼ ìœ„í•´ ìˆœì°¨ ì²˜ë¦¬ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            use_parallel = False
        result = await asyncio.to_thread(
            orchestrator.process_pdf, str(pdf_path), use_parallel
        )

        if result["status"] != "success":
            logger.error(f"âŒ Vision Pipeline ì‹¤íŒ¨: {result.get('error')}")
            return result

        # LLM ì¶œë ¥ ì €ì¥
        if args.save_outputs:
            save_llm_outputs(result, pdf_name, timestamp)

        # ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥ (demo í´ë” + S3)
        if args.save_preprocessed:
            saved_json = save_preprocessed_data(result, pdf_path, demo_dir)
            
            # S3 ì—…ë¡œë“œ
            if saved_json:
                try:
                    s3_key = s3_client.upload_json(str(saved_json))
                    logger.info(f"ğŸŒ S3 ì—…ë¡œë“œ ì™„ë£Œ: {s3_key}")
                except Exception as e:
                    logger.error(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

    # Phase 2: Change Detection (ì„ íƒì )
    if args.enable_change_detection:
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ” ë³€ê²½ ê°ì§€ ì‹œì‘")
        logger.info("=" * 60)

        from app.ai_pipeline.preprocess import preprocess_node
        from app.ai_pipeline.state import AppState
        import json

        # Legacy ë°ì´í„° ë¡œë“œ (íŒŒì¼ ê¸°ë°˜ ë¹„êµ)
        legacy_vision_results = None
        if args.legacy_file:
            legacy_path = Path(args.legacy_file)
            if not legacy_path.is_absolute():
                legacy_path = project_root / legacy_path

            if legacy_path.exists():
                legacy_data = json.loads(legacy_path.read_text(encoding="utf-8"))
                legacy_vision_results = legacy_data.get("vision_extraction_result", [])
                logger.info(f"ğŸ“‚ Legacy íŒŒì¼ ë¡œë“œ: {legacy_path.name}")
            else:
                logger.warning(f"âš ï¸ Legacy íŒŒì¼ ì—†ìŒ: {legacy_path}")

        # AppState êµ¬ì„±
        state: AppState = {
            "preprocess_request": {
                "pdf_paths": [str(pdf_path)],
                "use_vision_pipeline": True,
                "enable_change_detection": True,
            },
            "vision_extraction_result": result.get("vision_extraction_result", []),
            "graph_data": result.get("graph_data", {}),
            "dual_index_summary": result.get("dual_index_summary", {}),
            "change_context": (
                {
                    "legacy_regulation_id": args.legacy_id,
                    "legacy_vision_results": legacy_vision_results,
                }
                if args.legacy_id or legacy_vision_results
                else {}
            ),
        }

        # Change Detection ì‹¤í–‰
        try:
            from app.ai_pipeline.nodes.change_detection import change_detection_node

            state = await change_detection_node(state)

            # ë³€ê²½ ê°ì§€ ê²°ê³¼ ì¶”ê°€
            result["change_detection_results"] = state.get(
                "change_detection_results", []
            )
            result["change_summary"] = state.get("change_summary", {})

            # ë³€ê²½ ê°ì§€ ê²°ê³¼ ì¶œë ¥
            _print_change_detection_results(state)

        except Exception as e:
            logger.error(f"âŒ ë³€ê²½ ê°ì§€ ì‹¤íŒ¨: {e}", exc_info=True)
            result["change_detection_error"] = str(e)

    # ê²°ê³¼ ì¶œë ¥
    vision_results = result.get("vision_extraction_result", [])
    index_summary = result.get("dual_index_summary", {})
    total_tokens = sum(p.get("tokens_used", 0) for p in vision_results)

    if args.skip_indexing:
        logger.info(
            f"\nâœ… ì™„ë£Œ: {len(vision_results)}í˜ì´ì§€, {total_tokens:,}í† í° (Qdrant ì €ì¥ ê±´ë„ˆëœ€)"
        )
    else:
        logger.info(
            f"\nâœ… ì™„ë£Œ: {len(vision_results)}í˜ì´ì§€, {index_summary.get('qdrant_chunks', 0)}ì²­í¬, {total_tokens:,}í† í°"
        )

    return result


async def main():
    args = parse_args()

    # LangSmith ì„¤ì •
    if not args.disable_langsmith:
        PreprocessConfig.setup_langsmith()

    # JSON ì§ì ‘ ë¹„êµ ëª¨ë“œ
    if args.compare_jsons:
        await compare_json_files(args)
        return

    # PDF ëª©ë¡ ìˆ˜ì§‘
    pdf_files = []

    if args.pdf:
        # ë‹¨ì¼ íŒŒì¼ ì§€ì •
        pdf_path = Path(args.pdf)
        if not pdf_path.is_absolute():
            pdf_path = project_root / pdf_path
        if pdf_path.exists():
            pdf_files = [pdf_path]
        else:
            logger.error(f"âŒ PDF íŒŒì¼ ì—†ìŒ: {pdf_path}")
            return
    else:
        # í´ë” ì „ì²´ ì²˜ë¦¬
        folder_path = Path(args.folder)
        if not folder_path.is_absolute():
            folder_path = project_root / folder_path

        if not folder_path.exists():
            logger.error(f"âŒ í´ë” ì—†ìŒ: {folder_path}")
            return

        pdf_files = sorted(folder_path.glob("*.pdf"))
        pdf_files = [p for p in pdf_files if not p.name.startswith(".")]

    if not pdf_files:
        logger.error("âŒ ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        return

    logger.info(f"ğŸ“š ì´ {len(pdf_files)}ê°œ PDF íŒŒì¼ ë°œê²¬")
    logger.info(f"ğŸ” ì§€ì‹ ê·¸ë˜í”„: {'í™œì„±í™”' if args.enable_graph else 'ë¹„í™œì„±í™”'}")
    logger.info(f"ğŸ’¾ ì¶œë ¥ ì €ì¥: {'í™œì„±í™”' if args.save_outputs else 'ë¹„í™œì„±í™”'}")
    logger.info(
        f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: {'ë¹„í™œì„±í™”' if args.no_parallel else f'í™œì„±í™” (max_concurrency={args.max_concurrency})'}"
    )
    logger.info(
        f"ğŸ—„ï¸  Qdrant ì €ì¥: {'ê±´ë„ˆë›°ê¸° (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)' if args.skip_indexing else 'í™œì„±í™”'}"
    )
    logger.info(
        f"ğŸ”„ ë³€ê²½ ê°ì§€: {'í™œì„±í™”' if args.enable_change_detection else 'ë¹„í™œì„±í™”'}"
    )
    logger.info(
        f"ğŸ’¾ ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥: {'í™œì„±í™”' if args.save_preprocessed else 'ë¹„í™œì„±í™”'}"
    )
    if args.enable_change_detection:
        if args.legacy_id:
            logger.info(f"ğŸ“‹ Legacy ê·œì œ ID: {args.legacy_id}")
        if args.legacy_file:
            logger.info(f"ğŸ“‚ Legacy íŒŒì¼: {args.legacy_file}")

    # ì»¬ë ‰ì…˜ëª… ì„¤ì •
    collection_name = args.collection or os.getenv(
        "QDRANT_COLLECTION", "skala-2.4.17-regulation"
    )

    # Orchestrator ìƒì„±
    orchestrator = VisionOrchestrator(
        max_concurrency=args.max_concurrency,
        token_budget=args.token_budget,
        request_timeout=args.request_timeout,
        retry_max_attempts=args.retry_max_attempts,
        retry_backoff_seconds=args.retry_backoff_seconds,
        enable_graph=args.enable_graph,
    )

    # ì»¬ë ‰ì…˜ëª… ì„¤ì •
    if args.collection and not args.skip_indexing:
        from app.ai_pipeline.preprocess.semantic_processing import DualIndexer

        orchestrator.dual_indexer = DualIndexer(collection_name=args.collection)
        logger.info(f"ğŸ—„ï¸  Qdrant ì»¬ë ‰ì…˜: {args.collection}")
    else:
        logger.info(f"ğŸ—„ï¸  Qdrant ì»¬ë ‰ì…˜: {collection_name}")

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: Qdrant ì €ì¥ ê±´ë„ˆë›°ê¸°
    if args.skip_indexing:
        from unittest.mock import MagicMock

        orchestrator.dual_indexer = MagicMock()
        orchestrator.dual_indexer.index = lambda chunks, graph_data, source_file, regulation_id=None, vision_results=None: {
            "status": "skipped",
            "qdrant_chunks": 0,
            "reference_blocks_count": 0,
            "graph_nodes": len(graph_data.get("nodes", [])),
            "graph_edges": len(graph_data.get("edges", [])),
            "collection_name": "test_mode",
            "processed_at": "test_mode",
        }

    # ìˆœì°¨ ì²˜ë¦¬
    results = []
    for idx, pdf_path in enumerate(pdf_files, 1):
        logger.info(f"\n[{idx}/{len(pdf_files)}] {pdf_path.name}")
        result = await process_single_pdf(pdf_path, args, orchestrator)
        results.append({"file": pdf_path.name, "status": result["status"]})

    # ì „ì²´ ìš”ì•½
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ")
    logger.info("=" * 60)

    success_count = sum(1 for r in results if r["status"] == "success")
    logger.info(f"ì„±ê³µ: {success_count}/{len(results)}")

    if success_count < len(results):
        logger.info("\nì‹¤íŒ¨ íŒŒì¼:")
        for r in results:
            if r["status"] != "success":
                logger.info(f"  - {r['file']}")
    if args.save_outputs:
        logger.info(f"\nğŸ“ ì¶œë ¥ ìœ„ì¹˜: {OUTPUT_DIR}")


async def compare_json_files(args) -> None:
    """ê¸°ì¡´ JSON íŒŒì¼ 2ê°œë¥¼ ì§ì ‘ ë¹„êµ (Vision Pipeline ìƒëµ)."""
    import json
    from app.ai_pipeline.nodes.change_detection import change_detection_node
    from app.ai_pipeline.state import AppState

    if not args.new_json or not args.legacy_json:
        logger.error("âŒ --new-jsonê³¼ --legacy-json í•„ìˆ˜")
        return

    # JSON íŒŒì¼ ë¡œë“œ
    new_path = Path(args.new_json)
    legacy_path = Path(args.legacy_json)

    if not new_path.is_absolute():
        new_path = project_root / new_path
    if not legacy_path.is_absolute():
        legacy_path = project_root / legacy_path

    if not new_path.exists():
        logger.error(f"âŒ ì‹ ê·œ JSON ì—†ìŒ: {new_path}")
        return
    if not legacy_path.exists():
        logger.error(f"âŒ Legacy JSON ì—†ìŒ: {legacy_path}")
        return

    logger.info("=" * 60)
    logger.info("ğŸ” JSON ë¹„êµ ëª¨ë“œ")
    logger.info("=" * 60)
    logger.info(f"ì‹ ê·œ: {new_path.name}")
    logger.info(f"Legacy: {legacy_path.name}")

    new_data = json.loads(new_path.read_text(encoding="utf-8"))
    legacy_data = json.loads(legacy_path.read_text(encoding="utf-8"))

    new_vision = new_data.get("vision_extraction_result", [])
    legacy_vision = legacy_data.get("vision_extraction_result", [])

    logger.info(f"ì‹ ê·œ: {len(new_vision)}í˜ì´ì§€")
    logger.info(f"Legacy: {len(legacy_vision)}í˜ì´ì§€")

    # regulation_id ì¶”ì¶œ
    new_metadata = new_vision[0].get("structure", {}).get("metadata", {}) if new_vision else {}
    legacy_metadata = legacy_vision[0].get("structure", {}).get("metadata", {}) if legacy_vision else {}

    new_id = f"{new_metadata.get('regulation_type', 'REG')}-{new_metadata.get('country', 'US')}-{new_path.stem}"
    legacy_id = f"{legacy_metadata.get('regulation_type', 'REG')}-{legacy_metadata.get('country', 'US')}-{legacy_path.stem}"

    logger.info(f"\nì‹ ê·œ ID: {new_id}")
    logger.info(f"Legacy ID: {legacy_id}")

    # AppState êµ¬ì„±
    state: AppState = {
        "vision_extraction_result": new_vision,
        "change_context": {
            "new_regulation_id": new_id,
            "legacy_regulation_id": legacy_id,
            "legacy_vision_results": legacy_vision,  # ì§ì ‘ ì œê³µ
        },
    }

    # ë³€ê²½ ê°ì§€ ì‹¤í–‰
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ” ë³€ê²½ ê°ì§€ ì‹¤í–‰")
    logger.info("=" * 60)

    try:
        state = await change_detection_node(state)
        _print_change_detection_results(state)
    except Exception as e:
        logger.error(f"âŒ ë³€ê²½ ê°ì§€ ì‹¤íŒ¨: {e}", exc_info=True)


def _print_change_detection_results(state: dict) -> None:
    """ë³€ê²½ ê°ì§€ ê²°ê³¼ ì¶œë ¥."""
    change_summary = state.get("change_summary", {})
    change_results = state.get("change_detection_results", [])

    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š ë³€ê²½ ê°ì§€ ìš”ì•½")
    logger.info("=" * 60)
    logger.info(f"ìƒíƒœ: {change_summary.get('status')}")
    logger.info(f"ì´ Reference Blocks: {change_summary.get('total_reference_blocks', 0)}ê°œ")
    logger.info(f"ë³€ê²½ ê°ì§€: {change_summary.get('total_changes', 0)}ê°œ")
    logger.info(f"HIGH ì‹ ë¢°ë„: {change_summary.get('high_confidence_changes', 0)}ê°œ")
    logger.info(f"Legacy ID: {change_summary.get('legacy_regulation_id', 'N/A')}")
    logger.info(f"ì‹ ê·œ ID: {change_summary.get('new_regulation_id', 'N/A')}")

    if not change_results:
        logger.info("\në³€ê²½ ì‚¬í•­ ì—†ìŒ")
        return

    logger.info("\n" + "=" * 60)
    logger.info("ğŸ” ë³€ê²½ ê°ì§€ ìƒì„¸ ê²°ê³¼")
    logger.info("=" * 60)

    changes_found = [r for r in change_results if r.get("change_detected")]
    
    for i, result in enumerate(changes_found, 1):
        logger.info(f"\n[ë³€ê²½ {i}/{len(changes_found)}] Section {result.get('section_ref')}")
        logger.info(f"  Ref ID: {result.get('new_ref_id')} â†” {result.get('legacy_ref_id')}")
        logger.info(f"  ë³€ê²½ ìœ í˜•: {result.get('change_type')}")
        logger.info(
            f"  ì‹ ë¢°ë„: {result.get('confidence_score', 0):.2f} ({result.get('confidence_level')})"
        )
        logger.info(f"  Legacy: {result.get('legacy_snippet', '')[:100]}...")
        logger.info(f"  ì‹ ê·œ: {result.get('new_snippet', '')[:100]}...")

        # Chain of Thought
        reasoning = result.get("reasoning", {})
        if reasoning:
            logger.info(f"\n  íŒë‹¨ ê·¼ê±°:")
            logger.info(
                f"    Step 1: {reasoning.get('step1_context_analysis', '')[:150]}"
            )
            logger.info(
                f"    Step 2: {reasoning.get('step2_term_comparison', '')[:150]}"
            )
            logger.info(
                f"    Step 3: {reasoning.get('step3_semantic_evaluation', '')[:150]}"
            )
            logger.info(
                f"    Step 4: {reasoning.get('step4_final_judgment', '')[:150]}"
            )

        # ìˆ˜ì¹˜ ë³€ê²½
        numerical_changes = result.get("numerical_changes", [])
        if numerical_changes:
            logger.info(f"\n  ìˆ˜ì¹˜ ë³€ê²½:")
            for nc in numerical_changes:
                logger.info(
                    f"    - {nc.get('field')}: {nc.get('legacy_value')} â†’ {nc.get('new_value')}"
                )
                logger.info(f"      ë§¥ë½: {nc.get('context', 'N/A')}")
                logger.info(f"      ì˜í–¥ë„: {nc.get('impact')}")
        
        # Adversarial Check
        adv_check = result.get("adversarial_check", {})
        if adv_check:
            logger.info(f"\n  ë°˜ë°• ê²€ì¦:")
            logger.info(f"    ë°˜ë¡ : {adv_check.get('counter_argument', '')[:100]}")
            logger.info(f"    ì¬ë°˜ë°•: {adv_check.get('rebuttal', '')[:100]}")
            logger.info(f"    ì¡°ì • ì‹ ë¢°ë„: {adv_check.get('adjusted_confidence', 0):.2f}")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"\nâŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}", exc_info=True)
