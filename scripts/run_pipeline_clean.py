"""
module: run_pipeline_clean.py
description: REMON AI Pipeline í´ë¦° ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (í•˜ë“œì½”ë”© ì œê±°, ê·¸ë˜í”„/State ê¸°ë°˜ ìë™ ì‹¤í–‰)
author: AI Agent
created: 2025-01-21
updated: 2025-01-21
dependencies:
    - app.ai_pipeline.graph
    - app.ai_pipeline.state

ì‹¤í–‰ ë°©ë²•:
    # ë‹¨ì¼ ì œí’ˆ ì²˜ë¦¬
    python scripts/run_pipeline_clean.py --pdf /tmp/Regulation_Data_B.pdf --product-id 1
    
    # ì „ì²´ ì œí’ˆ ì²˜ë¦¬
    python scripts/run_pipeline_clean.py --pdf /tmp/Regulation_Data_B.pdf --all-products
    
    # S3 ìë™ ë¡œë“œ
    python scripts/run_pipeline_clean.py --s3-date 20250121 --all-products
"""

import asyncio
import argparse
import logging
import sys
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

from app.ai_pipeline.graph import build_graph
from app.ai_pipeline.state import AppState
from app.core.database import AsyncSessionLocal
from sqlalchemy import text

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(
            f'logs/clean_pipeline_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        ),
    ],
)
logger = logging.getLogger(__name__)


def print_summary(state: AppState):
    """íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥."""
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“‹ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼")
    logger.info("=" * 80)
    
    # ì „ì²˜ë¦¬
    preprocess_summary = state.get("preprocess_summary", {})
    if preprocess_summary:
        logger.info(f"\nğŸ“„ ì „ì²˜ë¦¬: {preprocess_summary.get('status')}")
        logger.info(f"  - ì„±ê³µ: {preprocess_summary.get('succeeded', 0)}ê°œ")
    
    # ë³€ê²½ ê°ì§€
    change_summary = state.get("change_summary", {})
    if change_summary:
        logger.info(f"\nğŸ” ë³€ê²½ ê°ì§€: {change_summary.get('status')}")
        logger.info(f"  - ë³€ê²½: {change_summary.get('total_changes', 0)}ê°œ")
        logger.info(f"  - ê³ ì‹ ë¢°ë„: {change_summary.get('high_confidence_changes', 0)}ê°œ")
    
    # ì„ë² ë”©
    dual_index = state.get("dual_index_summary", {})
    if dual_index:
        logger.info(f"\nğŸ“¦ ì„ë² ë”©: {dual_index.get('qdrant_chunks', 0)}ê°œ ì²­í¬")
    
    # ë§¤í•‘
    mapping = state.get("mapping", {})
    if mapping:
        items = mapping.get("items", [])
        applies = sum(1 for item in items if item.get("applies"))
        logger.info(f"\nğŸ”— ë§¤í•‘: {len(items)}ê°œ í•­ëª© ({applies}ê°œ ì ìš©)")
    
    # ì „ëµ
    strategies = state.get("strategies", [])
    if strategies:
        logger.info(f"\nğŸ’¡ ì „ëµ: {len(strategies)}ê°œ")
    
    # ì˜í–¥ë„
    impact_scores = state.get("impact_scores", [])
    if impact_scores:
        impact = impact_scores[0]
        logger.info(f"\nğŸ“Š ì˜í–¥ë„: {impact.get('impact_level')} ({impact.get('weighted_score', 0):.2f})")
    
    # ë¦¬í¬íŠ¸
    report = state.get("report", {})
    if report:
        logger.info(f"\nğŸ“‹ ë¦¬í¬íŠ¸: {len(report.get('sections', []))}ê°œ ì„¹ì…˜")
    
    logger.info("\n" + "=" * 80)


async def fetch_product_ids() -> list[int]:
    """DBì—ì„œ ì „ì²´ ì œí’ˆ ID ì¡°íšŒ."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            text("SELECT product_id FROM products ORDER BY product_id")
        )
        return [row[0] for row in result.fetchall()]


async def run_single_product(pdf_path: str, product_id: int):
    """ë‹¨ì¼ ì œí’ˆ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰."""
    logger.info("=" * 80)
    logger.info(f"ğŸš€ ë‹¨ì¼ ì œí’ˆ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (product_id={product_id})")
    logger.info("=" * 80)
    
    # ì´ˆê¸° State (ìµœì†Œí•œì˜ ì„¤ì •ë§Œ)
    state: AppState = {
        "preprocess_request": {
            "pdf_paths": [pdf_path],
            "use_vision_pipeline": True,
        },
        "change_context": {},  # ë¹„ì–´ìˆìŒ â†’ change_detection_nodeê°€ ìë™ ì²˜ë¦¬
        "mapping_filters": {"product_id": product_id},
    }
    
    # ê·¸ë˜í”„ ì‹¤í–‰ (start_node ì§€ì • ì•ˆ í•¨ = "preprocess")
    app = build_graph()
    
    try:
        final_state = await app.ainvoke(state)
        logger.info(f"âœ… ì œí’ˆ {product_id} íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        print_summary(final_state)
        return final_state
    except Exception as e:
        logger.error(f"âŒ ì œí’ˆ {product_id} íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}", exc_info=True)
        return None


async def run_all_products(pdf_path: str):
    """ì „ì²´ ì œí’ˆ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì „ì²˜ë¦¬ 1íšŒ ì¬ì‚¬ìš©)."""
    logger.info("=" * 80)
    logger.info("ğŸš€ ì „ì²´ ì œí’ˆ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
    logger.info("=" * 80)
    
    # Step 1: ì „ì²˜ë¦¬ + ë³€ê²½ ê°ì§€ 1íšŒ ì‹¤í–‰
    logger.info("\n[Step 1] ì „ì²˜ë¦¬ + ë³€ê²½ ê°ì§€ (1íšŒ)")
    
    from app.ai_pipeline.preprocess import preprocess_node
    from app.ai_pipeline.nodes.change_detection import change_detection_node
    from app.ai_pipeline.nodes.embedding import embedding_node
    
    base_state: AppState = {
        "preprocess_request": {
            "pdf_paths": [pdf_path],
            "use_vision_pipeline": True,
        },
        "change_context": {},  # ë¹„ì–´ìˆìŒ â†’ ìë™ ì²˜ë¦¬
    }
    
    # ì „ì²˜ë¦¬
    base_state = await preprocess_node(base_state)
    
    # ë³€ê²½ ê°ì§€
    base_state = await change_detection_node(base_state)
    
    # ì„ë² ë”© (í•„ìš” ì‹œ)
    if base_state.get("needs_embedding"):
        logger.info("ğŸ“¦ ì„ë² ë”© ì‹¤í–‰")
        base_state = await embedding_node(base_state)
    else:
        logger.info("ğŸ“¦ ì„ë² ë”© ìŠ¤í‚µ (ë³€ê²½ ì—†ìŒ)")
    
    # Step 2: ì œí’ˆ ëª©ë¡ ì¡°íšŒ
    logger.info("\n[Step 2] ì œí’ˆ ëª©ë¡ ì¡°íšŒ")
    product_ids = await fetch_product_ids()
    logger.info(f"  âœ… {len(product_ids)}ê°œ ì œí’ˆ ë°œê²¬")
    
    if not product_ids:
        logger.error("âŒ ì œí’ˆì´ ì—†ìŠµë‹ˆë‹¤")
        return
    
    # Step 3: ì œí’ˆë³„ ë§¤í•‘/ì „ëµ/ë¦¬í¬íŠ¸ ì‹¤í–‰
    logger.info("\n[Step 3] ì œí’ˆë³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
    
    # map_productsë¶€í„° ì‹œì‘í•˜ëŠ” ê·¸ë˜í”„
    app = build_graph(start_node="map_products")
    
    import copy
    results = []
    
    for pid in product_ids:
        logger.info(f"\nâ–¶ï¸ ì œí’ˆ {pid} ì²˜ë¦¬ ì¤‘...")
        
        # State ë³µì‚¬ (ì „ì²˜ë¦¬ ê²°ê³¼ ì¬ì‚¬ìš©)
        per_product_state = copy.deepcopy(base_state)
        per_product_state["mapping_filters"] = {"product_id": pid}
        
        # validation_retry_count ëˆ„ì  ìœ ì§€
        if results:
            last_retry = results[-1].get("validation_retry_count", 0)
            per_product_state["validation_retry_count"] = last_retry
        else:
            per_product_state["validation_retry_count"] = 0
        
        try:
            final_state = await app.ainvoke(per_product_state)
            logger.info(f"âœ… ì œí’ˆ {pid} ì™„ë£Œ")
            results.append(final_state)
        except Exception as e:
            logger.error(f"âŒ ì œí’ˆ {pid} ì‹¤íŒ¨: {e}")
            continue
    
    # Step 4: ì „ì²´ ê²°ê³¼ ìš”ì•½
    logger.info("\n[Step 4] ì „ì²´ ê²°ê³¼ ìš”ì•½")
    logger.info(f"  - ì²˜ë¦¬: {len(results)}/{len(product_ids)}ê°œ ì œí’ˆ")
    logger.info(f"  - ì„±ê³µ: {len(results)}ê°œ")
    logger.info(f"  - ì‹¤íŒ¨: {len(product_ids) - len(results)}ê°œ")
    
    if results:
        logger.info("\në§ˆì§€ë§‰ ì œí’ˆ ìƒì„¸:")
        print_summary(results[-1])
    
    return results


async def run_s3_auto_load(s3_date: str | None, product_id: int | None):
    """S3 ìë™ ë¡œë“œ + íŒŒì´í”„ë¼ì¸ ì‹¤í–‰."""
    logger.info("=" * 80)
    logger.info("ğŸš€ S3 ìë™ ë¡œë“œ íŒŒì´í”„ë¼ì¸")
    logger.info("=" * 80)
    
    # ì´ˆê¸° State
    state: AppState = {
        "preprocess_request": {
            "load_from_s3": True,
            "s3_date": s3_date,  # YYYYMMDD or None (ì˜¤ëŠ˜)
            "use_vision_pipeline": True,
        },
        "change_context": {},
    }
    
    # ë‹¨ì¼ ì œí’ˆ ë˜ëŠ” ì „ì²´ ì œí’ˆ
    if product_id:
        state["mapping_filters"] = {"product_id": product_id}
    
    # ê·¸ë˜í”„ ì‹¤í–‰
    app = build_graph()
    
    try:
        final_state = await app.ainvoke(state)
        logger.info("âœ… S3 ìë™ ë¡œë“œ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        print_summary(final_state)
        return final_state
    except Exception as e:
        logger.error(f"âŒ S3 ìë™ ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)
        return None


async def main():
    parser = argparse.ArgumentParser(
        description="REMON AI Pipeline í´ë¦° ì‹¤í–‰ (í•˜ë“œì½”ë”© ì œê±°)"
    )
    
    # ì‹¤í–‰ ëª¨ë“œ
    parser.add_argument(
        "--mode",
        choices=["single", "all", "s3"],
        default="single",
        help="ì‹¤í–‰ ëª¨ë“œ: single (ë‹¨ì¼ ì œí’ˆ), all (ì „ì²´ ì œí’ˆ), s3 (S3 ìë™ ë¡œë“œ)",
    )
    
    # PDF ì„¤ì •
    parser.add_argument(
        "--pdf",
        default="/tmp/Regulation_Data_B.pdf",
        help="ë¡œì»¬ PDF ê²½ë¡œ (mode=single/all)",
    )
    
    # ì œí’ˆ ì„¤ì •
    parser.add_argument(
        "--product-id",
        type=int,
        help="ì œí’ˆ ID (mode=single)",
    )
    
    parser.add_argument(
        "--all-products",
        action="store_true",
        help="ì „ì²´ ì œí’ˆ ì²˜ë¦¬ (mode=allê³¼ ë™ì¼)",
    )
    
    # S3 ì„¤ì •
    parser.add_argument(
        "--s3-date",
        help="S3 ë‚ ì§œ (YYYYMMDD, mode=s3)",
    )
    
    args = parser.parse_args()
    
    # ëª¨ë“œ ìë™ ê²°ì •
    if args.all_products:
        args.mode = "all"
    
    # ì‹¤í–‰
    if args.mode == "single":
        if not args.product_id:
            logger.error("âŒ --product-id í•„ìˆ˜ (ë‹¨ì¼ ì œí’ˆ ëª¨ë“œ)")
            return
        await run_single_product(args.pdf, args.product_id)
    
    elif args.mode == "all":
        await run_all_products(args.pdf)
    
    elif args.mode == "s3":
        await run_s3_auto_load(args.s3_date, args.product_id)


if __name__ == "__main__":
    asyncio.run(main())
