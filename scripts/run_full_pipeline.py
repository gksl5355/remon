"""
module: run_full_pipeline.py
description: REMON AI Pipeline ì „ì²´ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (S3 PDF â†’ ìµœì¢… ë¦¬í¬íŠ¸)
author: AI Agent
created: 2025-01-19
updated: 2025-01-21 (í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ í†µí•©: traceable + citation_code íŒŒë¼ë¯¸í„°)

ì‹¤í–‰ ë°©ë²•:
    # Legacy ê·œì œ ì „ì²˜ë¦¬ (1íšŒë§Œ)
    python scripts/run_full_pipeline.py --mode legacy

    # New ê·œì œ ì²˜ë¦¬ (ì „ì²´ íŒŒì´í”„ë¼ì¸)
    python scripts/run_full_pipeline.py --mode new
    python scripts/run_full_pipeline.py  # ê¸°ë³¸ê°’ = new
"""

import asyncio
import logging
import sys
import argparse
import copy
from pathlib import Path
from datetime import datetime
from typing import Dict, Any
from sqlalchemy import text

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

from app.ai_pipeline.graph import build_graph
from app.ai_pipeline.preprocess import preprocess_node
from app.ai_pipeline.state import AppState
from app.core.database import AsyncSessionLocal
from langsmith import traceable
from app.core.repositories.product_repository import ProductRepository

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
Path("logs").mkdir(exist_ok=True)
Path("output").mkdir(exist_ok=True)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(
            f'logs/pipeline_run_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        ),
    ],
)
logger = logging.getLogger(__name__)


def print_pipeline_summary(final_state: AppState):
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“‹ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½")
    logger.info("=" * 80)

    # ë³€ê²½ ê°ì§€
    change_summary = final_state.get("change_summary", {})
    if change_summary:
        logger.info(f"\nğŸ” ë³€ê²½ ê°ì§€:")
        logger.info(f"  - ìƒíƒœ: {change_summary.get('status')}")
        logger.info(f"  - ë³€ê²½ ê±´ìˆ˜: {change_summary.get('total_changes', 0)}")
        logger.info(f"  - ê³ ì‹ ë¢°ë„: {change_summary.get('high_confidence_changes', 0)}")

        # ë³€ê²½ ìƒì„¸
        change_results = final_state.get("change_detection_results", [])
        if change_results:
            logger.info(f"\n  ğŸ“ ë³€ê²½ ìƒì„¸ (ìƒìœ„ 5ê°œ):")
            for idx, result in enumerate(change_results[:5], 1):
                if result.get("change_detected"):
                    logger.info(
                        f"    {idx}. [{result.get('section_ref')}] "
                        f"{result.get('change_type')} - {result.get('confidence_level')}"
                    )

    # ë§¤í•‘
    mapping = final_state.get("mapping", {})
    mapping_items = mapping.get("items", [])
    if mapping_items:
        logger.info(f"\nğŸ”— ì œí’ˆ-ê·œì œ ë§¤í•‘:")
        logger.info(f"  - ë§¤í•‘ í•­ëª©: {len(mapping_items)}ê°œ")
        applies_count = sum(1 for item in mapping_items if item.get("applies"))
        logger.info(f"  - ì ìš© ëŒ€ìƒ: {applies_count}ê°œ")

    # ì „ëµ
    strategies = final_state.get("strategies", [])
    if strategies:
        logger.info(f"\nğŸ’¡ ëŒ€ì‘ ì „ëµ:")
        logger.info(f"  - ì „ëµ ê°œìˆ˜: {len(strategies)}ê°œ")
        for i, strategy in enumerate(strategies[:3], 1):
            logger.info(f"  {i}. {strategy[:80]}...")

    # ì˜í–¥ë„
    impact_scores = final_state.get("impact_scores", [])
    if impact_scores:
        impact = impact_scores[0]
        logger.info(f"\nğŸ“Š ì˜í–¥ë„ í‰ê°€:")
        logger.info(f"  - ì˜í–¥ë„: {impact.get('impact_level')}")
        logger.info(f"  - ì ìˆ˜: {impact.get('weighted_score'):.2f}")

    # ë¦¬í¬íŠ¸
    report = final_state.get("report", {})
    if report:
        logger.info(f"\nğŸ“‹ ìµœì¢… ë¦¬í¬íŠ¸:")
        logger.info(f"  - ìƒì„± ì‹œê°: {report.get('generated_at')}")
        logger.info(f"  - ì„¹ì…˜ ìˆ˜: {len(report.get('sections', []))}")
        logger.info(f"  - Report ID: {report.get('report_id')}")

    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
    logger.info("=" * 80)


async def download_pdf_from_s3(s3_key: str, local_path: str) -> str:
    """S3ì—ì„œ PDF ë‹¤ìš´ë¡œë“œ"""
    import boto3

    logger.info(f"ğŸ“¥ S3ì—ì„œ PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {s3_key}")

    s3_client = boto3.client("s3")
    bucket = "arn:aws:s3:ap-northeast-2:881490135253:accesspoint/sk-team-storage"

    Path(local_path).parent.mkdir(parents=True, exist_ok=True)
    s3_client.download_file(bucket, s3_key, local_path)
    logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_path}")
    return local_path


async def run_legacy_preprocessing():
    """Legacy ê·œì œ ì „ì²˜ë¦¬ ë° DB ì €ì¥ (1íšŒë§Œ ì‹¤í–‰)"""

    # í…ŒìŠ¤íŠ¸ìš© í•˜ë“œì½”ë”© ì„¤ì •
    legacy_s3_key = "skala2/skala-2.4.17/regulation/US/Regulation Data A (1).pdf"
    local_legacy_path = "/tmp/Regulation_Data_A.pdf"

    logger.info("=" * 80)
    logger.info("ğŸ”§ Legacy ê·œì œ ì „ì²˜ë¦¬ ëª¨ë“œ")
    logger.info("=" * 80)

    # Step 1: S3ì—ì„œ Legacy PDF ë‹¤ìš´ë¡œë“œ
    try:
        logger.info("\n[Step 1] S3ì—ì„œ Legacy ê·œì œ PDF ë‹¤ìš´ë¡œë“œ")
        await download_pdf_from_s3(legacy_s3_key, local_legacy_path)
        logger.info(f"   âœ… Legacy: {local_legacy_path}")
    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # Step 2: Legacy ì „ì²˜ë¦¬
    logger.info("\n[Step 2] Legacy ê·œì œ ì „ì²˜ë¦¬ (Vision Pipeline)")
    from app.ai_pipeline.preprocess.vision_orchestrator import VisionOrchestrator

    orchestrator = VisionOrchestrator()
    legacy_result = await orchestrator.process_pdf_async(
        local_legacy_path, use_parallel=True, language_code=None
    )

    if legacy_result["status"] != "success":
        logger.error("âŒ Legacy ì „ì²˜ë¦¬ ì‹¤íŒ¨")
        return

    logger.info(
        f"  âœ… Legacy ì „ì²˜ë¦¬ ì™„ë£Œ: {len(legacy_result['vision_extraction_result'])}í˜ì´ì§€"
    )

    # Step 3: DB ì €ì¥
    logger.info("\n[Step 3] PostgreSQL DB ì €ì¥")
    from app.core.repositories.regulation_repository import RegulationRepository

    regulation_id = None
    async with AsyncSessionLocal() as session:
        repo = RegulationRepository()
        try:
            legacy_reg = await repo.create_from_vision_result(session, legacy_result)
            await session.commit()
            regulation_id = legacy_reg.regulation_id
            logger.info(
                f"  âœ… Legacy ì €ì¥ ì™„ë£Œ: regulation_id={regulation_id}"
            )
            logger.info(f"  âœ… citation_code: {legacy_reg.citation_code}")
        except Exception as e:
            await session.rollback()
            logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback

            traceback.print_exc()
            return

    # Step 4: ì„ë² ë”© (Qdrant ì €ì¥)
    logger.info("\n[Step 4] ì„ë² ë”© ë° VectorDB ì €ì¥")
    from app.ai_pipeline.preprocess.semantic_processing import DualIndexer

    chunks = legacy_result.get("chunks", [])
    graph_data = legacy_result.get("graph_data", {"nodes": [], "edges": []})
    vision_results = legacy_result.get("vision_extraction_result", [])

    if chunks:
        indexer = DualIndexer()
        index_summary = indexer.index(
            chunks=chunks,
            graph_data=graph_data,
            source_file=Path(local_legacy_path).name,
            regulation_id=regulation_id,
            vision_results=vision_results
        )
        logger.info(f"  âœ… ì„ë² ë”© ì™„ë£Œ: {index_summary.get('qdrant_chunks', 0)}ê°œ ì²­í¬")
    else:
        logger.warning("  âš ï¸ ì²­í¬ ì—†ìŒ, ì„ë² ë”© ìŠ¤í‚µ")

    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ Legacy ê·œì œ ì „ì²˜ë¦¬ ì™„ë£Œ (ì„ë² ë”© í¬í•¨)!")
    logger.info("=" * 80)


@traceable(name="REMON_Full_Pipeline", run_type="chain")
async def run_full_pipeline(citation_code: str = None):
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (S3 ìë™ ë¡œë“œ + LangGraph)
    
    Args:
        citation_code: ê·œì œ ì‹ë³„ ì½”ë“œ (Noneì´ë©´ ì „ì²˜ë¦¬ì—ì„œ ìë™ ì¶”ì¶œ)
    """

    logger.info("=" * 80)
    logger.info("ğŸš€ REMON AI Pipeline ì „ì²´ ì‹¤í–‰ ì‹œì‘")
    logger.info("=" * 80)

    # Step 1: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (S3 ìë™ ë¡œë“œ + ë™ì  í•„í„°ë§)
    logger.info("\n[Step 1] ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
    logger.info("  â„¹ï¸ S3ì—ì„œ ì˜¤ëŠ˜ ì—…ë¡œë“œëœ íŒŒì¼ ìë™ ë¡œë“œ")
    logger.info("  â„¹ï¸ ì „ì²˜ë¦¬ì—ì„œ citation_code ìë™ ì¶”ì¶œ")
    logger.info("  â„¹ï¸ change_detection_nodeì—ì„œ Legacy ìë™ ê²€ìƒ‰")
    logger.info("  â„¹ï¸ êµ­ê°€ ì •ë³´ë¡œ ì œí’ˆ ìë™ í•„í„°ë§")

    initial_state: AppState = {
        "preprocess_request": {
            "load_from_s3": True,
            "s3_date": None,
            "use_vision_pipeline": True,
            "enable_change_detection": True,
        },
        "change_context": {},  # ì „ì²˜ë¦¬ í›„ ìë™ ì±„ì›Œì§
        "mapping_filters": {},
        "validation_retry_count": 0,
    }

    app = build_graph()
    
    try:
        final_state = await app.ainvoke(initial_state, config={"configurable": {}})
        logger.info("âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}", exc_info=True)
        return

    if final_state:
        logger.info("\n[Step 4] ì‹¤í–‰ ê²°ê³¼ ìš”ì•½")
        print_pipeline_summary(final_state)

    return final_state


async def main():
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description="REMON AI Pipeline ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument(
        "--mode",
        choices=["legacy", "new"],
        default="new",
        help="ì‹¤í–‰ ëª¨ë“œ: legacy (Legacy ì „ì²˜ë¦¬ë§Œ), new (ì „ì²´ íŒŒì´í”„ë¼ì¸)",
    )
    parser.add_argument(
        "--citation-code",
        default=None,
        help="(ì„ íƒ) ê·œì œ ì‹ë³„ìš© citation_code (ë¯¸ì§€ì • ì‹œ ì „ì²˜ë¦¬ì—ì„œ ìë™ ì¶”ì¶œ)"
    )
    args = parser.parse_args()

    if args.mode == "legacy":
        await run_legacy_preprocessing()
    else:
        await run_full_pipeline(citation_code=args.citation_code)


if __name__ == "__main__":
    asyncio.run(main())
