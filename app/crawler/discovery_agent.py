import asyncio
from typing import List, Dict
from tavily import TavilyClient
from app.core.repositories.crawl_repository import CrawlRepository
from app.crawler.crawling_regulation.base import UniversalFetcher

class DiscoveryAgent:
    def __init__(self, db_session, tavily_api_key: str):
        self.repository = CrawlRepository(db_session)
        self.tavily_client = None
        if tavily_api_key:
            self.tavily_client = TavilyClient(api_key=tavily_api_key)

    async def search_tavily(self, query: str) -> List[Dict]:
        if not self.tavily_client:
            return [] # Mock data ìƒëžµ

        print(f"ðŸ”Ž [Tavily] íƒìƒ‰ ì¤‘: '{query}'")
        try:
            response = await asyncio.to_thread(
                self.tavily_client.search,
                query=query,
                search_depth="basic",
                include_answer=False,
                include_raw_content=False,
                max_results=10  # [ìˆ˜ì •] 5ê°œ -> 10ê°œë¡œ ì¦ê°€ (ë” ë§Žì€ ê²°ê³¼)
            )
            return response.get('results', [])
        except Exception as e:
            print(f"âŒ Tavily API ì˜¤ë¥˜: {e}")
            return []

    async def run(self, country: str, keywords: List[str], category: str = "regulation"):
        """
        [ìˆ˜ì •] category ì¸ìž ì¶”ê°€ (regulation ë˜ëŠ” news)
        """
        # "news" ì¹´í…Œê³ ë¦¬ë©´ PDF í•œì • ê²€ìƒ‰ì„ í’€ì–´ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ë„ ë‚˜ì˜¤ê²Œ í•¨
        if category == "news":
            query = f"{country} {' '.join(keywords)}"
        else:
            # ê·œì œëŠ” PDF ìœ„ì£¼ë¡œ ê²€ìƒ‰
            query = f"{country} {' '.join(keywords)} filetype:pdf"
        
        results = await self.search_tavily(query)
        
        if not results:
            print(f"   ðŸ’¨ ê²°ê³¼ ì—†ìŒ")
            return

        print(f"   âœ¨ {len(results)}ê±´ ë°œê²¬ ({category})")
        crawler = UniversalFetcher()
        
        try:
            for item in results:
                data = {
                    "url": item.get('url'),
                    "hash_value": crawler.generate_hash(item.get('url')),
                    "country_code": self._map_country_code(country),
                    "title": item.get('title'),
                    "proclaimed_date": None,
                    "source_id": 99,
                    "category": category  # [ì¤‘ìš”] ì¹´í…Œê³ ë¦¬ ì „ë‹¬
                }
                
                await self.repository.process_crawled_data(data, crawler)
                
        finally:
            await crawler.close()

    def _map_country_code(self, country_name: str) -> str:
        # ë§¤í•‘ ë¡œì§ (ë¯¸êµ­ ì—°ë°©, ìº˜ë¦¬í¬ë‹ˆì•„ ë“±ì„ USë¡œ í†µì¼í• ì§€, ë‚˜ëˆŒì§€ ê²°ì •)
        if "USA" in country_name: return "US"
        if "Russia" in country_name: return "RU"
        if "Indonesia" in country_name: return "ID"
        return "ZZ"

# import asyncio
# from typing import List, Dict
# from tavily import TavilyClient

# # ê¸°ì¡´ ëª¨ë“ˆ ìž„í¬íŠ¸ ìœ ì§€
# from app.core.repositories.crawl_repository import CrawlRepository
# from app.crawler.crawling_regulation.base import UniversalFetcher

# class DiscoveryAgent:
#     """
#     [Tavily ê¸°ë°˜ ë²”ìš© ê·œì œ íƒì§€ ì—ì´ì „íŠ¸]
#     Google Search API ëŒ€ì‹  Tavilyë¥¼ ì‚¬ìš©í•˜ì—¬
#     ê²€ìƒ‰ + ë³¸ë¬¸ ì¶”ì¶œ + ë…¸ì´ì¦ˆ ì œê±°ë¥¼ í•œ ë²ˆì— ìˆ˜í–‰í•©ë‹ˆë‹¤.
#     """
#     def __init__(self, db_session, tavily_api_key: str):
#         self.repository = CrawlRepository(db_session)
#         self.tavily_client = None
        
#         if tavily_api_key:
#             self.tavily_client = TavilyClient(api_key=tavily_api_key)
#         else:
#             print("âš ï¸ Tavily API Keyê°€ ì—†ìŠµë‹ˆë‹¤. (Mock ëª¨ë“œ)")

#     async def search_tavily(self, query: str) -> List[Dict]:
#         """
#         Tavily APIë¥¼ ì‚¬ìš©í•˜ì—¬ 'ê³ í’ˆì§ˆ' ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜
#         """
#         if not self.tavily_client:
#             # Mock ë°ì´í„° (í…ŒìŠ¤íŠ¸ìš©)
#             return [{
#                 "url": "https://eec.eaeunion.org/upload/medialibrary/1ad/TR-TS-035-2014.pdf",
#                 "content": "Technical regulation on tobacco products TR CU 035/2014 full text...",
#                 "title": "[Mock] Russia Tobacco Regulation PDF"
#             }]

#         print(f"ðŸ”Ž [Tavily] ê·œì œ íƒìƒ‰ ì¤‘: '{query}'")
        
#         # Tavilyì˜ ê°•ë ¥í•œ ê¸°ëŠ¥: search_depth="advanced"ë¥¼ ì“°ë©´ ë” ê¹Šê²Œ ì°¾ì§€ë§Œ í¬ë ˆë”§ 2ë°° ì†Œëª¨
#         # ë¬´ë£Œ í‹°ì–´ ì•„ë¼ê¸° ìœ„í•´ "basic" ì‚¬ìš© ê¶Œìž¥
#         try:
#             # ë¹„ë™ê¸° ì‹¤í–‰ì„ ìœ„í•´ to_thread ì‚¬ìš© (Tavily SDKëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë™ê¸°)
#             response = await asyncio.to_thread(
#                 self.tavily_client.search,
#                 query=query,
#                 search_depth="basic", # advancedëŠ” í¬ë ˆë”§ ì†Œëª¨ í¼. basic ì¶”ì²œ.
#                 include_answer=False, # ë‹µë³€ ìƒì„± ë¶ˆí•„ìš” (í† í° ì ˆì•½)
#                 include_raw_content=False,
#                 max_results=5 # ìƒìœ„ 5ê°œë§Œ í™•ì¸ (ì ˆì•½)
#             )
#             return response.get('results', [])
#         except Exception as e:
#             print(f"âŒ Tavily API ì˜¤ë¥˜: {e}")
#             return []

#     async def run(self, country: str, keywords: List[str]):
#         """
#         [ì‹¤í–‰ ë¡œì§]
#         1. Tavilyë¡œ ê²€ìƒ‰í•˜ì—¬ [ì œëª©, URL, ë³¸ë¬¸ìš”ì•½]ì„ ë°›ì•„ì˜´
#         2. PDF íŒŒì¼ì´ê±°ë‚˜, ì œëª©ì— 'Regulation'ì´ í¬í•¨ëœ ì¤‘ìš” ë§í¬ë§Œ ì„ ë³„
#         3. CrawlRepositoryë¡œ ë‹¤ìš´ë¡œë“œ (PDFëŠ” íŒŒì¼ë¡œ, ì›¹ì€ í…ìŠ¤íŠ¸ë¡œ)
#         """
#         # ê²€ìƒ‰ì–´ ìµœì í™”: "filetype:pdf"ë¥¼ ë¶™ì´ë©´ Tavilyê°€ PDFë¥¼ ìž˜ ì°¾ì•„ì¤Œ
#         query = f"{country} {' '.join(keywords)} filetype:pdf"
        
#         # 1. Tavily ê²€ìƒ‰
#         results = await self.search_tavily(query)
        
#         if not results:
#             print(f"   ðŸ’¨ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
#             return

#         print(f"   âœ¨ Tavilyê°€ {len(results)}ê°œì˜ í›„ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

#         # 2. ë‹¤ìš´ë¡œë“œ ë° ì €ìž¥
#         crawler = UniversalFetcher()
#         try:
#             for item in results:
#                 url = item.get('url')
#                 title = item.get('title')
#                 content = item.get('content') # Tavilyê°€ ê¸ì–´ì˜¨ ë³¸ë¬¸ ì¼ë¶€

#                 # [í•„í„°ë§] TavilyëŠ” ì´ë¯¸ ê´€ë ¨ì„± ë†’ì€ê±¸ ì£¼ì§€ë§Œ, í•œ ë²ˆ ë” ì²´í¬
#                 # ë§Œì•½ content ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ê´‘ê³  ê°™ìœ¼ë©´ skip í•˜ëŠ” ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
                
#                 print(f"   ðŸ“¥ ìˆ˜ì§‘ ì‹œë„: {title}")

#                 data = {
#                     "url": url,
#                     "hash_value": crawler.generate_hash(url),
#                     "country_code": self._map_country_code(country),
#                     "title": title,
#                     "proclaimed_date": None, # ë©”íƒ€ë°ì´í„°
#                     "source_id": 99, # Global Discovery Source
#                     "summary_preview": content # (ì˜µì…˜) Tavilyê°€ ì¤€ ìš”ì•½ì„ ì €ìž¥í•˜ê³  ì‹¶ë‹¤ë©´
#                 }

#                 # Repositoryê°€ URLì— ì ‘ì†í•´ì„œ ì‹¤ì œ íŒŒì¼(PDF)ì„ ë‹¤ìš´ë¡œë“œí•¨
#                 await self.repository.process_crawled_data(data, crawler)
                
#         finally:
#             await crawler.close()

#     def _map_country_code(self, country_name: str) -> str:
#         mapping = {
#             "Russia": "RU", "Indonesia": "ID", "USA": "US", "Vietnam": "VN", "Korea": "KR"
#         }
#         return mapping.get(country_name, "ZZ")
