import asyncio
from typing import List, Dict
from tavily import TavilyClient
from app.core.repositories.crawl_repository import CrawlRepository
from app.crawler.crawling_regulation.base import UniversalFetcher

class DiscoveryAgent:
    def __init__(self, db_session, tavily_api_key: str):
        self.repository = CrawlRepository(db_session)
        self.tavily_client = None
        if tavily_api_key:
            self.tavily_client = TavilyClient(api_key=tavily_api_key)

    async def search_tavily(self, query: str) -> List[Dict]:
        if not self.tavily_client:
            return [] # Mock data ìƒëµ

        print(f"ğŸ” [Tavily] íƒìƒ‰ ì¤‘: '{query}'")
        try:
            response = await asyncio.to_thread(
                self.tavily_client.search,
                query=query,
                search_depth="basic",
                include_answer=False,
                include_raw_content=False,
                max_results=10  # [ìˆ˜ì •] 5ê°œ -> 10ê°œë¡œ ì¦ê°€ (ë” ë§ì€ ê²°ê³¼)
            )
            return response.get('results', [])
        except Exception as e:
            print(f"âŒ Tavily API ì˜¤ë¥˜: {e}")
            return []

    async def run(self, country: str, keywords: List[str], category: str = "regulation"):
        """
        [ìˆ˜ì •] category ì¸ì ì¶”ê°€ (regulation ë˜ëŠ” news)
        """
        # "news" ì¹´í…Œê³ ë¦¬ë©´ PDF í•œì • ê²€ìƒ‰ì„ í’€ì–´ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ë„ ë‚˜ì˜¤ê²Œ í•¨
        if category == "news":
            query = f"{country} {' '.join(keywords)}"
        else:
            # ê·œì œëŠ” PDF ìœ„ì£¼ë¡œ ê²€ìƒ‰
            query = f"{country} {' '.join(keywords)} filetype:pdf"
        
        results = await self.search_tavily(query)
        
        if not results:
            print(f"   ğŸ’¨ ê²°ê³¼ ì—†ìŒ")
            return

        print(f"   âœ¨ {len(results)}ê±´ ë°œê²¬ ({category})")
        crawler = UniversalFetcher()
        
        try:
            for item in results:
                data = {
                    "url": item.get('url'),
                    "hash_value": crawler.generate_hash(item.get('url')),
                    "country_code": self._map_country_code(country),
                    "title": item.get('title'),
                    "proclaimed_date": None,
                    "source_id": 99,
                    "category": category  # [ì¤‘ìš”] ì¹´í…Œê³ ë¦¬ ì „ë‹¬
                }
                
                await self.repository.process_crawled_data(data, crawler)
                
        finally:
            await crawler.close()

    def _map_country_code(self, country_name: str) -> str:
        # ë§¤í•‘ ë¡œì§ (ë¯¸êµ­ ì—°ë°©, ìº˜ë¦¬í¬ë‹ˆì•„ ë“±ì„ USë¡œ í†µì¼í• ì§€, ë‚˜ëˆŒì§€ ê²°ì •)
        if "USA" in country_name: return "US"
        if "Russia" in country_name: return "RU"
        if "Indonesia" in country_name: return "ID"
        return "ZZ"

