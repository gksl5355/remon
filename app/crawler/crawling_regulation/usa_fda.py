import re
from bs4 import BeautifulSoup
from app.crawler.crawling_regulation.base import BaseCrawler
from typing import List, Dict, Any
from datetime import datetime
from dateutil import parser

class USAFDACrawler(BaseCrawler):
    # FDA ê·œì œ ëª©ë¡ í˜ì´ì§€
    TARGET_URL = "https://www.fda.gov/tobacco-products/rules-regulations-and-guidance/rules-and-regulations"

    async def run(self) -> List[Dict[str, Any]]:
        print(f"ğŸ‡ºğŸ‡¸ [US] FDA í¬ë¡¤ë§ ì‹œì‘: {self.TARGET_URL}")
        
        html = await self.fetch(self.TARGET_URL)
        if not html:
            return []

        results = await self.parse(html, self.TARGET_URL)
        return results

    async def parse(self, html: str, url: str) -> List[Dict[str, Any]]:
        soup = BeautifulSoup(html, "lxml")
        results = []

        rows = soup.select("table tbody tr")
        if not rows:
            rows = soup.select(".views-row")

        if not rows:
            print("âš ï¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. HTML êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return []

        for row in rows:
            try:
                # 1. ì œëª© ë° ë§í¬ ì¶”ì¶œ
                link_tag = row.select_one("a")
                if not link_tag:
                    continue
                
                title = link_tag.get_text(strip=True)
                href = link_tag.get("href")
                
                if href.startswith("/"):
                    full_url = f"https://www.fda.gov{href}"
                else:
                    full_url = href

                # 2. ë‚ ì§œ ì¶”ì¶œ ë¡œì§ (ê°œì„ ë¨)
                date_str = None

                # [ì „ëµ 1] URLì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ê°€ì¥ ì •í™•í•¨)
                # íŒ¨í„´: .../documents/YYYY/MM/DD/...
                url_date_match = re.search(r'/documents/(\d{4})/(\d{2})/(\d{2})/', full_url)
                if url_date_match:
                    y, m, d = url_date_match.groups()
                    date_str = f"{y}-{m}-{d}"
                
                # [ì „ëµ 2] HTML í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (URLì— ì—†ì„ ê²½ìš°)
                if not date_str:
                    row_text = row.get_text(" ", strip=True)
                    # ì •ê·œì‹: MM/DD/YYYY ë˜ëŠ” Month DD, YYYY
                    text_date_match = re.search(r'(\d{2}/\d{2}/\d{4})|([A-Z][a-z]+ \d{1,2}, \d{4})', row_text)
                    if text_date_match:
                        try:
                            dt = parser.parse(text_date_match.group(0))
                            date_str = dt.strftime("%Y-%m-%d")
                        except:
                            pass

                # [ì „ëµ 3] Fallback (ì˜¤ëŠ˜ ë‚ ì§œ)
                if not date_str:
                    # ë‚ ì§œë¥¼ ëª» ì°¾ì•˜ë‹¤ëŠ” ê²ƒì„ ì•Œë¦¬ê¸° ìœ„í•´ ë¡œê·¸ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                    # print(f"âš ï¸ ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨ (ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš©): {title[:30]}...") 
                    date_str = datetime.now().strftime("%Y-%m-%d")

                # 3. í•´ì‹œ ìƒì„±
                unique_content = f"{title}{full_url}"
                content_hash = self.generate_hash(unique_content)

                data = {
                    "country_code": "US",
                    "title": title,
                    "url": full_url,
                    "proclaimed_date": date_str,
                    "hash_value": content_hash,
                    "source_type": "html"
                }
                results.append(data)

            except Exception as e:
                print(f"âš ï¸ Parse Error: {e}")
                continue

        print(f"âœ… [US] {len(results)}ê±´ì˜ ê·œì œ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
        return results


# import re
# from bs4 import BeautifulSoup
# from app.crawler.base import BaseCrawler
# from typing import List, Dict, Any
# from datetime import datetime
# from dateutil import parser  # ë‚ ì§œ íŒŒì‹± ë¼ì´ë¸ŒëŸ¬ë¦¬

# class USAFDACrawler(BaseCrawler):
#     # FDA ê·œì œ ëª©ë¡ í˜ì´ì§€
#     TARGET_URL = "https://www.fda.gov/tobacco-products/rules-regulations-and-guidance/rules-and-regulations"

#     async def run(self) -> List[Dict[str, Any]]:
#         print(f"ğŸ‡ºğŸ‡¸ [US] FDA í¬ë¡¤ë§ ì‹œì‘: {self.TARGET_URL}")
        
#         html = await self.fetch(self.TARGET_URL)
#         if not html:
#             return []

#         results = await self.parse(html, self.TARGET_URL)
#         return results

#     async def parse(self, html: str, url: str) -> List[Dict[str, Any]]:
#         soup = BeautifulSoup(html, "lxml")
#         results = []

#         # FDA í˜ì´ì§€ëŠ” ë³´í†µ í…Œì´ë¸”(table) ì•ˆì— ê·œì œ ëª©ë¡ì´ ìˆìŠµë‹ˆë‹¤.
#         rows = soup.select("table tbody tr")
        
#         # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ(.views-row)ì¼ ìˆ˜ë„ ìˆìŒ
#         if not rows:
#             rows = soup.select(".views-row")

#         if not rows:
#             print("âš ï¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. HTML êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
#             return []

#         for row in rows:
#             try:
#                 # 1. ì œëª© ë° ë§í¬ ì¶”ì¶œ
#                 link_tag = row.select_one("a")
#                 if not link_tag:
#                     continue
                
#                 title = link_tag.get_text(strip=True)
#                 href = link_tag.get("href")
                
#                 if href.startswith("/"):
#                     full_url = f"https://www.fda.gov{href}"
#                 else:
#                     full_url = href

#                 # 2. ë‚ ì§œ ì¶”ì¶œ (Date Extraction) - ì—¬ê¸°ê°€ í•µì‹¬ ê°œì„  í¬ì¸íŠ¸
#                 date_str = None
                
#                 # ì „ëµ A: <time> íƒœê·¸ ì°¾ê¸°
#                 time_tag = row.select_one("time")
#                 if time_tag and time_tag.get("datetime"):
#                     date_str = time_tag.get("datetime")
                
#                 # ì „ëµ B: í…Œì´ë¸” ì»¬ëŸ¼(td) í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ í˜•ì‹ ì°¾ê¸°
#                 if not date_str:
#                     # ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™€ì„œ ë‚ ì§œ íŒ¨í„´ ê²€ìƒ‰ (ì˜ˆ: 01/23/2024 or Jan 23, 2024)
#                     row_text = row.get_text(" ", strip=True)
#                     # ì •ê·œì‹: ì›”/ì¼/ë…„ ë˜ëŠ” ì˜ë¬¸ì›” ì¼, ë…„
#                     date_match = re.search(r'(\d{2}/\d{2}/\d{4})|([A-Z][a-z]+ \d{1,2}, \d{4})', row_text)
                    
#                     if date_match:
#                         raw_date = date_match.group(0)
#                         try:
#                             # "November 26, 2025" -> "2025-11-26" ë³€í™˜
#                             dt = parser.parse(raw_date)
#                             date_str = dt.strftime("%Y-%m-%d")
#                         except:
#                             pass

#                 # ì „ëµ C: ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œ (Fallback)
#                 if not date_str:
#                     date_str = datetime.now().strftime("%Y-%m-%d")

#                 # 3. í•´ì‹œ ìƒì„±
#                 unique_content = f"{title}{full_url}"
#                 content_hash = self.generate_hash(unique_content)

#                 data = {
#                     "country_code": "US",
#                     "title": title,
#                     "url": full_url,
#                     "proclaimed_date": date_str,  # ì¶”ì¶œí•œ ì‹¤ì œ ë‚ ì§œ
#                     "hash_value": content_hash,
#                     "source_type": "html"
#                 }
#                 results.append(data)

#             except Exception as e:
#                 print(f"âš ï¸ Parse Error: {e}")
#                 continue

#         print(f"âœ… [US] {len(results)}ê±´ì˜ ê·œì œ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
#         return results

# # app/crawler/usa_fda.py

# from bs4 import BeautifulSoup
# from app.crawler.base import BaseCrawler
# from typing import List, Dict, Any
# from datetime import datetime

# class USAFDACrawler(BaseCrawler):
#     TARGET_URL = "https://www.fda.gov/tobacco-products/rules-regulations-and-guidance/rules-and-regulations"

#     async def run(self) -> List[Dict[str, Any]]:
#         print(f"ğŸ‡ºğŸ‡¸ [US] FDA í¬ë¡¤ë§ ì‹œì‘: {self.TARGET_URL}")
        
#         html = await self.fetch(self.TARGET_URL)
#         if not html:
#             return []

#         results = await self.parse(html, self.TARGET_URL)
#         return results

#     async def parse(self, html: str, url: str) -> List[Dict[str, Any]]:
#         soup = BeautifulSoup(html, "lxml")
#         results = []

#         # [ìˆ˜ì •] ì„ íƒì ì „ëµ ë³€ê²½
#         # ì „ëµ 1: í…Œì´ë¸” êµ¬ì¡° (tbody > tr) í™•ì¸
#         rows = soup.select("table tbody tr")
        
#         # ì „ëµ 2: í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° (.views-row) í™•ì¸
#         if not rows:
#             rows = soup.select(".views-row")

#         # ë””ë²„ê¹…: ì—¬ì „íˆ ëª» ì°¾ìœ¼ë©´ HTML ì¼ë¶€ ì¶œë ¥
#         if not rows:
#             print("âš ï¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. HTML êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
#             # body íƒœê·¸ ë‚´ë¶€ì˜ ì²˜ìŒ 500ìë§Œ ì¶œë ¥í•´ì„œ í™•ì¸
#             body = soup.find("body")
#             print(f"ğŸ” HTML Preview: {body.get_text()[:200] if body else 'No Body'}")
#             return []

#         for row in rows:
#             try:
#                 # 1. ë§í¬(a íƒœê·¸) ì°¾ê¸°
#                 link_tag = row.select_one("a")
#                 if not link_tag:
#                     continue
                
#                 title = link_tag.get_text(strip=True)
#                 href = link_tag.get("href")
                
#                 # ë§í¬ ì •ì œ
#                 if href.startswith("/"):
#                     full_url = f"https://www.fda.gov{href}"
#                 else:
#                     full_url = href

#                 # 2. ë‚ ì§œ ì°¾ê¸° (time íƒœê·¸ ë˜ëŠ” í…Œì´ë¸”ì˜ íŠ¹ì • ì—´)
#                 # í…Œì´ë¸” êµ¬ì¡°ì¼ ê²½ìš° ë³´í†µ ë‚ ì§œ ì—´ì´ ë”°ë¡œ ìˆìŒ (ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”)
#                 date_tag = row.select_one("time")
#                 if date_tag:
#                     date_str = date_tag.get("datetime")
#                 else:
#                     # ë‚ ì§œ íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œ ì„ì‹œ ì‚¬ìš© (ì¶”í›„ ì •ë°€ íŒŒì‹± í•„ìš”)
#                     date_str = datetime.now().strftime("%Y-%m-%d")

#                 # 3. í•´ì‹œ ìƒì„±
#                 unique_content = f"{title}{full_url}"
#                 content_hash = self.generate_hash(unique_content)

#                 data = {
#                     "country_code": "US",
#                     "title": title,
#                     "url": full_url,
#                     "proclaimed_date": date_str,
#                     "hash_value": content_hash,
#                     "source_type": "html"
#                 }
#                 results.append(data)

#             except Exception as e:
#                 print(f"âš ï¸ Parse Error: {e}")
#                 continue

#         print(f"âœ… [US] {len(results)}ê±´ì˜ ê·œì œ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
#         return results

