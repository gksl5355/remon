import re
from bs4 import BeautifulSoup
from app.crawler.crawling_regulation.base import BaseCrawler
from typing import List, Dict, Any
from datetime import datetime
from dateutil import parser

class USAFDACrawler(BaseCrawler):
    # FDA ê·œì œ ëª©ë¡ í˜ì´ì§€
    TARGET_URL = "https://www.fda.gov/tobacco-products/rules-regulations-and-guidance/rules-and-regulations"

    async def run(self) -> List[Dict[str, Any]]:
        print(f"ğŸ‡ºğŸ‡¸ [US] FDA í¬ë¡¤ë§ ì‹œì‘: {self.TARGET_URL}")
        
        html = await self.fetch(self.TARGET_URL)
        if not html:
            return []

        results = await self.parse(html, self.TARGET_URL)
        return results

    async def parse(self, html: str, url: str) -> List[Dict[str, Any]]:
        soup = BeautifulSoup(html, "lxml")
        results = []

        rows = soup.select("table tbody tr")
        if not rows:
            rows = soup.select(".views-row")

        if not rows:
            print("âš ï¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. HTML êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return []

        for row in rows:
            try:
                # 1. ì œëª© ë° ë§í¬ ì¶”ì¶œ
                link_tag = row.select_one("a")
                if not link_tag:
                    continue
                
                title = link_tag.get_text(strip=True)
                href = link_tag.get("href")
                
                if href.startswith("/"):
                    full_url = f"https://www.fda.gov{href}"
                else:
                    full_url = href

                # 2. ë‚ ì§œ ì¶”ì¶œ ë¡œì§ (ê°œì„ ë¨)
                date_str = None

                # [ì „ëµ 1] URLì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ê°€ì¥ ì •í™•í•¨)
                # íŒ¨í„´: .../documents/YYYY/MM/DD/...
                url_date_match = re.search(r'/documents/(\d{4})/(\d{2})/(\d{2})/', full_url)
                if url_date_match:
                    y, m, d = url_date_match.groups()
                    date_str = f"{y}-{m}-{d}"
                
                # [ì „ëµ 2] HTML í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (URLì— ì—†ì„ ê²½ìš°)
                if not date_str:
                    row_text = row.get_text(" ", strip=True)
                    # ì •ê·œì‹: MM/DD/YYYY ë˜ëŠ” Month DD, YYYY
                    text_date_match = re.search(r'(\d{2}/\d{2}/\d{4})|([A-Z][a-z]+ \d{1,2}, \d{4})', row_text)
                    if text_date_match:
                        try:
                            dt = parser.parse(text_date_match.group(0))
                            date_str = dt.strftime("%Y-%m-%d")
                        except:
                            pass

                # [ì „ëµ 3] Fallback (ì˜¤ëŠ˜ ë‚ ì§œ)
                if not date_str:
                    # ë‚ ì§œë¥¼ ëª» ì°¾ì•˜ë‹¤ëŠ” ê²ƒì„ ì•Œë¦¬ê¸° ìœ„í•´ ë¡œê·¸ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                    # print(f"âš ï¸ ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨ (ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš©): {title[:30]}...") 
                    date_str = datetime.now().strftime("%Y-%m-%d")

                # 3. í•´ì‹œ ìƒì„±
                unique_content = f"{title}{full_url}"
                content_hash = self.generate_hash(unique_content)

                data = {
                    "country_code": "US",
                    "title": title,
                    "url": full_url,
                    "proclaimed_date": date_str,
                    "hash_value": content_hash,
                    "source_type": "html"
                }
                results.append(data)

            except Exception as e:
                print(f"âš ï¸ Parse Error: {e}")
                continue

        print(f"âœ… [US] {len(results)}ê±´ì˜ ê·œì œ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
        return results


