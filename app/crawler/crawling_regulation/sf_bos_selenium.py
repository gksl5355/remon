import time
from bs4 import BeautifulSoup
from typing import List, Dict, Any
from datetime import datetime
from dateutil import parser

# Selenium ê´€ë ¨ ì„í¬íŠ¸
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# BaseCrawler ìƒì† (hash ìƒì„± ë“± ìœ í‹¸ë¦¬í‹° ì‚¬ìš©ì„ ìœ„í•´)
from app.crawler.crawling_regulation.base import BaseCrawler

class SFBOSSeleniumCrawler(BaseCrawler):
    TARGET_URL = "https://sfbos.org/all-pages-docs?as_q=cigarette&cof=FORID%3A11&ie=UTF-8"

    def __init__(self):
        super().__init__()
        # 1. í¬ë¡¬ ì˜µì…˜ ì„¤ì • (WSL/ì„œë²„ í™˜ê²½ ìµœì í™”)
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # í™”ë©´ ì—†ì´ ì‹¤í–‰
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36")

        # 2. ë“œë¼ì´ë²„ ì„¤ì •
        self.driver = webdriver.Chrome(
            service=Service(ChromeDriverManager().install()),
            options=chrome_options
        )

    async def run(self) -> List[Dict[str, Any]]:
        print(f"ğŸ‡ºğŸ‡¸ [SF] San Francisco BOS (Selenium) ì‹œì‘: {self.TARGET_URL}")
        
        try:
            # 3. í˜ì´ì§€ ì ‘ì†
            self.driver.get(self.TARGET_URL)
            
            # 4. ìë°”ìŠ¤í¬ë¦½íŠ¸ ë¡œë”© ëŒ€ê¸° (ê²€ìƒ‰ ê²°ê³¼ê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°)
            # êµ¬ê¸€ ì»¤ìŠ¤í…€ ê²€ìƒ‰ ê²°ê³¼ëŠ” ë³´í†µ 'gsc-webResult' í´ë˜ìŠ¤ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
            print("â³ JS ë Œë”ë§ ëŒ€ê¸° ì¤‘...")
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".gsc-webResult"))
            )
            
            # 5. ë¡œë”©ëœ HTML ê°€ì ¸ì˜¤ê¸°
            html = self.driver.page_source
            
            # 6. íŒŒì‹± ì‹œì‘
            return self.parse(html, self.TARGET_URL)
            
        except Exception as e:
            print(f"âŒ Selenium Error: {e}")
            return []
        finally:
            self.driver.quit()

    def parse(self, html: str, url: str) -> List[Dict[str, Any]]:
        soup = BeautifulSoup(html, "lxml")
        results = []

        # Google Custom Search ê²°ê³¼ ì„ íƒì
        rows = soup.select(".gsc-webResult.gsc-result")

        if not rows:
            print("âš ï¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        for row in rows:
            try:
                # A. ì œëª© ë° ë§í¬
                link_tag = row.select_one("a.gs-title")
                if not link_tag:
                    continue
                
                # êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ëŠ” í…ìŠ¤íŠ¸ê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì •ì œ í•„ìš”
                title = link_tag.get_text(strip=True)
                href = link_tag.get("href") # data-ctorig ì†ì„±ì— ì§„ì§œ URLì´ ìˆì„ ìˆ˜ë„ ìˆìŒ
                if link_tag.get("data-ctorig"):
                    href = link_tag.get("data-ctorig")

                if not href:
                    continue

                # B. ë‚ ì§œ ì¶”ì¶œ (Snippetì—ì„œ ì°¾ê¸°)
                snippet_div = row.select_one(".gs-snippet")
                snippet_text = snippet_div.get_text(strip=True) if snippet_div else ""
                
                date_str = datetime.now().strftime("%Y-%m-%d") # ê¸°ë³¸ê°’
                
                # ìŠ¤ë‹ˆí« ì•ë¶€ë¶„ì— ë‚ ì§œê°€ ìˆëŠ” ê²½ìš°ê°€ ë§ìŒ (ì˜ˆ: "Sep 24, 2024 ...")
                # ì •ê·œì‹ìœ¼ë¡œ ë‚ ì§œ ì°¾ê¸°
                import re
                date_match = re.search(r'([A-Z][a-z]{2}\s\d{1,2},\s\d{4})', snippet_text)
                if date_match:
                    try:
                        dt = parser.parse(date_match.group(0))
                        date_str = dt.strftime("%Y-%m-%d")
                    except:
                        pass

                # C. í•´ì‹œ ìƒì„±
                unique_content = f"{title}{href}"
                content_hash = self.generate_hash(unique_content)

                data = {
                    "country_code": "US",
                    "title": title,
                    "url": href,
                    "proclaimed_date": date_str,
                    "hash_value": content_hash,
                    "source_type": "html" # or pdf check
                }
                results.append(data)

            except Exception as e:
                print(f"âš ï¸ Parse Error: {e}")
                continue
        
        print(f"âœ… [SF] {len(results)}ê±´ ì¶”ì¶œ ì™„ë£Œ")
        return results