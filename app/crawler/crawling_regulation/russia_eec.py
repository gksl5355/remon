import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime
from app.crawler.crawling_regulation.base import BaseCrawler
from typing import List, Dict, Any

class RussiaEECCrawler(BaseCrawler):
    # ìœ ë¼ì‹œì•„ ê²½ì œ ì—°í•©(EAEU) ê¸°ìˆ  ê·œì • í˜ì´ì§€ (ë‹´ë°°)
    # ì‚¬ìš©ìê°€ ì œê³µí•œ URLì´ 404ì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ìƒìœ„ ëª©ë¡ í˜ì´ì§€ë„ ê³ ë ¤í•´ì•¼ í•˜ì§€ë§Œ, 
    # ì¼ë‹¨ ì œê³µí•´ì£¼ì‹  URL íŒ¨í„´ì„ ë”°ë¥´ë˜, ì‹¤ì œ ì‘ë™í•˜ëŠ” ëª©ë¡ í˜ì´ì§€ë¥¼ íƒ€ê²ŸíŒ…í•©ë‹ˆë‹¤.
    TARGET_URL = "https://eec.eaeunion.org/comission/department/deptexreg/tr/tabac.php"

    async def run(self) -> List[Dict[str, Any]]:
        print(f"ğŸ‡·ğŸ‡º [RU] EAEU(Russia) ê·œì œ í¬ë¡¤ë§ ì‹œì‘: {self.TARGET_URL}")
        
        html = await self.fetch(self.TARGET_URL)
        if not html:
            # í˜¹ì‹œ URLì´ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ë©”ì¸ ê¸°ìˆ ê·œì • í˜ì´ì§€ë„ ë°±ì—…ìœ¼ë¡œ ê³ ë ¤ ê°€ëŠ¥
            print("âŒ í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        results = await self.parse(html, self.TARGET_URL)
        return results

    async def parse(self, html: str, url: str) -> List[Dict[str, Any]]:
        soup = BeautifulSoup(html, "lxml")
        results = []

        # EEC ì‚¬ì´íŠ¸ëŠ” ë³´í†µ ê·œì œ ëª©ë¡ì„ í…ìŠ¤íŠ¸ ë§í¬ë¡œ ì œê³µí•©ë‹ˆë‹¤.
        # "035/2014" (ë‹´ë°° ê·œì œ ë²ˆí˜¸)ê°€ í¬í•¨ëœ ë§í¬ë¥¼ ëª¨ë‘ ì°¾ìŠµë‹ˆë‹¤.
        # ëŸ¬ì‹œì•„ì–´: "Ğ¢Ğ  Ğ¢Ğ¡ 035/2014" (TR CU 035/2014)
        target_keyword = "035/2014"
        
        # ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸° (ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ, ì¼ë°˜ì ìœ¼ë¡œ content ì˜ì—­)
        content_div = soup.select_one(".content") or soup.body

        links = content_div.find_all("a", href=True)

        for link in links:
            link_text = link.get_text(strip=True)
            href = link.get("href")

            # ë§í¬ í…ìŠ¤íŠ¸ë‚˜ hrefì— ê·œì œ ë²ˆí˜¸ê°€ ìˆëŠ”ì§€ í™•ì¸
            if target_keyword in link_text or target_keyword in href:
                
                # ì •ì œëœ ì œëª©
                title = link_text if len(link_text) > 10 else f"TR CU {target_keyword} Document"
                
                # ì ˆëŒ€ ê²½ë¡œ ë³€í™˜
                full_url = urljoin(url, href)

                # íŒŒì¼ í™•ì¥ì í™•ì¸ (PDFë‚˜ DOCXì¸ ê²½ìš°ê°€ ë§ìŒ)
                ext = "html"
                if full_url.lower().endswith(".pdf"):
                    ext = "pdf"
                elif full_url.lower().endswith(".doc") or full_url.lower().endswith(".docx"):
                    ext = "doc"

                # ë‚ ì§œ: í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•˜ê¸° ì–´ë ¤ìš°ë©´ ì˜¤ëŠ˜ ë‚ ì§œ (ì´í›„ ë©”íƒ€ë°ì´í„° ê°œì„  ê°€ëŠ¥)
                date_str = datetime.now().strftime("%Y-%m-%d")

                # í•´ì‹œ ìƒì„±
                unique_content = f"{title}{full_url}"
                content_hash = self.generate_hash(unique_content)

                data = {
                    "country_code": "RU", # ëŸ¬ì‹œì•„/EAEU
                    "title": title,
                    "url": full_url,
                    "proclaimed_date": date_str,
                    "hash_value": content_hash,
                    "source_type": ext # pdf, doc, html ë“±
                }
                
                # ì¤‘ë³µ ë§í¬ ì œê±°ë¥¼ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ì— ì—†ëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                if not any(d['url'] == full_url for d in results):
                    results.append(data)

        print(f"âœ… [RU] {len(results)}ê±´ì˜ ê·œì œ ë¬¸ì„œ ë°œê²¬")
        return results