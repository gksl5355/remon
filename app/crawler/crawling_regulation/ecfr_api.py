import httpx
from typing import List, Dict, Any
from datetime import datetime
from app.crawler.crawling_regulation.base import BaseCrawler

class ECFRAPICrawler(BaseCrawler):
    API_URL = "https://www.ecfr.gov/api/search/v1/results"

    def __init__(self, title_number: str = "21", query: str = "tobacco"):
        # eCFR ì „ìš© í—¤ë”
        ecfr_headers = {
            "Referer": "https://www.ecfr.gov/",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Site": "same-origin",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Dest": "empty",
            "Origin": "https://www.ecfr.gov"
        }
        
        super().__init__(headers=ecfr_headers)
        
        self.title_number = title_number
        self.query = query

    async def run(self) -> List[Dict[str, Any]]:
        print(f"ğŸ‡ºğŸ‡¸ [API] eCFR Title {self.title_number} ê²€ìƒ‰ ì‹œì‘: '{self.query}'")
        
        params = {
            "query": self.query,
            "hierarchy[title]": self.title_number,
            "per_page": 50
        }

        try:
            response = await self.session.get(self.API_URL, params=params)
            
            if response.status_code != 200:
                try:
                    error_msg = response.json()
                except:
                    error_msg = response.text
                print(f"âŒ API Error: {response.status_code} - {error_msg}")
                return []

            data = response.json()
            results = data.get("results", [])
            
            return self.parse(results, self.API_URL)

        except Exception as e:
            print(f"âŒ eCFR API Connection Error: {e}")
            return []

    def parse(self, results: list, url: str) -> List[Dict[str, Any]]:
        parsed_data = []

        for item in results:
            try:
                hierarchy = item.get("hierarchy", {})
                title = hierarchy.get("title", "")
                section = hierarchy.get("section", "")
                
                # ì œëª© êµ¬ì„±
                full_title = f"Title {title} Section {section}: {item.get('headline', '')}"
                date_str = item.get("last_modified_date") or datetime.now().strftime("%Y-%m-%d")
                
                # [í•µì‹¬ ìˆ˜ì •] ë·°ì–´ URL ëŒ€ì‹  'ë°ì´í„° ë Œë”ë§ API URL' ìƒì„±
                # ì˜ˆ: https://www.ecfr.gov/api/renderer/v1/content/newest/title-27?region=section-1.1
                # ì´ URLì€ ìë°”ìŠ¤í¬ë¦½íŠ¸ ì—†ì´ë„ ìˆœìˆ˜í•œ ê·œì œ ë³¸ë¬¸ HTMLì„ ë°˜í™˜í•©ë‹ˆë‹¤.
                if title and section:
                    full_url = f"https://www.ecfr.gov/api/renderer/v1/content/newest/title-{title}?region=section-{section}"
                else:
                    # fallback (êµ¬ì¡°ê°€ ì•ˆ ì¡íˆë©´ ì¼ë‹¨ ë·°ì–´ URL)
                    short_url = item.get("structure_index_url", "")
                    full_url = f"https://www.ecfr.gov{short_url}"

                # API ë°ì´í„°ëŠ” ë‚ ì§œê°€ ë°”ë€Œë©´ ìƒˆë¡œìš´ ë‚´ìš©ì´ë¯€ë¡œ í•´ì‹œì— ë‚ ì§œ í¬í•¨
                unique_content = f"{full_title}{full_url}{date_str}"
                content_hash = self.generate_hash(unique_content)

                data = {
                    "country_code": "US",
                    "title": full_title,
                    "url": full_url, # ì´ì œ ì—¬ê¸°ê°€ API ì£¼ì†Œê°€ ë¨
                    "proclaimed_date": date_str,
                    "hash_value": content_hash,
                    "source_type": "api"
                }
                parsed_data.append(data)

            except Exception as e:
                print(f"âš ï¸ Parsing Item Error: {e}")
                continue

        print(f"âœ… [API] {len(parsed_data)}ê±´ ê·œì œ ì •ë³´ ìˆ˜ì‹  ì™„ë£Œ")
        return parsed_data


# import httpx
# from typing import List, Dict, Any
# from datetime import datetime
# from app.crawler.crawling_regulation.base import BaseCrawler

# class ECFRAPICrawler(BaseCrawler):
#     API_URL = "https://www.ecfr.gov/api/search/v1/results"

#     def __init__(self, title_number: str = "21", query: str = "tobacco"):
#         # eCFR ì „ìš© í—¤ë” ì„¤ì •
#         ecfr_headers = {
#             "Referer": "https://www.ecfr.gov/",
#             "Upgrade-Insecure-Requests": "1",
#             "Sec-Fetch-Site": "same-origin",
#             "Sec-Fetch-Mode": "cors",
#             "Sec-Fetch-Dest": "empty",
#             "Origin": "https://www.ecfr.gov"
#         }
        
#         super().__init__(headers=ecfr_headers)
        
#         self.title_number = title_number
#         self.query = query

#     async def run(self) -> List[Dict[str, Any]]:
#         print(f"ğŸ‡ºğŸ‡¸ [API] eCFR Title {self.title_number} ê²€ìƒ‰ ì‹œì‘: '{self.query}'")
        
#         # [ìˆ˜ì •ë¨] title -> hierarchy[title] ë¡œ ë³€ê²½
#         # APIê°€ ê³„ì¸µ êµ¬ì¡° í•„í„°ë§ì„ ìš”êµ¬í•©ë‹ˆë‹¤.
#         params = {
#             "query": self.query,
#             "hierarchy[title]": self.title_number,  # <--- í•µì‹¬ ìˆ˜ì • ì‚¬í•­
#             "per_page": 50
#         }

#         try:
#             response = await self.session.get(self.API_URL, params=params)
            
#             if response.status_code != 200:
#                 # ì—ëŸ¬ ë°œìƒ ì‹œ ìƒì„¸ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•´ì„œ ë””ë²„ê¹…ì„ ë•ìŠµë‹ˆë‹¤.
#                 # json() íŒŒì‹± ì‹œë„ í›„ ì‹¤íŒ¨í•˜ë©´ text ì¶œë ¥
#                 try:
#                     error_msg = response.json()
#                 except:
#                     error_msg = response.text
#                 print(f"âŒ API Error: {response.status_code} - {error_msg}")
#                 return []

#             data = response.json()
#             results = data.get("results", [])
            
#             return self.parse(results, self.API_URL)

#         except Exception as e:
#             print(f"âŒ eCFR API Connection Error: {e}")
#             return []

#     def parse(self, results: list, url: str) -> List[Dict[str, Any]]:
#         parsed_data = []

#         for item in results:
#             try:
#                 hierarchy = item.get("hierarchy", {})
#                 title = hierarchy.get("title", "")
#                 section = hierarchy.get("section", "")
                
#                 full_title = f"Title {title} Section {section}: {item.get('headline', '')}"
#                 date_str = item.get("last_modified_date") or datetime.now().strftime("%Y-%m-%d")
                
#                 short_url = item.get("structure_index_url", "")
#                 if short_url:
#                     full_url = f"https://www.ecfr.gov{short_url}"
#                 else:
#                     full_url = f"https://www.ecfr.gov/current/title-{self.title_number}"

#                 # API ë°ì´í„°ëŠ” ë‚´ìš©ì´ ê°™ì•„ë„ ë‚ ì§œê°€ ê°±ì‹ ë˜ë©´ ìƒˆë¡œìš´ ê²ƒìœ¼ë¡œ ì³ì•¼ í•˜ë¯€ë¡œ date_strì„ í•´ì‹œì— í¬í•¨
#                 unique_content = f"{full_title}{full_url}{date_str}"
#                 content_hash = self.generate_hash(unique_content)

#                 data = {
#                     "country_code": "US",
#                     "title": full_title,
#                     "url": full_url,
#                     "proclaimed_date": date_str,
#                     "hash_value": content_hash,
#                     "source_type": "api"
#                 }
#                 parsed_data.append(data)

#             except Exception as e:
#                 print(f"âš ï¸ Parsing Item Error: {e}")
#                 continue

#         print(f"âœ… [API] {len(parsed_data)}ê±´ ê·œì œ ì •ë³´ ìˆ˜ì‹  ì™„ë£Œ")
#         return parsed_data

# import httpx
# from typing import List, Dict, Any
# from datetime import datetime
# from app.crawler.crawling_regulation.base import BaseCrawler

# class ECFRAPICrawler(BaseCrawler):
#     API_URL = "https://www.ecfr.gov/api/search/v1/results"

#     def __init__(self, title_number: str = "21", query: str = "tobacco"):
#         ecfr_headers = {
#             "Referer": "https://www.ecfr.gov/",
#             "Upgrade-Insecure-Requests": "1",
#             "Sec-Fetch-Site": "same-origin",
#             "Sec-Fetch-Mode": "cors",
#             "Sec-Fetch-Dest": "empty",
#             "Origin": "https://www.ecfr.gov"
#         }
        
#         super().__init__(headers=ecfr_headers)
        
#         self.title_number = title_number
#         self.query = query

#     async def run(self) -> List[Dict[str, Any]]:
#         print(f"ğŸ‡ºğŸ‡¸ [API] eCFR Title {self.title_number} ê²€ìƒ‰ ì‹œì‘: '{self.query}'")
        
#         # [ìˆ˜ì •ë¨] title -> titles (ë³µìˆ˜í˜•), ê°’ì€ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬
#         params = {
#             "query": self.query,
#             "titles": [self.title_number],  # <--- ì—¬ê¸°ê°€ í•µì‹¬ ìˆ˜ì • ì‚¬í•­ì…ë‹ˆë‹¤.
#             "per_page": 50
#         }

#         try:
#             response = await self.session.get(self.API_URL, params=params)
            
#             if response.status_code != 200:
#                 # ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„¸ ì¶œë ¥
#                 print(f"âŒ API Error: {response.status_code} - {response.text}")
#                 return []

#             data = response.json()
#             results = data.get("results", [])
            
#             return self.parse(results, self.API_URL)

#         except Exception as e:
#             print(f"âŒ eCFR API Connection Error: {e}")
#             return []

#     def parse(self, results: list, url: str) -> List[Dict[str, Any]]:
#         parsed_data = []

#         for item in results:
#             try:
#                 hierarchy = item.get("hierarchy", {})
#                 title = hierarchy.get("title", "")
#                 section = hierarchy.get("section", "")
                
#                 full_title = f"Title {title} Section {section}: {item.get('headline', '')}"
#                 date_str = item.get("last_modified_date") or datetime.now().strftime("%Y-%m-%d")
                
#                 short_url = item.get("structure_index_url", "")
#                 if short_url:
#                     full_url = f"https://www.ecfr.gov{short_url}"
#                 else:
#                     full_url = f"https://www.ecfr.gov/current/title-{self.title_number}"

#                 unique_content = f"{full_title}{full_url}{date_str}"
#                 content_hash = self.generate_hash(unique_content)

#                 data = {
#                     "country_code": "US",
#                     "title": full_title,
#                     "url": full_url,
#                     "proclaimed_date": date_str,
#                     "hash_value": content_hash,
#                     "source_type": "api"
#                 }
#                 parsed_data.append(data)

#             except Exception as e:
#                 print(f"âš ï¸ Parsing Item Error: {e}")
#                 continue

#         print(f"âœ… [API] {len(parsed_data)}ê±´ ê·œì œ ì •ë³´ ìˆ˜ì‹  ì™„ë£Œ")
#         return parsed_data

# import httpx
# from typing import List, Dict, Any
# from datetime import datetime
# from app.crawler.crawling_regulation.base import BaseCrawler

# class ECFRAPICrawler(BaseCrawler):
#     API_URL = "https://www.ecfr.gov/api/search/v1/results"

#     def __init__(self, title_number: str = "21", query: str = "tobacco"):
#         # eCFR ì „ìš© í—¤ë” ì„¤ì •
#         ecfr_headers = {
#             "Referer": "https://www.ecfr.gov/",
#             "Upgrade-Insecure-Requests": "1",
#             "Sec-Fetch-Site": "same-origin",
#             "Sec-Fetch-Mode": "cors",
#             "Sec-Fetch-Dest": "empty",
#             "Origin": "https://www.ecfr.gov"
#         }
        
#         super().__init__(headers=ecfr_headers)
        
#         self.title_number = title_number
#         self.query = query

#     async def run(self) -> List[Dict[str, Any]]:
#         print(f"ğŸ‡ºğŸ‡¸ [API] eCFR Title {self.title_number} ê²€ìƒ‰ ì‹œì‘: '{self.query}'")
        
#         # [ìˆ˜ì •ë¨] 400 ì—ëŸ¬ë¥¼ ìœ ë°œí•˜ëŠ” order, highlight íŒŒë¼ë¯¸í„° ì œê±°
#         # eCFR API v1ì€ ê¸°ë³¸ì ìœ¼ë¡œ 'ê´€ë ¨ë„(Relevance)' ìˆœìœ¼ë¡œ ì •ë ¬ë©ë‹ˆë‹¤.
#         params = {
#             "query": self.query,
#             "title": self.title_number,
#             "per_page": 50,  # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê°œìˆ˜
#             # "order": "newest",   <-- [ì‚­ì œ] ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°
#             # "highlight": "true"  <-- [ì‚­ì œ] ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°
#         }

#         try:
#             response = await self.session.get(self.API_URL, params=params)
            
#             if response.status_code != 200:
#                 # ì—ëŸ¬ ë°œìƒ ì‹œ ìƒì„¸ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•´ì„œ ë””ë²„ê¹…ì„ ë•ìŠµë‹ˆë‹¤.
#                 print(f"âŒ API Error: {response.status_code} - {response.text}")
#                 return []

#             data = response.json()
#             results = data.get("results", [])
            
#             return self.parse(results, self.API_URL)

#         except Exception as e:
#             print(f"âŒ eCFR API Connection Error: {e}")
#             return []

#     def parse(self, results: list, url: str) -> List[Dict[str, Any]]:
#         parsed_data = []

#         for item in results:
#             try:
#                 hierarchy = item.get("hierarchy", {})
#                 title = hierarchy.get("title", "")
#                 section = hierarchy.get("section", "")
                
#                 full_title = f"Title {title} Section {section}: {item.get('headline', '')}"
#                 date_str = item.get("last_modified_date") or datetime.now().strftime("%Y-%m-%d")
                
#                 short_url = item.get("structure_index_url", "")
#                 if short_url:
#                     full_url = f"https://www.ecfr.gov{short_url}"
#                 else:
#                     full_url = f"https://www.ecfr.gov/current/title-{self.title_number}"

#                 # API ë°ì´í„°ëŠ” ë‚´ìš©ì´ ê°™ì•„ë„ ë‚ ì§œê°€ ê°±ì‹ ë˜ë©´ ìƒˆë¡œìš´ ê²ƒìœ¼ë¡œ ì³ì•¼ í•˜ë¯€ë¡œ date_strì„ í•´ì‹œì— í¬í•¨
#                 unique_content = f"{full_title}{full_url}{date_str}"
#                 content_hash = self.generate_hash(unique_content)

#                 data = {
#                     "country_code": "US",
#                     "title": full_title,
#                     "url": full_url,
#                     "proclaimed_date": date_str,
#                     "hash_value": content_hash,
#                     "source_type": "api"
#                 }
#                 parsed_data.append(data)

#             except Exception as e:
#                 print(f"âš ï¸ Parsing Item Error: {e}")
#                 continue

#         print(f"âœ… [API] {len(parsed_data)}ê±´ ê·œì œ ì •ë³´ ìˆ˜ì‹  ì™„ë£Œ")
#         return parsed_data
