import httpx
from typing import List, Dict, Any
from datetime import datetime
from app.crawler.crawling_regulation.base import BaseCrawler

class ECFRAPICrawler(BaseCrawler):
    API_URL = "https://www.ecfr.gov/api/search/v1/results"

    def __init__(self, title_number: str = "21", query: str = "tobacco"):
        # eCFR ì „ìš© í—¤ë”
        ecfr_headers = {
            "Referer": "https://www.ecfr.gov/",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Site": "same-origin",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Dest": "empty",
            "Origin": "https://www.ecfr.gov"
        }
        
        super().__init__(headers=ecfr_headers)
        
        self.title_number = title_number
        self.query = query

    async def run(self) -> List[Dict[str, Any]]:
        print(f"ğŸ‡ºğŸ‡¸ [API] eCFR Title {self.title_number} ê²€ìƒ‰ ì‹œì‘: '{self.query}'")
        
        params = {
            "query": self.query,
            "hierarchy[title]": self.title_number,
            "per_page": 50
        }

        try:
            response = await self.session.get(self.API_URL, params=params)
            
            if response.status_code != 200:
                try:
                    error_msg = response.json()
                except:
                    error_msg = response.text
                print(f"âŒ API Error: {response.status_code} - {error_msg}")
                return []

            data = response.json()
            results = data.get("results", [])
            
            return self.parse(results, self.API_URL)

        except Exception as e:
            print(f"âŒ eCFR API Connection Error: {e}")
            return []

    def parse(self, results: list, url: str) -> List[Dict[str, Any]]:
        parsed_data = []

        for item in results:
            try:
                hierarchy = item.get("hierarchy", {})
                title = hierarchy.get("title", "")
                section = hierarchy.get("section", "")
                
                # ì œëª© êµ¬ì„±
                full_title = f"Title {title} Section {section}: {item.get('headline', '')}"
                date_str = item.get("last_modified_date") or datetime.now().strftime("%Y-%m-%d")
                
                # [í•µì‹¬ ìˆ˜ì •] ë·°ì–´ URL ëŒ€ì‹  'ë°ì´í„° ë Œë”ë§ API URL' ìƒì„±
                # ì˜ˆ: https://www.ecfr.gov/api/renderer/v1/content/newest/title-27?region=section-1.1
                # ì´ URLì€ ìë°”ìŠ¤í¬ë¦½íŠ¸ ì—†ì´ë„ ìˆœìˆ˜í•œ ê·œì œ ë³¸ë¬¸ HTMLì„ ë°˜í™˜í•©ë‹ˆë‹¤.
                if title and section:
                    full_url = f"https://www.ecfr.gov/api/renderer/v1/content/newest/title-{title}?region=section-{section}"
                else:
                    # fallback (êµ¬ì¡°ê°€ ì•ˆ ì¡íˆë©´ ì¼ë‹¨ ë·°ì–´ URL)
                    short_url = item.get("structure_index_url", "")
                    full_url = f"https://www.ecfr.gov{short_url}"

                # API ë°ì´í„°ëŠ” ë‚ ì§œê°€ ë°”ë€Œë©´ ìƒˆë¡œìš´ ë‚´ìš©ì´ë¯€ë¡œ í•´ì‹œì— ë‚ ì§œ í¬í•¨
                unique_content = f"{full_title}{full_url}{date_str}"
                content_hash = self.generate_hash(unique_content)

                data = {
                    "country_code": "US",
                    "title": full_title,
                    "url": full_url, # ì´ì œ ì—¬ê¸°ê°€ API ì£¼ì†Œê°€ ë¨
                    "proclaimed_date": date_str,
                    "hash_value": content_hash,
                    "source_type": "api"
                }
                parsed_data.append(data)

            except Exception as e:
                print(f"âš ï¸ Parsing Item Error: {e}")
                continue

        print(f"âœ… [API] {len(parsed_data)}ê±´ ê·œì œ ì •ë³´ ìˆ˜ì‹  ì™„ë£Œ")
        return parsed_data


