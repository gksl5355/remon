import re
import os
from bs4 import BeautifulSoup
from app.crawler.crawling_regulation.base import BaseCrawler
from typing import List, Dict, Any
from datetime import datetime

class CaliforniaLawCrawler(BaseCrawler):
    # [ìº˜ë¦¬í¬ë‹ˆì•„ ë¯¼ë²•(Civil Code) ì˜ˆì‹œ URL]
    TARGET_URL = "https://leginfo.legislature.ca.gov/faces/codes_displayText.xhtml?lawCode=CIV&division=1.&title=&part=2.&chapter=&article=" 

    async def run(self) -> List[Dict[str, Any]]:
        print(f"ğŸ‡ºğŸ‡¸ [CA] California Law í¬ë¡¤ë§ ì‹œì‘: {self.TARGET_URL}")
        
        html = await self.fetch(self.TARGET_URL)
        if not html:
            return []

        results = await self.parse(html, self.TARGET_URL)
        return results

    async def parse(self, html: str, url: str) -> List[Dict[str, Any]]:
        # [ìˆ˜ì • 1] Warning í•´ê²°: "html.parser"ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©
        soup = BeautifulSoup(html, "html.parser")
        results = []

        # [ìˆ˜ì • 2] ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ëŠ” ì„ íƒì ì‹œë„
        # ì´ ì‚¬ì´íŠ¸ëŠ” ë³´í†µ id="main_content" ë˜ëŠ” ë³„ë„ ID ì—†ì´ <body> ì•ˆì— ë°”ë¡œ ë‚´ìš©ì´ ìˆì„ ìˆ˜ ìˆìŒ
        # ì—¬ëŸ¬ í›„ë³´êµ°ì„ ìˆœì„œëŒ€ë¡œ ì°¾ì•„ë´…ë‹ˆë‹¤.
        container = soup.select_one("#main_content") or \
                    soup.select_one("#siteContent") or \
                    soup.select_one("form#myForm") or \
                    soup.select_one("body")
        
        if not container:
            print("âš ï¸ ì½˜í…ì¸  ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            # [ë””ë²„ê¹…] ë¬´ì—‡ì„ ê°€ì ¸ì™”ëŠ”ì§€ íŒŒì¼ë¡œ ì €ì¥í•´ì„œ í™•ì¸
            with open("california_debug.html", "w", encoding="utf-8") as f:
                f.write(soup.prettify())
            print("ğŸ› [Debug] 'california_debug.html' íŒŒì¼ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì—´ì–´ì„œ êµ¬ì¡°ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")
            return []

        # [ìˆ˜ì • 3] ë§í¬ ì¶”ì¶œ ë¡œì§ ê°œì„ 
        # ë³¸ë¬¸ ë‚´ì˜ ëª¨ë“  ë§í¬(a)ë¥¼ ì°¾ë˜, javascript: ê°™ì€ ê±´ ì œì™¸
        links = container.select("a") 
        print(f"ğŸ” ë°œê²¬ëœ ë§í¬ ìˆ˜: {len(links)}ê°œ")

        for link in links:
            try:
                title = link.get_text(strip=True)
                href = link.get("href")
                
                # ìœ íš¨í•œ ë§í¬ë§Œ í•„í„°ë§
                if not href or "javascript" in href or href == "#":
                    continue
                
                # ì œëª©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜(í˜ì´ì§€ ì´ë™ ë²„íŠ¼ ë“±) ë¹„ì–´ìˆìœ¼ë©´ íŒ¨ìŠ¤
                if len(title) < 5:
                    continue

                # ìƒëŒ€ ê²½ë¡œ -> ì ˆëŒ€ ê²½ë¡œ ë³€í™˜
                if not href.startswith("http"):
                    # ì‚¬ì´íŠ¸ ì£¼ì†Œ êµ¬ì¡°ì— ë§ê²Œ ì¡°í•©
                    full_url = f"https://leginfo.legislature.ca.gov/faces/{href}"
                else:
                    full_url = href

                date_str = datetime.now().strftime("%Y-%m-%d")
                
                # í•´ì‹œ ìƒì„±
                unique_content = f"{title}{full_url}"
                content_hash = self.generate_hash(unique_content)

                data = {
                    "country_code": "US", # ì¼ë‹¨ USë¡œ í†µì¼ (DB FK ì œì•½ ë•Œë¬¸)
                    "title": title,
                    "url": full_url,
                    "proclaimed_date": date_str,
                    "hash_value": content_hash,
                    "source_type": "html"
                }
                results.append(data)

            except Exception as e:
                # print(f"âš ï¸ Parse Error: {e}")
                continue
        
        print(f"âœ… [CA] {len(results)}ê±´ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
        return results