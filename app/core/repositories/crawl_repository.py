import os
import aiofiles
from bs4 import BeautifulSoup
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, desc
from datetime import datetime
from typing import Optional

from app.core.models.regulation_model import Regulation, RegulationVersion, RegulationChangeHistory
from app.ai_pipeline.preprocess.preprocess_agent import PreprocessAgent
from app.crawler.crawling_regulation.base import UniversalFetcher

class CrawlRepository:
    def __init__(self, db: AsyncSession):
        self.db = db
        self.preprocess_agent = PreprocessAgent()
        # ê¸°ë³¸ ê²½ë¡œ (ìƒì„±ìì—ì„œëŠ” baseë§Œ ì¡ìŒ)
        self.base_dir = "db" 

    async def process_crawled_data(self, data: dict, crawler: Optional[UniversalFetcher] = None):
        should_close_crawler = False
        if not crawler:
            crawler = UniversalFetcher()
            should_close_crawler = True

        try:
            url = data["url"]
            # DB ë¡œì§ì€ ë™ì¼...
            stmt = (
                select(Regulation)
                .join(RegulationVersion, Regulation.regulation_id == RegulationVersion.regulation_id)
                .where(RegulationVersion.original_uri == url)
                .limit(1)
            )
            result = await self.db.execute(stmt)
            existing_reg = result.scalar_one_or_none()

            if not existing_reg:
                return await self._create_new_regulation(data, crawler)
            else:
                return await self._handle_existing_regulation(existing_reg, data, crawler)
        finally:
            if should_close_crawler:
                await crawler.close()

    async def _save_file_locally(self, url: str, hash_value: str, crawler: UniversalFetcher, category: str = "regulation") -> Optional[str]:
        """
        [ìˆ˜ì •] category ì¸ìë¥¼ ë°›ì•„ì„œ ì €ì¥ í´ë”ë¥¼ ë™ì ìœ¼ë¡œ ê²°ì •
        """
        if not crawler:
            return None

        # 1. ì €ì¥ ê²½ë¡œ ê²°ì • (regulation vs news)
        # ì˜ˆ: db/regulation/abc.pdf ë˜ëŠ” db/news/xyz.txt
        save_dir = os.path.join(self.base_dir, category)
        os.makedirs(save_dir, exist_ok=True)

        # 2. ë‹¤ìš´ë¡œë“œ ë° ë§¤ì§ ë°”ì´íŠ¸ ì²´í¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        content = await crawler.fetch_binary(url)
        if not content:
            return None

        is_pdf = content.startswith(b'%PDF') or url.lower().endswith(".pdf")

        if is_pdf:
            filename = f"{hash_value}.pdf"
            file_path = os.path.join(save_dir, filename)
            if os.path.exists(file_path): return file_path
            
            async with aiofiles.open(file_path, "wb") as f:
                await f.write(content)
            print(f"ğŸ’¾ [{category.upper()}] PDF ì €ì¥: {file_path}")
            return file_path

        else:
            filename = f"{hash_value}.txt"
            file_path = os.path.join(save_dir, filename)
            if os.path.exists(file_path): return file_path

            try:
                html_text = content.decode('utf-8')
            except:
                try: html_text = content.decode('latin-1')
                except: return None

            soup = BeautifulSoup(html_text, "lxml")
            for script in soup(["script", "style", "header", "footer", "nav", "noscript"]):
                script.extract()
            clean_text = soup.get_text(separator="\n", strip=True)

            async with aiofiles.open(file_path, "w", encoding="utf-8") as f:
                await f.write(clean_text)
            
            print(f"ğŸ’¾ [{category.upper()}] í…ìŠ¤íŠ¸ ì €ì¥: {file_path}")
            return file_path

    async def _create_new_regulation(self, data: dict, crawler: UniversalFetcher):
        # [ìˆ˜ì •] data ë”•ì…”ë„ˆë¦¬ì—ì„œ categoryë¥¼ êº¼ë‚´ì„œ ì „ë‹¬
        category = data.get("category", "regulation")
        file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler, category)

        # (DB ì €ì¥ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼)
        # ë‹¨, Newsì¸ ê²½ìš° DBì— íƒœê·¸ë¥¼ ë‹¤ë¥´ê²Œ ë‹¬ê±°ë‚˜ ë³„ë„ í…Œì´ë¸”ë¡œ ëº„ ìˆ˜ë„ ìˆì§€ë§Œ,
        # ì¼ë‹¨ì€ Regulation í…Œì´ë¸”ì— ì €ì¥í•˜ë˜ titleì— íƒœê·¸ë¥¼ ë¶™ì´ëŠ” ì‹ìœ¼ë¡œ êµ¬ë¶„ ê°€ëŠ¥
        
        # ... (DB Insert ì½”ë“œ ìƒëµ - ê¸°ì¡´ê³¼ ë™ì¼) ...
        # ... (ìƒˆë¡œìš´ íŒŒì¼ì´ ìˆìœ¼ë©´ PreprocessAgent ì‹¤í–‰) ...
        
        # ì—¬ê¸°ì„œëŠ” ìƒëµí–ˆì§€ë§Œ, ì‹¤ì œ ì½”ë“œì—ëŠ” DB Insert ë¶€ë¶„ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        # í¸ì˜ìƒ í•µì‹¬ì¸ _save_file_locally í˜¸ì¶œë¶€ë§Œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
        
        # [ë³µì›ìš© DB ì½”ë“œ]
        proclaimed_date = None
        if data.get("proclaimed_date"):
            # ... ë‚ ì§œ ì²˜ë¦¬ ...
            pass
            
        new_reg = Regulation(
            source_id=data.get("source_id", 1),
            country_code=data.get("country_code", "US"),
            title=f"[{category.upper()}] {data.get('title', 'No Title')}", # ì œëª©ì— ì¹´í…Œê³ ë¦¬ í‘œì‹œ
            status="active"
        )
        self.db.add(new_reg)
        await self.db.flush()
        
        new_version = RegulationVersion(
            regulation_id=new_reg.regulation_id,
            version_number=1,
            original_uri=data["url"],
            hash_value=data["hash_value"]
        )
        self.db.add(new_version)
        
        history = RegulationChangeHistory(
            version=new_version,
            change_type="new",
            change_summary=f"ìˆ˜ì§‘ë¨ ({category})"
        )
        self.db.add(history)
        await self.db.commit()

        if file_path:
            await self.preprocess_agent.run(file_path, data)
            
        return "created"

    async def _handle_existing_regulation(self, regulation: Regulation, data: dict, crawler: UniversalFetcher):
        # [ìˆ˜ì •] ì—…ë°ì´íŠ¸ ì‹œì—ë„ ì¹´í…Œê³ ë¦¬ ì „ë‹¬
        category = data.get("category", "regulation")
        
        # ... (ë²„ì „ ì²´í¬ ë¡œì§ ê¸°ì¡´ ë™ì¼) ...
        stmt = select(RegulationVersion).where(RegulationVersion.regulation_id == regulation.regulation_id).order_by(desc(RegulationVersion.version_number)).limit(1)
        result = await self.db.execute(stmt)
        latest_version = result.scalar_one_or_none()

        if latest_version and latest_version.hash_value == data["hash_value"]:
            return "skipped"

        file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler, category)
        
        # ... (ë²„ì „ ì—…ë°ì´íŠ¸ DB ë¡œì§ ê¸°ì¡´ ë™ì¼) ...
        new_v_num = latest_version.version_number + 1
        new_version = RegulationVersion(
            regulation_id=regulation.regulation_id,
            version_number=new_v_num,
            original_uri=data["url"],
            hash_value=data["hash_value"]
        )
        self.db.add(new_version)
        history = RegulationChangeHistory(
            version=new_version,
            change_type="append",
            change_summary=f"ì—…ë°ì´íŠ¸ë¨ ({category})"
        )
        self.db.add(history)
        await self.db.commit()

        if file_path:
            await self.preprocess_agent.run(file_path, data)

        return "updated"

# import os
# import aiofiles
# from bs4 import BeautifulSoup
# from sqlalchemy.ext.asyncio import AsyncSession
# from sqlalchemy import select, desc
# from datetime import datetime
# from typing import Optional

# from app.core.models.regulation_model import Regulation, RegulationVersion, RegulationChangeHistory
# from app.ai_pipeline.preprocess.preprocess_agent import PreprocessAgent
# from app.crawler.crawling_regulation.base import UniversalFetcher

# class CrawlRepository:
#     def __init__(self, db: AsyncSession):
#         self.db = db
#         self.preprocess_agent = PreprocessAgent()
#         self.save_dir = os.path.join("db", "regulation")
#         os.makedirs(self.save_dir, exist_ok=True)

#     async def process_crawled_data(self, data: dict, crawler: Optional[UniversalFetcher] = None):
#         should_close_crawler = False
#         if not crawler:
#             crawler = UniversalFetcher()
#             should_close_crawler = True

#         try:
#             url = data["url"]
            
#             # DB ì¤‘ë³µ ì²´í¬
#             stmt = (
#                 select(Regulation)
#                 .join(RegulationVersion, Regulation.regulation_id == RegulationVersion.regulation_id)
#                 .where(RegulationVersion.original_uri == url)
#                 .limit(1)
#             )
#             result = await self.db.execute(stmt)
#             existing_reg = result.scalar_one_or_none()

#             # ì‹ ê·œ ë“±ë¡ ë˜ëŠ” ì—…ë°ì´íŠ¸ ì²˜ë¦¬
#             if not existing_reg:
#                 return await self._create_new_regulation(data, crawler)
#             else:
#                 return await self._handle_existing_regulation(existing_reg, data, crawler)
#         finally:
#             if should_close_crawler:
#                 await crawler.close()

#     async def _save_file_locally(self, url: str, hash_value: str, crawler: UniversalFetcher) -> Optional[str]:
#         """
#         [ê°œì„ ëœ ë¡œì§] URL í™•ì¥ìê°€ ì•„ë‹Œ 'íŒŒì¼ ì‹¤ì œ í—¤ë”(Magic Bytes)'ë¡œ í˜•ì‹ì„ íŒë‹¨í•˜ì—¬ ì €ì¥
#         """
#         if not crawler:
#             return None

#         # 1. ì¼ë‹¨ ë°”ì´ë„ˆë¦¬ë¡œ ë‹¤ìš´ë¡œë“œ (PDFì¼ ìˆ˜ë„, HTMLì¼ ìˆ˜ë„ ìˆìŒ)
#         content = await crawler.fetch_binary(url)
#         if not content:
#             return None

#         # 2. íŒŒì¼ í˜•ì‹ íŒë³„ (Magic Bytes Check)
#         is_pdf = False
        
#         # PDF íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ í™•ì¸ (%PDF-)
#         if content.startswith(b'%PDF'):
#             is_pdf = True
        
#         # (ì˜µì…˜) URLì´ ê°•ì œë¡œ .pdfì¸ ê²½ìš°ë„ í¬í•¨
#         elif url.lower().endswith(".pdf"):
#             is_pdf = True

#         # 3. í˜•ì‹ì— ë”°ë¥¸ ì €ì¥ ë¶„ê¸°
#         if is_pdf:
#             # === PDF ì €ì¥ ===
#             filename = f"{hash_value}.pdf"
#             file_path = os.path.join(self.save_dir, filename)
            
#             if os.path.exists(file_path):
#                 return file_path

#             async with aiofiles.open(file_path, "wb") as f:
#                 await f.write(content)
#             print(f"ğŸ’¾ PDF ì €ì¥ ì™„ë£Œ (Auto-detected): {file_path}")
#             return file_path

#         else:
#             # === HTML/Text ì €ì¥ ===
#             filename = f"{hash_value}.txt"
#             file_path = os.path.join(self.save_dir, filename)

#             if os.path.exists(file_path):
#                 return file_path

#             try:
#                 # ë°”ì´ë„ˆë¦¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”© (ëŸ¬ì‹œì•„ì–´ ë“± ê¹¨ì§ ë°©ì§€ ì‹œë„)
#                 # 1ì°¨ ì‹œë„: utf-8
#                 html_text = content.decode('utf-8')
#             except UnicodeDecodeError:
#                 try:
#                     # 2ì°¨ ì‹œë„: latin-1 (í˜¹ì€ chardet ì‚¬ìš© ê°€ëŠ¥)
#                     html_text = content.decode('latin-1')
#                 except:
#                     print(f"âš ï¸ ë””ì½”ë”© ì‹¤íŒ¨: {url}")
#                     return None

#             # BeautifulSoupìœ¼ë¡œ íƒœê·¸ ì œê±°í•˜ê³  ë³¸ë¬¸ë§Œ ì¶”ì¶œ
#             soup = BeautifulSoup(html_text, "lxml")
            
#             # ë…¸ì´ì¦ˆ ì œê±°
#             for script in soup(["script", "style", "header", "footer", "nav", "iframe", "noscript"]):
#                 script.extract()
            
#             clean_text = soup.get_text(separator="\n", strip=True)

#             async with aiofiles.open(file_path, "w", encoding="utf-8") as f:
#                 await f.write(clean_text)
            
#             print(f"ğŸ’¾ í…ìŠ¤íŠ¸ ë³€í™˜ ë° ì €ì¥ ì™„ë£Œ: {file_path}")
#             return file_path

#     async def _create_new_regulation(self, data: dict, crawler: UniversalFetcher):
#         # íŒŒì¼ ì €ì¥ í˜¸ì¶œ
#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

#         proclaimed_date = None
#         if data.get("proclaimed_date"):
#             try:
#                 if isinstance(data["proclaimed_date"], str):
#                     proclaimed_date = datetime.strptime(data["proclaimed_date"], "%Y-%m-%d").date()
#                 else:
#                     proclaimed_date = data["proclaimed_date"]
#             except ValueError:
#                 pass

#         new_reg = Regulation(
#             source_id=data.get("source_id", 1),
#             country_code=data.get("country_code", "US"),
#             title=data.get("title", "No Title"),
#             proclaimed_date=proclaimed_date,
#             status="active"
#         )
#         self.db.add(new_reg)
#         await self.db.flush()

#         new_version = RegulationVersion(
#             regulation_id=new_reg.regulation_id,
#             version_number=1,
#             original_uri=data["url"],
#             hash_value=data["hash_value"]
#         )
#         self.db.add(new_version)
        
#         history = RegulationChangeHistory(
#             version=new_version,
#             change_type="new", 
#             change_summary="ìµœì´ˆ ìˆ˜ì§‘ë¨ (Discovery Agent)"
#         )
#         self.db.add(history)
        
#         await self.db.commit()
#         print(f"âœ¨ [New] ì‹ ê·œ ê·œì œ ë“±ë¡: {new_reg.title[:30]}...")

#         if file_path:
#             # ì „ì²˜ë¦¬ ì—ì´ì „íŠ¸ ì‹¤í–‰
#             await self.preprocess_agent.run(file_path, data)
            
#         return "created"

#     async def _handle_existing_regulation(self, regulation: Regulation, data: dict, crawler: UniversalFetcher):
#         stmt = select(RegulationVersion).where(
#             RegulationVersion.regulation_id == regulation.regulation_id
#         ).order_by(desc(RegulationVersion.version_number)).limit(1)
        
#         result = await self.db.execute(stmt)
#         latest_version = result.scalar_one_or_none()

#         if latest_version and latest_version.hash_value == data["hash_value"]:
#             return "skipped"

#         print(f"ğŸ”„ ë³€ê²½ ê°ì§€ë¨! íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

#         new_v_num = latest_version.version_number + 1 if latest_version else 1
        
#         new_version = RegulationVersion(
#             regulation_id=regulation.regulation_id,
#             version_number=new_v_num,
#             original_uri=data["url"],
#             hash_value=data["hash_value"]
#         )
#         self.db.add(new_version)

#         history = RegulationChangeHistory(
#             version=new_version,
#             change_type="append",
#             change_summary=f"ë²„ì „ {new_v_num}ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨"
#         )
#         self.db.add(history)
        
#         await self.db.commit()
#         print(f"ğŸ”„ [Update] ê·œì œ ì—…ë°ì´íŠ¸ ì™„ë£Œ (v{new_v_num})")

#         if file_path:
#             await self.preprocess_agent.run(file_path, data)

#         return "updated"

# import os
# import aiofiles
# from bs4 import BeautifulSoup
# from sqlalchemy.ext.asyncio import AsyncSession
# from sqlalchemy import select, desc
# from datetime import datetime
# from typing import Optional

# from app.core.models.regulation_model import Regulation, RegulationVersion, RegulationChangeHistory
# from app.ai_pipeline.preprocess.preprocess_agent import PreprocessAgent
# from app.crawler.crawling_regulation.base import UniversalFetcher  # [ìˆ˜ì •] UniversalFetcher ì„í¬íŠ¸

# class CrawlRepository:
#     def __init__(self, db: AsyncSession):
#         self.db = db
#         self.preprocess_agent = PreprocessAgent()
#         self.save_dir = os.path.join("db", "regulation")
#         os.makedirs(self.save_dir, exist_ok=True)

#     async def process_crawled_data(self, data: dict, crawler: Optional[UniversalFetcher] = None):
#         """
#         [ë²”ìš©ì„± ê°œì„ ] 
#         crawler ê°ì²´ê°€ ì—†ìœ¼ë©´ UniversalFetcherë¥¼ ì„ì‹œë¡œ ìƒì„±í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
#         ì´ë¥¼ í†µí•´ Discovery Agentê°€ URLë§Œ ë˜ì ¸ì¤˜ë„ ì•Œì•„ì„œ ë‹¤ìš´ë¡œë“œê¹Œì§€ ìˆ˜í–‰í•©ë‹ˆë‹¤.
#         """
#         should_close_crawler = False
#         if not crawler:
#             crawler = UniversalFetcher()
#             should_close_crawler = True

#         try:
#             url = data["url"]
            
#             stmt = (
#                 select(Regulation)
#                 .join(RegulationVersion, Regulation.regulation_id == RegulationVersion.regulation_id)
#                 .where(RegulationVersion.original_uri == url)
#                 .limit(1)
#             )
#             result = await self.db.execute(stmt)
#             existing_reg = result.scalar_one_or_none()

#             if not existing_reg:
#                 return await self._create_new_regulation(data, crawler)
#             else:
#                 return await self._handle_existing_regulation(existing_reg, data, crawler)
#         finally:
#             # ì„ì‹œë¡œ ë§Œë“  í¬ë¡¤ëŸ¬ë¼ë©´ ë‹«ì•„ì¤€ë‹¤
#             if should_close_crawler:
#                 await crawler.close()

#     async def _save_file_locally(self, url: str, hash_value: str, crawler: UniversalFetcher) -> Optional[str]:
#         if not crawler:
#             return None

#         # 1. PDF ì²˜ë¦¬
#         if url.lower().endswith(".pdf"):
#             filename = f"{hash_value}.pdf"
#             file_path = os.path.join(self.save_dir, filename)
            
#             if os.path.exists(file_path):
#                 return file_path

#             content = await crawler.fetch_binary(url)
#             if content:
#                 async with aiofiles.open(file_path, "wb") as f:
#                     await f.write(content)
#                 print(f"ğŸ’¾ PDF ì €ì¥ ì™„ë£Œ: {file_path}")
#                 return file_path
        
#         # 2. ì¼ë°˜ ì›¹í˜ì´ì§€ (HTML -> Text)
#         else:
#             filename = f"{hash_value}.txt"
#             file_path = os.path.join(self.save_dir, filename)

#             if os.path.exists(file_path):
#                 return file_path

#             html_content = await crawler.fetch(url)
#             if html_content:
#                 soup = BeautifulSoup(html_content, "lxml")
                
#                 # ë…¸ì´ì¦ˆ ì œê±°
#                 for script in soup(["script", "style", "header", "footer", "nav", "iframe", "noscript"]):
#                     script.extract()
                
#                 clean_text = soup.get_text(separator="\n", strip=True)

#                 async with aiofiles.open(file_path, "w", encoding="utf-8") as f:
#                     await f.write(clean_text)
                
#                 print(f"ğŸ’¾ í…ìŠ¤íŠ¸ ë³€í™˜ ë° ì €ì¥ ì™„ë£Œ: {file_path}")
#                 return file_path

#         return None

#     async def _create_new_regulation(self, data: dict, crawler: UniversalFetcher):
#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

#         # [ì•ˆì „ì¥ì¹˜] ë‚ ì§œ í¬ë§·ì´ ì•ˆ ë§ê±°ë‚˜ ì—†ìœ¼ë©´ None ì²˜ë¦¬
#         proclaimed_date = None
#         if data.get("proclaimed_date"):
#             try:
#                 # LLMì´ YYYY-MM-DD í˜•ì‹ì„ ì•ˆ ì§€ì¼°ì„ ê²½ìš° ëŒ€ë¹„
#                 if isinstance(data["proclaimed_date"], str):
#                     proclaimed_date = datetime.strptime(data["proclaimed_date"], "%Y-%m-%d").date()
#                 else:
#                     proclaimed_date = data["proclaimed_date"]
#             except ValueError:
#                 print(f"âš ï¸ ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜ (ê¸°ë¡ ìƒëµ): {data['proclaimed_date']}")

#         # [ë²”ìš©ì„±] source_idê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 1 ì‚¬ìš© (ì¶”í›„ 'General Web' ì†ŒìŠ¤ IDë¡œ ë³€ê²½ ê¶Œì¥)
#         source_id = data.get("source_id", 1)

#         new_reg = Regulation(
#             source_id=source_id,
#             country_code=data.get("country_code", "US"), # ê¸°ë³¸ê°’ US
#             title=data.get("title", "No Title"),
#             proclaimed_date=proclaimed_date,
#             status="active"
#         )
#         self.db.add(new_reg)
#         await self.db.flush()

#         new_version = RegulationVersion(
#             regulation_id=new_reg.regulation_id,
#             version_number=1,
#             original_uri=data["url"],
#             hash_value=data["hash_value"]
#         )
#         self.db.add(new_version)
        
#         history = RegulationChangeHistory(
#             version=new_version,
#             change_type="new", 
#             change_summary="ìµœì´ˆ ìˆ˜ì§‘ë¨ (Discovery Agent)"
#         )
#         self.db.add(history)
        
#         await self.db.commit()
#         print(f"âœ¨ [New] ì‹ ê·œ ê·œì œ ë“±ë¡: {new_reg.title[:30]}...")

#         if file_path:
#             # PreprocessAgent ì‹¤í–‰ (ë¹„ë™ê¸° ì²˜ë¦¬ ê¶Œì¥)
#             # await self.preprocess_agent.run(file_path, data) 
#             # -> ì„±ëŠ¥ì„ ìœ„í•´ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ë„˜ê¸°ëŠ” ê²ƒì„ ê³ ë ¤í•  ìˆ˜ ìˆìŒ
#             await self.preprocess_agent.run(file_path, data)
            
#         return "created"

#     async def _handle_existing_regulation(self, regulation: Regulation, data: dict, crawler: UniversalFetcher):
#         # ê¸°ì¡´ ë¡œì§ ìœ ì§€ (ë²„ì „ ê´€ë¦¬)
#         stmt = select(RegulationVersion).where(
#             RegulationVersion.regulation_id == regulation.regulation_id
#         ).order_by(desc(RegulationVersion.version_number)).limit(1)
        
#         result = await self.db.execute(stmt)
#         latest_version = result.scalar_one_or_none()

#         if latest_version and latest_version.hash_value == data["hash_value"]:
#             return "skipped"

#         print(f"ğŸ”„ ë³€ê²½ ê°ì§€ë¨! íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

#         new_v_num = latest_version.version_number + 1 if latest_version else 1
        
#         new_version = RegulationVersion(
#             regulation_id=regulation.regulation_id,
#             version_number=new_v_num,
#             original_uri=data["url"],
#             hash_value=data["hash_value"]
#         )
#         self.db.add(new_version)

#         history = RegulationChangeHistory(
#             version=new_version,
#             change_type="append",
#             change_summary=f"ë²„ì „ {new_v_num}ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨"
#         )
#         self.db.add(history)
        
#         await self.db.commit()
#         print(f"ğŸ”„ [Update] ê·œì œ ì—…ë°ì´íŠ¸ ì™„ë£Œ (v{new_v_num})")

#         if file_path:
#             await self.preprocess_agent.run(file_path, data)

#         return "updated"

# # app/services/regulation_service.py

# import os
# import aiofiles
# from bs4 import BeautifulSoup # [ì¶”ê°€] í…ìŠ¤íŠ¸ ì¶”ì¶œìš©
# from sqlalchemy.ext.asyncio import AsyncSession
# from sqlalchemy import select, desc
# from datetime import datetime

# from app.core.models.regulation_model import Regulation, RegulationVersion, RegulationChangeHistory
# from app.ai_pipeline.preprocess.preprocess_agent import PreprocessAgent

# class CrawlRepository:
#     def __init__(self, db: AsyncSession):
#         self.db = db
#         self.preprocess_agent = PreprocessAgent()
#         self.save_dir = os.path.join("db", "regulation")
#         os.makedirs(self.save_dir, exist_ok=True)

#     async def process_crawled_data(self, data: dict, crawler=None):
#         url = data["url"]
        
#         stmt = (
#             select(Regulation)
#             .join(RegulationVersion, Regulation.regulation_id == RegulationVersion.regulation_id)
#             .where(RegulationVersion.original_uri == url)
#             .limit(1)
#         )
#         result = await self.db.execute(stmt)
#         existing_reg = result.scalar_one_or_none()

#         if not existing_reg:
#             return await self._create_new_regulation(data, crawler)
#         else:
#             return await self._handle_existing_regulation(existing_reg, data, crawler)

#     # [í•µì‹¬ ìˆ˜ì •] íŒŒì¼ ì €ì¥ ë¡œì§ ë³€ê²½
#     async def _save_file_locally(self, url: str, hash_value: str, crawler) -> str:
#         if not crawler:
#             return None

#         # 1. PDFì¸ ê²½ìš°: ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ ë°”ì´ë„ˆë¦¬ ì €ì¥
#         if url.lower().endswith(".pdf"):
#             filename = f"{hash_value}.pdf"
#             file_path = os.path.join(self.save_dir, filename)
            
#             if os.path.exists(file_path):
#                 return file_path

#             content = await crawler.fetch_binary(url)
#             if content:
#                 async with aiofiles.open(file_path, "wb") as f:
#                     await f.write(content)
#                 print(f"ğŸ’¾ PDF ì €ì¥ ì™„ë£Œ: {file_path}")
#                 return file_path
        
#         # 2. HTML(ì›¹í˜ì´ì§€)ì¸ ê²½ìš°: í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ .txtë¡œ ì €ì¥
#         else:
#             filename = f"{hash_value}.txt" # í™•ì¥ìë¥¼ .txtë¡œ ë³€ê²½
#             file_path = os.path.join(self.save_dir, filename)

#             if os.path.exists(file_path):
#                 return file_path

#             # fetch()ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸(HTML) ê°€ì ¸ì˜¤ê¸°
#             html_content = await crawler.fetch(url)
#             if html_content:
#                 # BeautifulSoupìœ¼ë¡œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
#                 soup = BeautifulSoup(html_content, "lxml")
                
#                 # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ë„¤ë¹„ê²Œì´ì…˜ ë“±)
#                 for script in soup(["script", "style", "header", "footer", "nav", "iframe"]):
#                     script.extract()
                
#                 # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê³µë°± ì •ë¦¬)
#                 clean_text = soup.get_text(separator="\n", strip=True)

#                 # .txt íŒŒì¼ë¡œ ì €ì¥
#                 async with aiofiles.open(file_path, "w", encoding="utf-8") as f:
#                     await f.write(clean_text)
                
#                 print(f"ğŸ’¾ í…ìŠ¤íŠ¸ ë³€í™˜ ë° ì €ì¥ ì™„ë£Œ: {file_path}")
#                 return file_path

#         return None

#     async def _create_new_regulation(self, data: dict, crawler):
#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

#         new_reg = Regulation(
#             source_id=1,
#             country_code=data["country_code"],
#             title=data["title"],
#             proclaimed_date=datetime.strptime(data["proclaimed_date"], "%Y-%m-%d").date() if data.get("proclaimed_date") else None,
#             status="active"
#         )
#         self.db.add(new_reg)
#         await self.db.flush()

#         new_version = RegulationVersion(
#             regulation_id=new_reg.regulation_id,
#             version_number=1,
#             original_uri=data["url"],
#             hash_value=data["hash_value"]
#         )
#         self.db.add(new_version)
        
#         history = RegulationChangeHistory(
#             version=new_version,
#             change_type="new", 
#             change_summary="ìµœì´ˆ ìˆ˜ì§‘ë¨"
#         )
#         self.db.add(history)
        
#         await self.db.commit()
#         print(f"âœ¨ [New] ì‹ ê·œ ê·œì œ ë“±ë¡: {data['title'][:30]}...")

#         if file_path:
#             await self.preprocess_agent.run(file_path, data)
            
#         return "created"

#     async def _handle_existing_regulation(self, regulation: Regulation, data: dict, crawler):
#         stmt = select(RegulationVersion).where(
#             RegulationVersion.regulation_id == regulation.regulation_id
#         ).order_by(desc(RegulationVersion.version_number)).limit(1)
        
#         result = await self.db.execute(stmt)
#         latest_version = result.scalar_one_or_none()

#         if latest_version and latest_version.hash_value == data["hash_value"]:
#             return "skipped"

#         print(f"ğŸ”„ ë³€ê²½ ê°ì§€ë¨! íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

#         new_v_num = latest_version.version_number + 1 if latest_version else 1
        
#         new_version = RegulationVersion(
#             regulation_id=regulation.regulation_id,
#             version_number=new_v_num,
#             original_uri=data["url"],
#             hash_value=data["hash_value"]
#         )
#         self.db.add(new_version)

#         history = RegulationChangeHistory(
#             version=new_version,
#             change_type="append",
#             change_summary=f"ë²„ì „ {new_v_num}ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨"
#         )
#         self.db.add(history)
        
#         await self.db.commit()
#         print(f"ğŸ”„ [Update] ê·œì œ ì—…ë°ì´íŠ¸ ì™„ë£Œ (v{new_v_num})")

#         if file_path:
#             await self.preprocess_agent.run(file_path, data)

#         return "updated"