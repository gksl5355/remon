# app/services/regulation_service.py

import os
import aiofiles
from bs4 import BeautifulSoup # [ì¶”ê°€] í…ìŠ¤íŠ¸ ì¶”ì¶œìš©
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, desc
from datetime import datetime

from app.core.models.regulation_model import Regulation, RegulationVersion, RegulationChangeHistory
from app.ai_pipeline.preprocess.preprocess_agent import PreprocessAgent

class CrawlRepository:
    def __init__(self, db: AsyncSession):
        self.db = db
        self.preprocess_agent = PreprocessAgent()
        self.save_dir = os.path.join("db", "regulation")
        os.makedirs(self.save_dir, exist_ok=True)

    async def process_crawled_data(self, data: dict, crawler=None):
        url = data["url"]
        
        stmt = (
            select(Regulation)
            .join(RegulationVersion, Regulation.regulation_id == RegulationVersion.regulation_id)
            .where(RegulationVersion.original_uri == url)
            .limit(1)
        )
        result = await self.db.execute(stmt)
        existing_reg = result.scalar_one_or_none()

        if not existing_reg:
            return await self._create_new_regulation(data, crawler)
        else:
            return await self._handle_existing_regulation(existing_reg, data, crawler)

    # [í•µì‹¬ ìˆ˜ì •] íŒŒì¼ ì €ì¥ ë¡œì§ ë³€ê²½
    async def _save_file_locally(self, url: str, hash_value: str, crawler) -> str:
        if not crawler:
            return None

        # 1. PDFì¸ ê²½ìš°: ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ ë°”ì´ë„ˆë¦¬ ì €ì¥
        if url.lower().endswith(".pdf"):
            filename = f"{hash_value}.pdf"
            file_path = os.path.join(self.save_dir, filename)
            
            if os.path.exists(file_path):
                return file_path

            content = await crawler.fetch_binary(url)
            if content:
                async with aiofiles.open(file_path, "wb") as f:
                    await f.write(content)
                print(f"ğŸ’¾ PDF ì €ì¥ ì™„ë£Œ: {file_path}")
                return file_path
        
        # 2. HTML(ì›¹í˜ì´ì§€)ì¸ ê²½ìš°: í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ .txtë¡œ ì €ì¥
        else:
            filename = f"{hash_value}.txt" # í™•ì¥ìë¥¼ .txtë¡œ ë³€ê²½
            file_path = os.path.join(self.save_dir, filename)

            if os.path.exists(file_path):
                return file_path

            # fetch()ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸(HTML) ê°€ì ¸ì˜¤ê¸°
            html_content = await crawler.fetch(url)
            if html_content:
                # BeautifulSoupìœ¼ë¡œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                soup = BeautifulSoup(html_content, "lxml")
                
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ë„¤ë¹„ê²Œì´ì…˜ ë“±)
                for script in soup(["script", "style", "header", "footer", "nav", "iframe"]):
                    script.extract()
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê³µë°± ì •ë¦¬)
                clean_text = soup.get_text(separator="\n", strip=True)

                # .txt íŒŒì¼ë¡œ ì €ì¥
                async with aiofiles.open(file_path, "w", encoding="utf-8") as f:
                    await f.write(clean_text)
                
                print(f"ğŸ’¾ í…ìŠ¤íŠ¸ ë³€í™˜ ë° ì €ì¥ ì™„ë£Œ: {file_path}")
                return file_path

        return None

    async def _create_new_regulation(self, data: dict, crawler):
        file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

        new_reg = Regulation(
            source_id=1,
            country_code=data["country_code"],
            title=data["title"],
            proclaimed_date=datetime.strptime(data["proclaimed_date"], "%Y-%m-%d").date() if data.get("proclaimed_date") else None,
            status="active"
        )
        self.db.add(new_reg)
        await self.db.flush()

        new_version = RegulationVersion(
            regulation_id=new_reg.regulation_id,
            version_number=1,
            original_uri=data["url"],
            hash_value=data["hash_value"]
        )
        self.db.add(new_version)
        
        history = RegulationChangeHistory(
            version=new_version,
            change_type="new", 
            change_summary="ìµœì´ˆ ìˆ˜ì§‘ë¨"
        )
        self.db.add(history)
        
        await self.db.commit()
        print(f"âœ¨ [New] ì‹ ê·œ ê·œì œ ë“±ë¡: {data['title'][:30]}...")

        if file_path:
            await self.preprocess_agent.run(file_path, data)
            
        return "created"

    async def _handle_existing_regulation(self, regulation: Regulation, data: dict, crawler):
        stmt = select(RegulationVersion).where(
            RegulationVersion.regulation_id == regulation.regulation_id
        ).order_by(desc(RegulationVersion.version_number)).limit(1)
        
        result = await self.db.execute(stmt)
        latest_version = result.scalar_one_or_none()

        if latest_version and latest_version.hash_value == data["hash_value"]:
            return "skipped"

        print(f"ğŸ”„ ë³€ê²½ ê°ì§€ë¨! íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

        new_v_num = latest_version.version_number + 1 if latest_version else 1
        
        new_version = RegulationVersion(
            regulation_id=regulation.regulation_id,
            version_number=new_v_num,
            original_uri=data["url"],
            hash_value=data["hash_value"]
        )
        self.db.add(new_version)

        history = RegulationChangeHistory(
            version=new_version,
            change_type="append",
            change_summary=f"ë²„ì „ {new_v_num}ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨"
        )
        self.db.add(history)
        
        await self.db.commit()
        print(f"ğŸ”„ [Update] ê·œì œ ì—…ë°ì´íŠ¸ ì™„ë£Œ (v{new_v_num})")

        if file_path:
            await self.preprocess_agent.run(file_path, data)

        return "updated"