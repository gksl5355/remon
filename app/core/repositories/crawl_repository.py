import os
import aiofiles
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, desc
from typing import Optional

# S3 ì—…ë¡œë” ë° ëª¨ë¸ ì„í¬íŠ¸
from app.utils.s3_uploader import S3Uploader
from app.core.models.regulation_model import Regulation, RegulationVersion, RegulationChangeHistory
from app.crawler.crawling_regulation.base import UniversalFetcher

class CrawlRepository:
    def __init__(self, db: AsyncSession):
        self.db = db
        self.s3_uploader = S3Uploader()
        self.local_backup_dir = "db"

    async def process_crawled_data(self, data: dict, crawler: Optional[UniversalFetcher] = None):
        """
        DiscoveryAgentì—ì„œ í˜¸ì¶œí•˜ëŠ” ì§„ì…ì 
        """
        should_close_crawler = False
        if not crawler:
            crawler = UniversalFetcher()
            should_close_crawler = True

        try:
            url = data["url"]
            
            # DB ì¤‘ë³µ ì²´í¬
            stmt = (
                select(Regulation)
                .join(RegulationVersion, Regulation.regulation_id == RegulationVersion.regulation_id)
                .where(RegulationVersion.original_uri == url)
                .limit(1)
            )
            result = await self.db.execute(stmt)
            existing_reg = result.scalar_one_or_none()

            if not existing_reg:
                return await self._create_new_regulation(data, crawler)
            else:
                return await self._handle_existing_regulation(existing_reg, data, crawler)
        finally:
            if should_close_crawler:
                await crawler.close()

    async def _upload_and_get_path(self, url: str, hash_value: str, crawler: UniversalFetcher, category: str) -> str:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ í›„ S3 ì—…ë¡œë“œ (ê²½ë¡œ ë°˜í™˜ì€ í•˜ì§€ë§Œ DB ì €ì¥ì€ ìƒëµë¨)"""
        # 1. íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        content = await crawler.fetch_binary(url)
        if not content:
            return None

        # 2. í™•ì¥ì íŒë³„
        is_pdf = content.startswith(b'%PDF') or url.lower().endswith(".pdf")
        ext = 'pdf' if is_pdf else 'txt'
        filename = f"{hash_value}.{ext}"

        # 3. S3 ì—…ë¡œë“œ ì‹œë„
        s3_path = self.s3_uploader.upload_file(content, filename, folder=category)
        
        if s3_path:
            print(f"âœ… S3 ì €ì¥ ì„±ê³µ: {s3_path}")
            return s3_path

        # 4. ì‹¤íŒ¨ ì‹œ ë¡œì»¬ ë°±ì—…
        print("âš ï¸ S3 ì—…ë¡œë“œ ì‹¤íŒ¨ -> ë¡œì»¬ ë°±ì—… ì§„í–‰")
        save_dir = os.path.join(self.local_backup_dir, category)
        os.makedirs(save_dir, exist_ok=True)
        local_path = os.path.join(save_dir, filename)
        
        async with aiofiles.open(local_path, "wb") as f:
            await f.write(content)
            
        return local_path

    async def _create_new_regulation(self, data: dict, crawler: UniversalFetcher):
        category = data.get("category", "regulation")
        
        # 1. íŒŒì¼ ì—…ë¡œë“œ (S3ì—ëŠ” ì˜¬ë¼ê°)
        storage_path = await self._upload_and_get_path(data["url"], data["hash_value"], crawler, category)

        if not storage_path:
            return "failed"

        # 2. Regulation í…Œì´ë¸” ì €ì¥
        new_reg = Regulation(
            source_id=data.get("source_id", 99),
            country_code=data.get("country_code", "ZZ"),
            title=data.get("title", "No Title"),
            status="active"
        )
        self.db.add(new_reg)
        await self.db.flush()
        
        # 3. RegulationVersion í…Œì´ë¸” ì €ì¥
        # [ìˆ˜ì •] file_path ì¸ì ì œê±° (DB ìŠ¤í‚¤ë§ˆì— ì»¬ëŸ¼ì´ ì—†ìœ¼ë¯€ë¡œ)
        new_version = RegulationVersion(
            regulation_id=new_reg.regulation_id,
            version_number=1,
            original_uri=data["url"],
            # file_path=storage_path,  <-- [ì‚­ì œë¨] ERDì— ì»¬ëŸ¼ì´ ì—†ì–´ì„œ ì—ëŸ¬ ìœ ë°œ
            hash_value=data["hash_value"]
        )
        self.db.add(new_version)
        
        history = RegulationChangeHistory(
            version=new_version,
            change_type="NE",
            change_summary=f"ìˆ˜ì§‘ë¨ ({category})"
        )
        self.db.add(history)
        
        await self.db.commit()
        print(f"âœ¨ [DB ë“±ë¡] {new_reg.title[:20]}... (S3 Uploaded)")
        return "created"

    async def _handle_existing_regulation(self, regulation: Regulation, data: dict, crawler: UniversalFetcher):
        category = data.get("category", "regulation")
        
        stmt = select(RegulationVersion).where(RegulationVersion.regulation_id == regulation.regulation_id).order_by(desc(RegulationVersion.version_number)).limit(1)
        result = await self.db.execute(stmt)
        latest_version = result.scalar_one_or_none()

        if latest_version and latest_version.hash_value == data["hash_value"]:
            return "skipped"

        # íŒŒì¼ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ ë° ì—…ë¡œë“œ
        storage_path = await self._upload_and_get_path(data["url"], data["hash_value"], crawler, category)
        
        new_v_num = latest_version.version_number + 1
        new_version = RegulationVersion(
            regulation_id=regulation.regulation_id,
            version_number=new_v_num,
            original_uri=data["url"],
            # file_path=storage_path, <-- [ì‚­ì œë¨]
            hash_value=data["hash_value"]
        )
        self.db.add(new_version)
        
        history = RegulationChangeHistory(
            version=new_version,
            change_type="A",
            change_summary=f"ì—…ë°ì´íŠ¸ë¨ ({category})"
        )
        self.db.add(history)
        
        await self.db.commit()
        print(f"ğŸ”„ [ì—…ë°ì´íŠ¸] v{new_v_num} (S3 Uploaded)")
        return "updated"

    # (_handle_existing_regulation ë©”ì„œë“œë„ ë™ì¼í•˜ê²Œ _upload_and_get_path ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì • í•„ìš”)

    # ... (_handle_existing_regulation ë„ ë™ì¼í•˜ê²Œ storage_path ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •) ...
