import os
import aiofiles
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, desc
from typing import Optional

# S3 ì—…ë¡œë” ë° ëª¨ë¸ ì„í¬íŠ¸
from app.utils.s3_uploader import S3Uploader
from app.core.models.regulation_model import Regulation, RegulationVersion, RegulationChangeHistory
from app.crawler.crawling_regulation.base import UniversalFetcher

class CrawlRepository:
    def __init__(self, db: AsyncSession):
        self.db = db
        self.s3_uploader = S3Uploader()
        self.local_backup_dir = "db"

    async def process_crawled_data(self, data: dict, crawler: Optional[UniversalFetcher] = None):
        """
        DiscoveryAgentì—ì„œ í˜¸ì¶œí•˜ëŠ” ì§„ì…ì 
        """
        should_close_crawler = False
        if not crawler:
            crawler = UniversalFetcher()
            should_close_crawler = True

        try:
            url = data["url"]
            
            # DB ì¤‘ë³µ ì²´í¬
            stmt = (
                select(Regulation)
                .join(RegulationVersion, Regulation.regulation_id == RegulationVersion.regulation_id)
                .where(RegulationVersion.original_uri == url)
                .limit(1)
            )
            result = await self.db.execute(stmt)
            existing_reg = result.scalar_one_or_none()

            if not existing_reg:
                return await self._create_new_regulation(data, crawler)
            else:
                return await self._handle_existing_regulation(existing_reg, data, crawler)
        finally:
            if should_close_crawler:
                await crawler.close()

    async def _upload_and_get_path(self, url: str, hash_value: str, crawler: UniversalFetcher, category: str) -> str:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ í›„ S3 ì—…ë¡œë“œ (ê²½ë¡œ ë°˜í™˜ì€ í•˜ì§€ë§Œ DB ì €ì¥ì€ ìƒëµë¨)"""
        # 1. íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        content = await crawler.fetch_binary(url)
        if not content:
            return None

        # 2. í™•ì¥ì íŒë³„
        is_pdf = content.startswith(b'%PDF') or url.lower().endswith(".pdf")
        ext = 'pdf' if is_pdf else 'txt'
        filename = f"{hash_value}.{ext}"

        # 3. S3 ì—…ë¡œë“œ ì‹œë„
        s3_path = self.s3_uploader.upload_file(content, filename, folder=category)
        
        if s3_path:
            print(f"âœ… S3 ì €ì¥ ì„±ê³µ: {s3_path}")
            return s3_path

        # 4. ì‹¤íŒ¨ ì‹œ ë¡œì»¬ ë°±ì—…
        print("âš ï¸ S3 ì—…ë¡œë“œ ì‹¤íŒ¨ -> ë¡œì»¬ ë°±ì—… ì§„í–‰")
        save_dir = os.path.join(self.local_backup_dir, category)
        os.makedirs(save_dir, exist_ok=True)
        local_path = os.path.join(save_dir, filename)
        
        async with aiofiles.open(local_path, "wb") as f:
            await f.write(content)
            
        return local_path

    async def _create_new_regulation(self, data: dict, crawler: UniversalFetcher):
        category = data.get("category", "regulation")
        
        # 1. íŒŒì¼ ì—…ë¡œë“œ (S3ì—ëŠ” ì˜¬ë¼ê°)
        storage_path = await self._upload_and_get_path(data["url"], data["hash_value"], crawler, category)

        if not storage_path:
            return "failed"

        # 2. Regulation í…Œì´ë¸” ì €ì¥
        new_reg = Regulation(
            source_id=data.get("source_id", 99),
            country_code=data.get("country_code", "ZZ"),
            title=data.get("title", "No Title"),
            status="active"
        )
        self.db.add(new_reg)
        await self.db.flush()
        
        # 3. RegulationVersion í…Œì´ë¸” ì €ì¥
        # [ìˆ˜ì •] file_path ì¸ì ì œê±° (DB ìŠ¤í‚¤ë§ˆì— ì»¬ëŸ¼ì´ ì—†ìœ¼ë¯€ë¡œ)
        new_version = RegulationVersion(
            regulation_id=new_reg.regulation_id,
            version_number=1,
            original_uri=data["url"],
            # file_path=storage_path,  <-- [ì‚­ì œë¨] ERDì— ì»¬ëŸ¼ì´ ì—†ì–´ì„œ ì—ëŸ¬ ìœ ë°œ
            hash_value=data["hash_value"]
        )
        self.db.add(new_version)
        
        history = RegulationChangeHistory(
            version=new_version,
            change_type="NE",
            change_summary=f"ìˆ˜ì§‘ë¨ ({category})"
        )
        self.db.add(history)
        
        await self.db.commit()
        print(f"âœ¨ [DB ë“±ë¡] {new_reg.title[:20]}... (S3 Uploaded)")
        return "created"

    async def _handle_existing_regulation(self, regulation: Regulation, data: dict, crawler: UniversalFetcher):
        category = data.get("category", "regulation")
        
        stmt = select(RegulationVersion).where(RegulationVersion.regulation_id == regulation.regulation_id).order_by(desc(RegulationVersion.version_number)).limit(1)
        result = await self.db.execute(stmt)
        latest_version = result.scalar_one_or_none()

        if latest_version and latest_version.hash_value == data["hash_value"]:
            return "skipped"

        # íŒŒì¼ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ ë° ì—…ë¡œë“œ
        storage_path = await self._upload_and_get_path(data["url"], data["hash_value"], crawler, category)
        
        new_v_num = latest_version.version_number + 1
        new_version = RegulationVersion(
            regulation_id=regulation.regulation_id,
            version_number=new_v_num,
            original_uri=data["url"],
            # file_path=storage_path, <-- [ì‚­ì œë¨]
            hash_value=data["hash_value"]
        )
        self.db.add(new_version)
        
        history = RegulationChangeHistory(
            version=new_version,
            change_type="A",
            change_summary=f"ì—…ë°ì´íŠ¸ë¨ ({category})"
        )
        self.db.add(history)
        
        await self.db.commit()
        print(f"ğŸ”„ [ì—…ë°ì´íŠ¸] v{new_v_num} (S3 Uploaded)")
        return "updated"

    # (_handle_existing_regulation ë©”ì„œë“œë„ ë™ì¼í•˜ê²Œ _upload_and_get_path ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì • í•„ìš”)

    # ... (_handle_existing_regulation ë„ ë™ì¼í•˜ê²Œ storage_path ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •) ...

# import os
# import aiofiles
# from sqlalchemy.ext.asyncio import AsyncSession
# from sqlalchemy import select, desc
# from datetime import datetime
# from typing import Optional

# # ëª¨ë¸ ì„í¬íŠ¸ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
# from app.core.models.regulation_model import Regulation, RegulationVersion, RegulationChangeHistory
# # ì „ì²˜ë¦¬ ì—ì´ì „íŠ¸ ì„í¬íŠ¸
# from app.ai_pipeline.preprocess.preprocess_agent import PreprocessAgent
# from app.crawler.crawling_regulation.base import UniversalFetcher

# class CrawlRepository:
#     def __init__(self, db: AsyncSession):
#         self.db = db
#         # [ì¤‘ìš”] ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
#         self.preprocess_agent = PreprocessAgent()
#         self.base_dir = "db"

#     async def process_crawled_data(self, data: dict, crawler: Optional[UniversalFetcher] = None):
#         # ... (ê¸°ì¡´ê³¼ ë™ì¼: í¬ë¡¤ëŸ¬ ìƒì„± ë° DB ì¤‘ë³µ ì²´í¬ ë¡œì§) ...
#         should_close_crawler = False
#         if not crawler:
#             crawler = UniversalFetcher()
#             should_close_crawler = True

#         try:
#             url = data["url"]
#             stmt = (
#                 select(Regulation)
#                 .join(RegulationVersion, Regulation.regulation_id == RegulationVersion.regulation_id)
#                 .where(RegulationVersion.original_uri == url)
#                 .limit(1)
#             )
#             result = await self.db.execute(stmt)
#             existing_reg = result.scalar_one_or_none()

#             if not existing_reg:
#                 return await self._create_new_regulation(data, crawler)
#             else:
#                 return await self._handle_existing_regulation(existing_reg, data, crawler)
#         finally:
#             if should_close_crawler:
#                 await crawler.close()

#     async def _save_file_locally(self, url: str, hash_value: str, crawler: UniversalFetcher, category: str) -> Optional[str]:
#         # ... (íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ë§¤ì§ë°”ì´íŠ¸ ì²´í¬ ë¡œì§ì€ ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€) ...
#         # (ì½”ë“œê°€ ê¸¸ì–´ ìƒëµí•˜ì§€ë§Œ, ì´ì „ì— ì‘ì„±í•œ _save_file_locally ë¡œì§ì„ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”)
        
#         # [í•µì‹¬] ë‹¤ìš´ë¡œë“œëŠ” ë¡œì§ ë³€ê²½ ì—†ìŒ
#         save_dir = os.path.join(self.base_dir, category)
#         os.makedirs(save_dir, exist_ok=True)
        
#         content = await crawler.fetch_binary(url)
#         if not content: return None

#         is_pdf = content.startswith(b'%PDF') or url.lower().endswith(".pdf")
#         filename = f"{hash_value}.{'pdf' if is_pdf else 'txt'}"
#         file_path = os.path.join(save_dir, filename)

#         if os.path.exists(file_path):
#             return file_path

#         async with aiofiles.open(file_path, "wb") as f:
#             await f.write(content) # HTMLì´ë©´ í…ìŠ¤íŠ¸ ë³€í™˜ ë¡œì§ì´ ë“¤ì–´ê°€ì•¼ í•˜ì§€ë§Œ í¸ì˜ìƒ ìƒëµ
            
#         print(f"ğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_path}")
#         return file_path

#     async def _create_new_regulation(self, data: dict, crawler: UniversalFetcher):
#         category = data.get("category", "regulation")
        
#         # 1. íŒŒì¼ ë‹¤ìš´ë¡œë“œ
#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler, category)

#         # 2. DB ì €ì¥ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
#         new_reg = Regulation(
#             source_id=data.get("source_id", 99),
#             country_code=data.get("country_code", "ZZ"),
#             title=data.get("title", "No Title"),
#             status="active"
#         )
#         self.db.add(new_reg)
#         await self.db.flush()
        
#         # ... (RegulationVersion, History ì¶”ê°€ ë¡œì§ ìƒëµ) ...
        
#         await self.db.commit()
#         print(f"âœ¨ [DB] ì‹ ê·œ ê·œì œ ë“±ë¡ ì™„ë£Œ: {data.get('title')[:20]}...")

#         # 3. [í•µì‹¬] ì—¬ê¸°ì„œ ì „ì²˜ë¦¬ ì—ì´ì „íŠ¸(AI)ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤!
#         if file_path:
#             print(f"â¡ï¸ ì „ì²˜ë¦¬ ì—ì´ì „íŠ¸ í˜¸ì¶œ (íŒŒì¼: {file_path})")
#             await self.preprocess_agent.run(file_path, data)
#         else:
#             print("âš ï¸ íŒŒì¼ì´ ì €ì¥ë˜ì§€ ì•Šì•„ AI ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

#         return "created"

#     async def _handle_existing_regulation(self, regulation: Regulation, data: dict, crawler: UniversalFetcher):
#         # ... (ë²„ì „ ì²´í¬ ë¡œì§ ìƒëµ) ...
        
#         # ì—…ë°ì´íŠ¸ ë°œìƒ ì‹œ
#         category = data.get("category", "regulation")
#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler, category)

#         # ... (DB ì—…ë°ì´íŠ¸ ë¡œì§ ìƒëµ) ...
#         await self.db.commit()

#         # [í•µì‹¬] ì—…ë°ì´íŠ¸ ì‹œì—ë„ AI í˜¸ì¶œ
#         if file_path:
#             print(f"â¡ï¸ [Update] ì „ì²˜ë¦¬ ì—ì´ì „íŠ¸ í˜¸ì¶œ (íŒŒì¼: {file_path})")
#             await self.preprocess_agent.run(file_path, data)

#         return "updated"


# import os
# import aiofiles
# from bs4 import BeautifulSoup
# from sqlalchemy.ext.asyncio import AsyncSession
# from sqlalchemy import select, desc
# from datetime import datetime
# from typing import Optional

# from app.core.models.regulation_model import Regulation, RegulationVersion, RegulationChangeHistory
# from app.ai_pipeline.preprocess.preprocess_agent import PreprocessAgent
# from app.crawler.crawling_regulation.base import UniversalFetcher

# class CrawlRepository:
#     def __init__(self, db: AsyncSession):
#         self.db = db
#         self.preprocess_agent = PreprocessAgent()
#         # ê¸°ë³¸ ê²½ë¡œ (ìƒì„±ìì—ì„œëŠ” baseë§Œ ì¡ìŒ)
#         self.base_dir = "db" 

#     async def process_crawled_data(self, data: dict, crawler: Optional[UniversalFetcher] = None):
#         should_close_crawler = False
#         if not crawler:
#             crawler = UniversalFetcher()
#             should_close_crawler = True

#         try:
#             url = data["url"]
#             # DB ë¡œì§ì€ ë™ì¼...
#             stmt = (
#                 select(Regulation)
#                 .join(RegulationVersion, Regulation.regulation_id == RegulationVersion.regulation_id)
#                 .where(RegulationVersion.original_uri == url)
#                 .limit(1)
#             )
#             result = await self.db.execute(stmt)
#             existing_reg = result.scalar_one_or_none()

#             if not existing_reg:
#                 return await self._create_new_regulation(data, crawler)
#             else:
#                 return await self._handle_existing_regulation(existing_reg, data, crawler)
#         finally:
#             if should_close_crawler:
#                 await crawler.close()

#     async def _save_file_locally(self, url: str, hash_value: str, crawler: UniversalFetcher, category: str = "regulation") -> Optional[str]:
#         """
#         [ìˆ˜ì •] category ì¸ìë¥¼ ë°›ì•„ì„œ ì €ì¥ í´ë”ë¥¼ ë™ì ìœ¼ë¡œ ê²°ì •
#         """
#         if not crawler:
#             return None

#         # 1. ì €ì¥ ê²½ë¡œ ê²°ì • (regulation vs news)
#         # ì˜ˆ: db/regulation/abc.pdf ë˜ëŠ” db/news/xyz.txt
#         save_dir = os.path.join(self.base_dir, category)
#         os.makedirs(save_dir, exist_ok=True)

#         # 2. ë‹¤ìš´ë¡œë“œ ë° ë§¤ì§ ë°”ì´íŠ¸ ì²´í¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
#         content = await crawler.fetch_binary(url)
#         if not content:
#             return None

#         is_pdf = content.startswith(b'%PDF') or url.lower().endswith(".pdf")

#         if is_pdf:
#             filename = f"{hash_value}.pdf"
#             file_path = os.path.join(save_dir, filename)
#             if os.path.exists(file_path): return file_path
            
#             async with aiofiles.open(file_path, "wb") as f:
#                 await f.write(content)
#             print(f"ğŸ’¾ [{category.upper()}] PDF ì €ì¥: {file_path}")
#             return file_path

#         else:
#             filename = f"{hash_value}.txt"
#             file_path = os.path.join(save_dir, filename)
#             if os.path.exists(file_path): return file_path

#             try:
#                 html_text = content.decode('utf-8')
#             except:
#                 try: html_text = content.decode('latin-1')
#                 except: return None

#             soup = BeautifulSoup(html_text, "lxml")
#             for script in soup(["script", "style", "header", "footer", "nav", "noscript"]):
#                 script.extract()
#             clean_text = soup.get_text(separator="\n", strip=True)

#             async with aiofiles.open(file_path, "w", encoding="utf-8") as f:
#                 await f.write(clean_text)
            
#             print(f"ğŸ’¾ [{category.upper()}] í…ìŠ¤íŠ¸ ì €ì¥: {file_path}")
#             return file_path

#     async def _create_new_regulation(self, data: dict, crawler: UniversalFetcher):
#         # [ìˆ˜ì •] data ë”•ì…”ë„ˆë¦¬ì—ì„œ categoryë¥¼ êº¼ë‚´ì„œ ì „ë‹¬
#         category = data.get("category", "regulation")
#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler, category)

#         # (DB ì €ì¥ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼)
#         # ë‹¨, Newsì¸ ê²½ìš° DBì— íƒœê·¸ë¥¼ ë‹¤ë¥´ê²Œ ë‹¬ê±°ë‚˜ ë³„ë„ í…Œì´ë¸”ë¡œ ëº„ ìˆ˜ë„ ìˆì§€ë§Œ,
#         # ì¼ë‹¨ì€ Regulation í…Œì´ë¸”ì— ì €ì¥í•˜ë˜ titleì— íƒœê·¸ë¥¼ ë¶™ì´ëŠ” ì‹ìœ¼ë¡œ êµ¬ë¶„ ê°€ëŠ¥
        
#         # ... (DB Insert ì½”ë“œ ìƒëµ - ê¸°ì¡´ê³¼ ë™ì¼) ...
#         # ... (ìƒˆë¡œìš´ íŒŒì¼ì´ ìˆìœ¼ë©´ PreprocessAgent ì‹¤í–‰) ...
        
#         # ì—¬ê¸°ì„œëŠ” ìƒëµí–ˆì§€ë§Œ, ì‹¤ì œ ì½”ë“œì—ëŠ” DB Insert ë¶€ë¶„ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
#         # í¸ì˜ìƒ í•µì‹¬ì¸ _save_file_locally í˜¸ì¶œë¶€ë§Œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
        
#         # [ë³µì›ìš© DB ì½”ë“œ]
#         proclaimed_date = None
#         if data.get("proclaimed_date"):
#             # ... ë‚ ì§œ ì²˜ë¦¬ ...
#             pass
            
#         new_reg = Regulation(
#             source_id=data.get("source_id", 1),
#             country_code=data.get("country_code", "US"),
#             title=f"[{category.upper()}] {data.get('title', 'No Title')}", # ì œëª©ì— ì¹´í…Œê³ ë¦¬ í‘œì‹œ
#             status="active"
#         )
#         self.db.add(new_reg)
#         await self.db.flush()
        
#         new_version = RegulationVersion(
#             regulation_id=new_reg.regulation_id,
#             version_number=1,
#             original_uri=data["url"],
#             hash_value=data["hash_value"]
#         )
#         self.db.add(new_version)
        
#         history = RegulationChangeHistory(
#             version=new_version,
#             change_type="new",
#             change_summary=f"ìˆ˜ì§‘ë¨ ({category})"
#         )
#         self.db.add(history)
#         await self.db.commit()

#         if file_path:
#             await self.preprocess_agent.run(file_path, data)
            
#         return "created"

#     async def _handle_existing_regulation(self, regulation: Regulation, data: dict, crawler: UniversalFetcher):
#         # [ìˆ˜ì •] ì—…ë°ì´íŠ¸ ì‹œì—ë„ ì¹´í…Œê³ ë¦¬ ì „ë‹¬
#         category = data.get("category", "regulation")
        
#         # ... (ë²„ì „ ì²´í¬ ë¡œì§ ê¸°ì¡´ ë™ì¼) ...
#         stmt = select(RegulationVersion).where(RegulationVersion.regulation_id == regulation.regulation_id).order_by(desc(RegulationVersion.version_number)).limit(1)
#         result = await self.db.execute(stmt)
#         latest_version = result.scalar_one_or_none()

#         if latest_version and latest_version.hash_value == data["hash_value"]:
#             return "skipped"

#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler, category)
        
#         # ... (ë²„ì „ ì—…ë°ì´íŠ¸ DB ë¡œì§ ê¸°ì¡´ ë™ì¼) ...
#         new_v_num = latest_version.version_number + 1
#         new_version = RegulationVersion(
#             regulation_id=regulation.regulation_id,
#             version_number=new_v_num,
#             original_uri=data["url"],
#             hash_value=data["hash_value"]
#         )
#         self.db.add(new_version)
#         history = RegulationChangeHistory(
#             version=new_version,
#             change_type="append",
#             change_summary=f"ì—…ë°ì´íŠ¸ë¨ ({category})"
#         )
#         self.db.add(history)
#         await self.db.commit()

#         if file_path:
#             await self.preprocess_agent.run(file_path, data)

#         return "updated"



################################################################


# import os
# import aiofiles
# from bs4 import BeautifulSoup
# from sqlalchemy.ext.asyncio import AsyncSession
# from sqlalchemy import select, desc
# from datetime import datetime
# from typing import Optional

# from app.core.models.regulation_model import Regulation, RegulationVersion, RegulationChangeHistory
# from app.ai_pipeline.preprocess.preprocess_agent import PreprocessAgent
# from app.crawler.crawling_regulation.base import UniversalFetcher

# class CrawlRepository:
#     def __init__(self, db: AsyncSession):
#         self.db = db
#         self.preprocess_agent = PreprocessAgent()
#         self.save_dir = os.path.join("db", "regulation")
#         os.makedirs(self.save_dir, exist_ok=True)

#     async def process_crawled_data(self, data: dict, crawler: Optional[UniversalFetcher] = None):
#         should_close_crawler = False
#         if not crawler:
#             crawler = UniversalFetcher()
#             should_close_crawler = True

#         try:
#             url = data["url"]
            
#             # DB ì¤‘ë³µ ì²´í¬
#             stmt = (
#                 select(Regulation)
#                 .join(RegulationVersion, Regulation.regulation_id == RegulationVersion.regulation_id)
#                 .where(RegulationVersion.original_uri == url)
#                 .limit(1)
#             )
#             result = await self.db.execute(stmt)
#             existing_reg = result.scalar_one_or_none()

#             # ì‹ ê·œ ë“±ë¡ ë˜ëŠ” ì—…ë°ì´íŠ¸ ì²˜ë¦¬
#             if not existing_reg:
#                 return await self._create_new_regulation(data, crawler)
#             else:
#                 return await self._handle_existing_regulation(existing_reg, data, crawler)
#         finally:
#             if should_close_crawler:
#                 await crawler.close()

#     async def _save_file_locally(self, url: str, hash_value: str, crawler: UniversalFetcher) -> Optional[str]:
#         """
#         [ê°œì„ ëœ ë¡œì§] URL í™•ì¥ìê°€ ì•„ë‹Œ 'íŒŒì¼ ì‹¤ì œ í—¤ë”(Magic Bytes)'ë¡œ í˜•ì‹ì„ íŒë‹¨í•˜ì—¬ ì €ì¥
#         """
#         if not crawler:
#             return None

#         # 1. ì¼ë‹¨ ë°”ì´ë„ˆë¦¬ë¡œ ë‹¤ìš´ë¡œë“œ (PDFì¼ ìˆ˜ë„, HTMLì¼ ìˆ˜ë„ ìˆìŒ)
#         content = await crawler.fetch_binary(url)
#         if not content:
#             return None

#         # 2. íŒŒì¼ í˜•ì‹ íŒë³„ (Magic Bytes Check)
#         is_pdf = False
        
#         # PDF íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ í™•ì¸ (%PDF-)
#         if content.startswith(b'%PDF'):
#             is_pdf = True
        
#         # (ì˜µì…˜) URLì´ ê°•ì œë¡œ .pdfì¸ ê²½ìš°ë„ í¬í•¨
#         elif url.lower().endswith(".pdf"):
#             is_pdf = True

#         # 3. í˜•ì‹ì— ë”°ë¥¸ ì €ì¥ ë¶„ê¸°
#         if is_pdf:
#             # === PDF ì €ì¥ ===
#             filename = f"{hash_value}.pdf"
#             file_path = os.path.join(self.save_dir, filename)
            
#             if os.path.exists(file_path):
#                 return file_path

#             async with aiofiles.open(file_path, "wb") as f:
#                 await f.write(content)
#             print(f"ğŸ’¾ PDF ì €ì¥ ì™„ë£Œ (Auto-detected): {file_path}")
#             return file_path

#         else:
#             # === HTML/Text ì €ì¥ ===
#             filename = f"{hash_value}.txt"
#             file_path = os.path.join(self.save_dir, filename)

#             if os.path.exists(file_path):
#                 return file_path

#             try:
#                 # ë°”ì´ë„ˆë¦¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”© (ëŸ¬ì‹œì•„ì–´ ë“± ê¹¨ì§ ë°©ì§€ ì‹œë„)
#                 # 1ì°¨ ì‹œë„: utf-8
#                 html_text = content.decode('utf-8')
#             except UnicodeDecodeError:
#                 try:
#                     # 2ì°¨ ì‹œë„: latin-1 (í˜¹ì€ chardet ì‚¬ìš© ê°€ëŠ¥)
#                     html_text = content.decode('latin-1')
#                 except:
#                     print(f"âš ï¸ ë””ì½”ë”© ì‹¤íŒ¨: {url}")
#                     return None

#             # BeautifulSoupìœ¼ë¡œ íƒœê·¸ ì œê±°í•˜ê³  ë³¸ë¬¸ë§Œ ì¶”ì¶œ
#             soup = BeautifulSoup(html_text, "lxml")
            
#             # ë…¸ì´ì¦ˆ ì œê±°
#             for script in soup(["script", "style", "header", "footer", "nav", "iframe", "noscript"]):
#                 script.extract()
            
#             clean_text = soup.get_text(separator="\n", strip=True)

#             async with aiofiles.open(file_path, "w", encoding="utf-8") as f:
#                 await f.write(clean_text)
            
#             print(f"ğŸ’¾ í…ìŠ¤íŠ¸ ë³€í™˜ ë° ì €ì¥ ì™„ë£Œ: {file_path}")
#             return file_path

#     async def _create_new_regulation(self, data: dict, crawler: UniversalFetcher):
#         # íŒŒì¼ ì €ì¥ í˜¸ì¶œ
#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

#         proclaimed_date = None
#         if data.get("proclaimed_date"):
#             try:
#                 if isinstance(data["proclaimed_date"], str):
#                     proclaimed_date = datetime.strptime(data["proclaimed_date"], "%Y-%m-%d").date()
#                 else:
#                     proclaimed_date = data["proclaimed_date"]
#             except ValueError:
#                 pass

#         new_reg = Regulation(
#             source_id=data.get("source_id", 1),
#             country_code=data.get("country_code", "US"),
#             title=data.get("title", "No Title"),
#             proclaimed_date=proclaimed_date,
#             status="active"
#         )
#         self.db.add(new_reg)
#         await self.db.flush()

#         new_version = RegulationVersion(
#             regulation_id=new_reg.regulation_id,
#             version_number=1,
#             original_uri=data["url"],
#             hash_value=data["hash_value"]
#         )
#         self.db.add(new_version)
        
#         history = RegulationChangeHistory(
#             version=new_version,
#             change_type="new", 
#             change_summary="ìµœì´ˆ ìˆ˜ì§‘ë¨ (Discovery Agent)"
#         )
#         self.db.add(history)
        
#         await self.db.commit()
#         print(f"âœ¨ [New] ì‹ ê·œ ê·œì œ ë“±ë¡: {new_reg.title[:30]}...")

#         if file_path:
#             # ì „ì²˜ë¦¬ ì—ì´ì „íŠ¸ ì‹¤í–‰
#             await self.preprocess_agent.run(file_path, data)
            
#         return "created"

#     async def _handle_existing_regulation(self, regulation: Regulation, data: dict, crawler: UniversalFetcher):
#         stmt = select(RegulationVersion).where(
#             RegulationVersion.regulation_id == regulation.regulation_id
#         ).order_by(desc(RegulationVersion.version_number)).limit(1)
        
#         result = await self.db.execute(stmt)
#         latest_version = result.scalar_one_or_none()

#         if latest_version and latest_version.hash_value == data["hash_value"]:
#             return "skipped"

#         print(f"ğŸ”„ ë³€ê²½ ê°ì§€ë¨! íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

#         new_v_num = latest_version.version_number + 1 if latest_version else 1
        
#         new_version = RegulationVersion(
#             regulation_id=regulation.regulation_id,
#             version_number=new_v_num,
#             original_uri=data["url"],
#             hash_value=data["hash_value"]
#         )
#         self.db.add(new_version)

#         history = RegulationChangeHistory(
#             version=new_version,
#             change_type="append",
#             change_summary=f"ë²„ì „ {new_v_num}ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨"
#         )
#         self.db.add(history)
        
#         await self.db.commit()
#         print(f"ğŸ”„ [Update] ê·œì œ ì—…ë°ì´íŠ¸ ì™„ë£Œ (v{new_v_num})")

#         if file_path:
#             await self.preprocess_agent.run(file_path, data)

#         return "updated"

# import os
# import aiofiles
# from bs4 import BeautifulSoup
# from sqlalchemy.ext.asyncio import AsyncSession
# from sqlalchemy import select, desc
# from datetime import datetime
# from typing import Optional

# from app.core.models.regulation_model import Regulation, RegulationVersion, RegulationChangeHistory
# from app.ai_pipeline.preprocess.preprocess_agent import PreprocessAgent
# from app.crawler.crawling_regulation.base import UniversalFetcher  # [ìˆ˜ì •] UniversalFetcher ì„í¬íŠ¸

# class CrawlRepository:
#     def __init__(self, db: AsyncSession):
#         self.db = db
#         self.preprocess_agent = PreprocessAgent()
#         self.save_dir = os.path.join("db", "regulation")
#         os.makedirs(self.save_dir, exist_ok=True)

#     async def process_crawled_data(self, data: dict, crawler: Optional[UniversalFetcher] = None):
#         """
#         [ë²”ìš©ì„± ê°œì„ ] 
#         crawler ê°ì²´ê°€ ì—†ìœ¼ë©´ UniversalFetcherë¥¼ ì„ì‹œë¡œ ìƒì„±í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
#         ì´ë¥¼ í†µí•´ Discovery Agentê°€ URLë§Œ ë˜ì ¸ì¤˜ë„ ì•Œì•„ì„œ ë‹¤ìš´ë¡œë“œê¹Œì§€ ìˆ˜í–‰í•©ë‹ˆë‹¤.
#         """
#         should_close_crawler = False
#         if not crawler:
#             crawler = UniversalFetcher()
#             should_close_crawler = True

#         try:
#             url = data["url"]
            
#             stmt = (
#                 select(Regulation)
#                 .join(RegulationVersion, Regulation.regulation_id == RegulationVersion.regulation_id)
#                 .where(RegulationVersion.original_uri == url)
#                 .limit(1)
#             )
#             result = await self.db.execute(stmt)
#             existing_reg = result.scalar_one_or_none()

#             if not existing_reg:
#                 return await self._create_new_regulation(data, crawler)
#             else:
#                 return await self._handle_existing_regulation(existing_reg, data, crawler)
#         finally:
#             # ì„ì‹œë¡œ ë§Œë“  í¬ë¡¤ëŸ¬ë¼ë©´ ë‹«ì•„ì¤€ë‹¤
#             if should_close_crawler:
#                 await crawler.close()

#     async def _save_file_locally(self, url: str, hash_value: str, crawler: UniversalFetcher) -> Optional[str]:
#         if not crawler:
#             return None

#         # 1. PDF ì²˜ë¦¬
#         if url.lower().endswith(".pdf"):
#             filename = f"{hash_value}.pdf"
#             file_path = os.path.join(self.save_dir, filename)
            
#             if os.path.exists(file_path):
#                 return file_path

#             content = await crawler.fetch_binary(url)
#             if content:
#                 async with aiofiles.open(file_path, "wb") as f:
#                     await f.write(content)
#                 print(f"ğŸ’¾ PDF ì €ì¥ ì™„ë£Œ: {file_path}")
#                 return file_path
        
#         # 2. ì¼ë°˜ ì›¹í˜ì´ì§€ (HTML -> Text)
#         else:
#             filename = f"{hash_value}.txt"
#             file_path = os.path.join(self.save_dir, filename)

#             if os.path.exists(file_path):
#                 return file_path

#             html_content = await crawler.fetch(url)
#             if html_content:
#                 soup = BeautifulSoup(html_content, "lxml")
                
#                 # ë…¸ì´ì¦ˆ ì œê±°
#                 for script in soup(["script", "style", "header", "footer", "nav", "iframe", "noscript"]):
#                     script.extract()
                
#                 clean_text = soup.get_text(separator="\n", strip=True)

#                 async with aiofiles.open(file_path, "w", encoding="utf-8") as f:
#                     await f.write(clean_text)
                
#                 print(f"ğŸ’¾ í…ìŠ¤íŠ¸ ë³€í™˜ ë° ì €ì¥ ì™„ë£Œ: {file_path}")
#                 return file_path

#         return None

#     async def _create_new_regulation(self, data: dict, crawler: UniversalFetcher):
#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

#         # [ì•ˆì „ì¥ì¹˜] ë‚ ì§œ í¬ë§·ì´ ì•ˆ ë§ê±°ë‚˜ ì—†ìœ¼ë©´ None ì²˜ë¦¬
#         proclaimed_date = None
#         if data.get("proclaimed_date"):
#             try:
#                 # LLMì´ YYYY-MM-DD í˜•ì‹ì„ ì•ˆ ì§€ì¼°ì„ ê²½ìš° ëŒ€ë¹„
#                 if isinstance(data["proclaimed_date"], str):
#                     proclaimed_date = datetime.strptime(data["proclaimed_date"], "%Y-%m-%d").date()
#                 else:
#                     proclaimed_date = data["proclaimed_date"]
#             except ValueError:
#                 print(f"âš ï¸ ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜ (ê¸°ë¡ ìƒëµ): {data['proclaimed_date']}")

#         # [ë²”ìš©ì„±] source_idê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 1 ì‚¬ìš© (ì¶”í›„ 'General Web' ì†ŒìŠ¤ IDë¡œ ë³€ê²½ ê¶Œì¥)
#         source_id = data.get("source_id", 1)

#         new_reg = Regulation(
#             source_id=source_id,
#             country_code=data.get("country_code", "US"), # ê¸°ë³¸ê°’ US
#             title=data.get("title", "No Title"),
#             proclaimed_date=proclaimed_date,
#             status="active"
#         )
#         self.db.add(new_reg)
#         await self.db.flush()

#         new_version = RegulationVersion(
#             regulation_id=new_reg.regulation_id,
#             version_number=1,
#             original_uri=data["url"],
#             hash_value=data["hash_value"]
#         )
#         self.db.add(new_version)
        
#         history = RegulationChangeHistory(
#             version=new_version,
#             change_type="new", 
#             change_summary="ìµœì´ˆ ìˆ˜ì§‘ë¨ (Discovery Agent)"
#         )
#         self.db.add(history)
        
#         await self.db.commit()
#         print(f"âœ¨ [New] ì‹ ê·œ ê·œì œ ë“±ë¡: {new_reg.title[:30]}...")

#         if file_path:
#             # PreprocessAgent ì‹¤í–‰ (ë¹„ë™ê¸° ì²˜ë¦¬ ê¶Œì¥)
#             # await self.preprocess_agent.run(file_path, data) 
#             # -> ì„±ëŠ¥ì„ ìœ„í•´ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ë„˜ê¸°ëŠ” ê²ƒì„ ê³ ë ¤í•  ìˆ˜ ìˆìŒ
#             await self.preprocess_agent.run(file_path, data)
            
#         return "created"

#     async def _handle_existing_regulation(self, regulation: Regulation, data: dict, crawler: UniversalFetcher):
#         # ê¸°ì¡´ ë¡œì§ ìœ ì§€ (ë²„ì „ ê´€ë¦¬)
#         stmt = select(RegulationVersion).where(
#             RegulationVersion.regulation_id == regulation.regulation_id
#         ).order_by(desc(RegulationVersion.version_number)).limit(1)
        
#         result = await self.db.execute(stmt)
#         latest_version = result.scalar_one_or_none()

#         if latest_version and latest_version.hash_value == data["hash_value"]:
#             return "skipped"

#         print(f"ğŸ”„ ë³€ê²½ ê°ì§€ë¨! íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

#         new_v_num = latest_version.version_number + 1 if latest_version else 1
        
#         new_version = RegulationVersion(
#             regulation_id=regulation.regulation_id,
#             version_number=new_v_num,
#             original_uri=data["url"],
#             hash_value=data["hash_value"]
#         )
#         self.db.add(new_version)

#         history = RegulationChangeHistory(
#             version=new_version,
#             change_type="append",
#             change_summary=f"ë²„ì „ {new_v_num}ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨"
#         )
#         self.db.add(history)
        
#         await self.db.commit()
#         print(f"ğŸ”„ [Update] ê·œì œ ì—…ë°ì´íŠ¸ ì™„ë£Œ (v{new_v_num})")

#         if file_path:
#             await self.preprocess_agent.run(file_path, data)

#         return "updated"

# # app/services/regulation_service.py

# import os
# import aiofiles
# from bs4 import BeautifulSoup # [ì¶”ê°€] í…ìŠ¤íŠ¸ ì¶”ì¶œìš©
# from sqlalchemy.ext.asyncio import AsyncSession
# from sqlalchemy import select, desc
# from datetime import datetime

# from app.core.models.regulation_model import Regulation, RegulationVersion, RegulationChangeHistory
# from app.ai_pipeline.preprocess.preprocess_agent import PreprocessAgent

# class CrawlRepository:
#     def __init__(self, db: AsyncSession):
#         self.db = db
#         self.preprocess_agent = PreprocessAgent()
#         self.save_dir = os.path.join("db", "regulation")
#         os.makedirs(self.save_dir, exist_ok=True)

#     async def process_crawled_data(self, data: dict, crawler=None):
#         url = data["url"]
        
#         stmt = (
#             select(Regulation)
#             .join(RegulationVersion, Regulation.regulation_id == RegulationVersion.regulation_id)
#             .where(RegulationVersion.original_uri == url)
#             .limit(1)
#         )
#         result = await self.db.execute(stmt)
#         existing_reg = result.scalar_one_or_none()

#         if not existing_reg:
#             return await self._create_new_regulation(data, crawler)
#         else:
#             return await self._handle_existing_regulation(existing_reg, data, crawler)

#     # [í•µì‹¬ ìˆ˜ì •] íŒŒì¼ ì €ì¥ ë¡œì§ ë³€ê²½
#     async def _save_file_locally(self, url: str, hash_value: str, crawler) -> str:
#         if not crawler:
#             return None

#         # 1. PDFì¸ ê²½ìš°: ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ ë°”ì´ë„ˆë¦¬ ì €ì¥
#         if url.lower().endswith(".pdf"):
#             filename = f"{hash_value}.pdf"
#             file_path = os.path.join(self.save_dir, filename)
            
#             if os.path.exists(file_path):
#                 return file_path

#             content = await crawler.fetch_binary(url)
#             if content:
#                 async with aiofiles.open(file_path, "wb") as f:
#                     await f.write(content)
#                 print(f"ğŸ’¾ PDF ì €ì¥ ì™„ë£Œ: {file_path}")
#                 return file_path
        
#         # 2. HTML(ì›¹í˜ì´ì§€)ì¸ ê²½ìš°: í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ .txtë¡œ ì €ì¥
#         else:
#             filename = f"{hash_value}.txt" # í™•ì¥ìë¥¼ .txtë¡œ ë³€ê²½
#             file_path = os.path.join(self.save_dir, filename)

#             if os.path.exists(file_path):
#                 return file_path

#             # fetch()ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸(HTML) ê°€ì ¸ì˜¤ê¸°
#             html_content = await crawler.fetch(url)
#             if html_content:
#                 # BeautifulSoupìœ¼ë¡œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
#                 soup = BeautifulSoup(html_content, "lxml")
                
#                 # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ë„¤ë¹„ê²Œì´ì…˜ ë“±)
#                 for script in soup(["script", "style", "header", "footer", "nav", "iframe"]):
#                     script.extract()
                
#                 # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê³µë°± ì •ë¦¬)
#                 clean_text = soup.get_text(separator="\n", strip=True)

#                 # .txt íŒŒì¼ë¡œ ì €ì¥
#                 async with aiofiles.open(file_path, "w", encoding="utf-8") as f:
#                     await f.write(clean_text)
                
#                 print(f"ğŸ’¾ í…ìŠ¤íŠ¸ ë³€í™˜ ë° ì €ì¥ ì™„ë£Œ: {file_path}")
#                 return file_path

#         return None

#     async def _create_new_regulation(self, data: dict, crawler):
#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

#         new_reg = Regulation(
#             source_id=1,
#             country_code=data["country_code"],
#             title=data["title"],
#             proclaimed_date=datetime.strptime(data["proclaimed_date"], "%Y-%m-%d").date() if data.get("proclaimed_date") else None,
#             status="active"
#         )
#         self.db.add(new_reg)
#         await self.db.flush()

#         new_version = RegulationVersion(
#             regulation_id=new_reg.regulation_id,
#             version_number=1,
#             original_uri=data["url"],
#             hash_value=data["hash_value"]
#         )
#         self.db.add(new_version)
        
#         history = RegulationChangeHistory(
#             version=new_version,
#             change_type="new", 
#             change_summary="ìµœì´ˆ ìˆ˜ì§‘ë¨"
#         )
#         self.db.add(history)
        
#         await self.db.commit()
#         print(f"âœ¨ [New] ì‹ ê·œ ê·œì œ ë“±ë¡: {data['title'][:30]}...")

#         if file_path:
#             await self.preprocess_agent.run(file_path, data)
            
#         return "created"

#     async def _handle_existing_regulation(self, regulation: Regulation, data: dict, crawler):
#         stmt = select(RegulationVersion).where(
#             RegulationVersion.regulation_id == regulation.regulation_id
#         ).order_by(desc(RegulationVersion.version_number)).limit(1)
        
#         result = await self.db.execute(stmt)
#         latest_version = result.scalar_one_or_none()

#         if latest_version and latest_version.hash_value == data["hash_value"]:
#             return "skipped"

#         print(f"ğŸ”„ ë³€ê²½ ê°ì§€ë¨! íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
#         file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

#         new_v_num = latest_version.version_number + 1 if latest_version else 1
        
#         new_version = RegulationVersion(
#             regulation_id=regulation.regulation_id,
#             version_number=new_v_num,
#             original_uri=data["url"],
#             hash_value=data["hash_value"]
#         )
#         self.db.add(new_version)

#         history = RegulationChangeHistory(
#             version=new_version,
#             change_type="append",
#             change_summary=f"ë²„ì „ {new_v_num}ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨"
#         )
#         self.db.add(history)
        
#         await self.db.commit()
#         print(f"ğŸ”„ [Update] ê·œì œ ì—…ë°ì´íŠ¸ ì™„ë£Œ (v{new_v_num})")

#         if file_path:
#             await self.preprocess_agent.run(file_path, data)

#         return "updated"