"""
module: preprocess_orchestrator.py
description: ì „ì²´ Preprocess íŒŒì´í”„ë¼ì¸ ì¡°ìœ¨ (ëª¨ë“  ëª¨ë“ˆ í†µí•©)
author: AI Agent
created: 2025-11-12
updated: 2025-11-12
dependencies:
    - app.config.logger, app.ai_pipeline.preprocess.config
    - app.ai_pipeline.preprocess.[metadata_extractor, definition_extractor, ...]
    - typing, json, datetime
"""

from typing import List, Dict, Optional, Any
import logging
import json
from datetime import datetime
from pathlib import Path

from app.ai_pipeline.preprocess.config import PreprocessConfig
from app.ai_pipeline.preprocess.metadata_extractor import MetadataExtractor
from app.ai_pipeline.preprocess.pdf_processor import PDFProcessor
from app.ai_pipeline.preprocess.embedding_pipeline import EmbeddingPipeline
from app.ai_pipeline.preprocess.semantic_chunker import SemanticChunker
from app.ai_pipeline.preprocess.proposition_extractor import PropositionExtractor
from app.ai_pipeline.preprocess.table_detector import TableDetector
from app.ai_pipeline.preprocess.hierarchy_extractor import HierarchyExtractor
from app.ai_pipeline.preprocess.definition_extractor import DefinitionExtractor
from app.ai_pipeline.preprocess.embedding_to_vectordb import EmbeddingToVectorDB

logger = logging.getLogger(__name__)


class PreprocessOrchestrator:
    """
    ì „ì²´ Preprocess íŒŒì´í”„ë¼ì¸ì„ ì¡°ìœ¨í•˜ëŠ” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°.
    
    íŒŒì´í”„ë¼ì¸:
    1. PDF ë¡œë“œ & í…ìŠ¤íŠ¸ ì¶”ì¶œ (PDFProcessor)
    2. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (MetadataExtractor)
    3. ì •ì˜ & ê³„ì¸µ êµ¬ì¡° ì¶”ì¶œ (DefinitionExtractor, HierarchyExtractor)
    4. í…Œì´ë¸” ê°ì§€ (TableDetector)
    5. ì˜ë¯¸ ê¸°ë°˜ ì²­í¬ ë¶„í•  (SemanticChunker)
    6. ì„ë² ë”© ìƒì„± (EmbeddingPipeline)
    7. BM25 ì¸ë±ì‹± (BM25Indexer)
    8. Chroma VectorDBì— ì €ì¥í•  í˜•ì‹ ì¤€ë¹„
    
    ì¶œë ¥: ChromaDB ì €ì¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ (ë³„ë„ index_managerë¡œ ì €ì¥)
    """
    
    def __init__(self, config: PreprocessConfig = None):
        """
        ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”.
        
        Args:
            config (PreprocessConfig): ì„¤ì • ê°ì²´. Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        """
        self.config = config or PreprocessConfig()
        
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ ì´ˆê¸°í™”
        self.metadata_extractor = MetadataExtractor()
        self.pdf_processor = PDFProcessor()
        self.table_detector = TableDetector()
        self.hierarchy_extractor = HierarchyExtractor()
        self.definition_extractor = DefinitionExtractor()
        self.semantic_chunker = SemanticChunker(
            chunk_size=self.config.MAX_CHUNK_SIZE,
        )
        self.proposition_extractor = PropositionExtractor(
            api_key=self.config.OPENAI_API_KEY,
            model=self.config.OPENAI_MODEL_PROPOSITION,
            max_workers=3
        )
        self.embedding_pipeline = EmbeddingPipeline(
            model_name=self.config.EMBEDDING_MODEL,
            use_fp16=self.config.USE_FP16,
            batch_size=self.config.EMBEDDING_BATCH_SIZE,
            use_sparse=True  # Sparse ë²¡í„° í™œì„±í™”
        )
        self.embedding_to_vectordb = EmbeddingToVectorDB()
        
        # Parent-Child Hierarchy ì €ì¥ì†Œ
        self.parent_chunks: Dict[str, str] = {}
        self.processed_hashes: set = set()
        
        logger.info("âœ… PreprocessOrchestrator ì´ˆê¸°í™” ì™„ë£Œ")
    
    def process_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """
        PDF íŒŒì¼ ì „ì²´ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            pdf_path (str): PDF íŒŒì¼ ê²½ë¡œ
        
        Returns:
            Dict[str, Any]: {
                "status": "success" | "error",
                "doc_id": "ìƒì„±ëœ ë¬¸ì„œ ID",
                "pipeline_output": {
                    "raw_text": "ì›ë³¸ í…ìŠ¤íŠ¸",
                    "metadata": {...},
                    "definitions": {...},
                    "hierarchy": {...},
                    "tables": {...},
                    "chunks": [...],
                    "embeddings": [...],  # numpy ë°°ì—´ì€ listë¡œ ë³€í™˜
                    "search_index": {...},  # BM25 ì¸ë±ìŠ¤ ì •ë³´
                },
                "chroma_ready_data": [  # Chromaì— ë°”ë¡œ ì €ì¥ ê°€ëŠ¥
                    {
                        "id": "chunk_id",
                        "text": "ì²­í¬ í…ìŠ¤íŠ¸",
                        "metadata": {...},
                        "embedding": [0.1, 0.2, ...],
                    },
                    ...
                ]
            }
        """
        logger.info(f"ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
        
        try:
            # Step 1: PDF ë¡œë“œ
            pdf_result = self.pdf_processor.load_and_extract(pdf_path)
            if pdf_result["status"] != "success":
                raise RuntimeError(f"PDF ì¶”ì¶œ ì‹¤íŒ¨: {pdf_result.get('error')}")
            
            raw_text = pdf_result.get("full_text", "")
            doc_id = self._generate_doc_id(pdf_path)
            
            # Step 2: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = self.metadata_extractor.extract_metadata(raw_text, source_url=pdf_path)
            metadata["doc_id"] = doc_id
            
            # Step 3: í…Œì´ë¸” ê°ì§€
            logger.info("  ğŸ“Š í…Œì´ë¸” ê°ì§€ ì¤‘...")
            table_results = self.table_detector.detect_and_convert_tables(raw_text)
            
            # Step 4: ê³„ì¸µ êµ¬ì¡° ì¶”ì¶œ
            logger.info("  ğŸ—ï¸ ê³„ì¸µ êµ¬ì¡° ì¶”ì¶œ ì¤‘...")
            hierarchy_results = self.hierarchy_extractor.extract_hierarchy(raw_text)
            
            # Step 5: ì •ì˜ ì¶”ì¶œ
            logger.info("  ğŸ“– ì •ì˜ ì¶”ì¶œ ì¤‘...")
            definition_results = self.definition_extractor.extract_definitions_and_acronyms(raw_text)
            
            # Step 6: ì˜ë¯¸ ê¸°ë°˜ ì²­í¬ ë¶„í•  (êµ¬ì¡° ì •ë³´ í™œìš©)
            # êµ¬ì¡° ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
            enhanced_metadata = {
                **metadata,
                "tables": table_results,
                "hierarchy": hierarchy_results,
                "definitions": definition_results,
            }
            
            chunking_result = self.semantic_chunker.chunk_document(raw_text, enhanced_metadata)
            chunks = chunking_result["chunks"]
            
            # Step 7: ëª…ì œ ì¶”ì¶œ (ë³‘ë ¬ ì²˜ë¦¬) - ì£¼ì„ ì²˜ë¦¬
            # logger.info(f"  ğŸ“ ëª…ì œ ì¶”ì¶œ ì¤‘ ({len(chunks)}ê°œ ì²­í¬)...")
            # all_propositions = self.proposition_extractor.extract_propositions_batch(
            #     [{"content": c["text"]} for c in chunks]
            # )
            # ëª…ì œ ì¶”ì¶œ ìŠ¤í‚µ - ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ëŒ€ì²´
            logger.info(f"  âš ï¸ ëª…ì œ ì¶”ì¶œ ìŠ¤í‚µ (í…ŒìŠ¤íŠ¸ìš©)")
            all_propositions = [[] for _ in chunks]  # ë¹ˆ ëª…ì œ ë¦¬ìŠ¤íŠ¸
            
            # Step 8: Parent-Child ê´€ê³„ êµ¬ì„±
            for i, chunk in enumerate(chunks):
                chunk_id = f"{doc_id}_chunk_{i}"
                self.parent_chunks[chunk_id] = chunk["text"]
            
            # Step 9: ì„ë² ë”© ìƒì„± (ì›ë¬¸ ì²­í¬ ë‹¨ìœ„, Dense + Sparse)
            chunk_texts = [chunk["text"] for chunk in chunks]
            embeddings_result = self.embedding_pipeline.embed_texts(chunk_texts)
            embeddings_array = embeddings_result["dense"]
            sparse_embeddings = embeddings_result.get("sparse")  # Sparse ë²¡í„° ì¶”ì¶œ
            
            if sparse_embeddings:
                logger.info(f"  âœ… Sparse ë²¡í„° ìƒì„± ì™„ë£Œ: {len(sparse_embeddings)}ê°œ")
            else:
                logger.warning("  âš ï¸ Sparse ë²¡í„° ìƒì„± ì‹¤íŒ¨ (FlagEmbedding í™•ì¸ í•„ìš”)")
            
            # Step 10: VectorDB ì €ì¥ìš© ë°ì´í„° êµ¬ì„± (ì›ë¬¸ ì²­í¬ ë‹¨ìœ„)
            qdrant_ready_data = self._prepare_for_qdrant_with_chunks(
                chunks, all_propositions, embeddings_array, metadata, sparse_embeddings
            )
            
            # ìµœì¢… ê²°ê³¼
            result = {
                "status": "success",
                "doc_id": doc_id,
                "processed_at": datetime.utcnow().isoformat() + "Z",
                "pipeline_output": {
                    "raw_text": raw_text[:1000] + "..." if len(raw_text) > 1000 else raw_text,  # ìƒ˜í”Œ
                    "metadata": metadata,
                    "tables": table_results,
                    "hierarchy": hierarchy_results,
                    "definitions": definition_results,
                    "chunks": chunks[:3],
                    "propositions_sample": all_propositions[:3],
                    "embeddings_stats": {
                        "num_embeddings": len(embeddings_array),
                        "num_chunks": len(chunks),
                        "embedding_dim": len(embeddings_array[0]) if embeddings_array and len(embeddings_array) > 0 else 1024,
                    },
                },
                "qdrant_ready_data": qdrant_ready_data,
                "summary": {
                    "num_chunks": len(chunks),
                    "num_propositions": sum(len(props) for props in all_propositions),
                    "total_text_chars": len(raw_text),
                }
            }
            
            # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥ (demo í´ë”)
            try:
                output_dir = Path(__file__).parent / "demo"
                output_dir.mkdir(parents=True, exist_ok=True)
                
                txt_file = output_dir / f"{doc_id}.txt"
                with open(txt_file, "w", encoding="utf-8") as f:
                    f.write("=" * 80 + "\n")
                    f.write("ë¬¸ì„œ ë©”íƒ€ë°ì´í„°\n")
                    f.write("=" * 80 + "\n")
                    f.write(f"ì œëª©: {metadata['title']}\n")
                    f.write(f"êµ­ê°€: {metadata['country']}\n")
                    f.write(f"ê´€í• ê¶Œ: {metadata['jurisdiction']}\n")
                    f.write(f"ê·œì œê¸°ê´€: {metadata.get('regulatory_body', 'N/A')}\n")
                    f.write(f"ë²•ë¥  ìœ í˜•: {metadata.get('law_type', 'N/A')}\n")
                    f.write(f"ë°œí‘œì¼: {metadata.get('publication_date', 'N/A')}\n")
                    f.write(f"ì´ ì²­í¬: {len(chunks)}ê°œ\n")
                    f.write(f"ì´ ë¬¸ì: {len(raw_text):,}ê°œ\n")
                    f.write("\n" + "=" * 80 + "\n")
                    f.write("ì›ë¬¸ í…ìŠ¤íŠ¸\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(raw_text)
                    f.write("\n\n" + "=" * 80 + "\n")
                    f.write(f"ì²­í¬ ëª©ë¡ ({len(chunks)}ê°œ)\n")
                    f.write("=" * 80 + "\n")
                    for i, chunk in enumerate(chunks, 1):
                        f.write(f"\n--- ì²­í¬ {i}/{len(chunks)} ---\n")
                        f.write(f"ì„¹ì…˜: {chunk.get('section', 'N/A')}\n")
                        f.write(f"ê³„ì¸µ: {chunk.get('hierarchy_path', 'N/A')}\n")
                        f.write(f"í…Œì´ë¸” í¬í•¨: {chunk.get('has_table', False)}\n")
                        f.write(f"í† í° ì˜ˆìƒ: {chunk.get('tokens_estimate', 0)}\n")
                        f.write(f"\n{chunk['text']}\n")
                
                logger.info(f"  ğŸ’¾ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥: {txt_file.name}")
                result["txt_file"] = str(txt_file)
                
            except Exception as e:
                logger.warning(f"  âš ï¸ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            
            # Qdrantì— ì´ì¤‘ ì €ì¥ (Docker + ë¡œì»¬)
            try:
                from app.vectorstore.vector_client import VectorClient
                logger.info(f"  ğŸ’¾ Qdrant VectorDBì— ì´ì¤‘ ì €ì¥ ì¤‘ (Docker + ë¡œì»¬)...")
                
                # Docker VectorClient
                vc_docker = VectorClient(use_local=False)
                # ë¡œì»¬ VectorClient  
                vc_local = VectorClient(use_local=True)
                
                # ë°ì´í„° ì¶”ì¶œ
                texts = [d["text"] for d in qdrant_ready_data]
                embeddings = [d["embedding"] for d in qdrant_ready_data]
                metadatas = [d["metadata"] for d in qdrant_ready_data]
                
                # Sparse embedding ì¶”ì¶œ (ìˆìœ¼ë©´)
                sparse_embeddings = None
                if qdrant_ready_data and "sparse_embedding" in qdrant_ready_data[0]["metadata"]:
                    sparse_embeddings = [d["metadata"].pop("sparse_embedding") for d in qdrant_ready_data]
                
                # Dockerì— ì €ì¥
                vc_docker.insert(
                    texts=texts,
                    dense_embeddings=embeddings,
                    metadatas=metadatas,
                    sparse_embeddings=sparse_embeddings
                )
                
                # ë¡œì»¬ì—ë„ ì €ì¥
                logger.info(f"  ğŸ’¾ ë¡œì»¬ VectorDBì—ë„ ì €ì¥ ì¤‘...")
                vc_local.insert(
                    texts=texts,
                    dense_embeddings=embeddings,
                    metadatas=metadatas,
                    sparse_embeddings=sparse_embeddings
                )
                
                logger.info(f"  âœ… Qdrant ì´ì¤‘ ì €ì¥ ì™„ë£Œ: {len(texts)}ê°œ ì²­í¬ (Docker + ë¡œì»¬)")
                result["qdrant_status"] = "saved_dual"
                result["qdrant_count"] = len(texts)
                result["storage_locations"] = {
                    "docker": "http://localhost:6333",
                    "local": "/home/minje/remon/data/qdrant",
                    "txt_file": result.get("txt_file", "N/A")
                }
                
            except Exception as e:
                logger.error(f"  âŒ Qdrant ì´ì¤‘ ì €ì¥ ì‹¤íŒ¨: {e}")
                result["qdrant_status"] = "failed"
                result["qdrant_error"] = str(e)
            
            # ì¤‘ë³µ ë°©ì§€
            self.processed_hashes.add(metadata.get("document_hash"))
            
            logger.info(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„± ë° ì„ë² ë”©")
            return result
        
        except Exception as e:
            logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "status": "error",
                "error": str(e),
            }
    
    def _prepare_for_qdrant_with_chunks(
        self,
        chunks: List[Dict[str, Any]],
        all_propositions: List[List[str]],
        embeddings_array,
        doc_metadata: Dict[str, Any],
        sparse_embeddings=None,
    ) -> List[Dict[str, Any]]:
        """Qdrant VectorDBì— ì €ì¥í•  í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì¤€ë¹„ (ì›ë¬¸ ì²­í¬ ë‹¨ìœ„, Dense + Sparse)."""
        chroma_data = []
        legal_hierarchy = doc_metadata.get("legal_hierarchy", {})
        sparse_count = 0
        
        for chunk_idx, (chunk, propositions) in enumerate(zip(chunks, all_propositions)):
            chunk_id = f"{doc_metadata.get('doc_id', 'unknown')}_chunk_{chunk_idx}"
            
            # Dense embedding
            dense_emb = embeddings_array[chunk_idx].tolist() if hasattr(embeddings_array[chunk_idx], 'tolist') else embeddings_array[chunk_idx]
            
            # Sparse embedding (ìˆìœ¼ë©´)
            sparse_emb = None
            if sparse_embeddings and chunk_idx < len(sparse_embeddings):
                sparse_emb = sparse_embeddings[chunk_idx]
                sparse_count += 1
            
            chroma_doc = {
                "id": chunk_id,
                "text": chunk["text"],  # ì›ë¬¸ ì²­í¬ ì €ì¥
                "embedding": dense_emb,
                "metadata": {
                    # Sparse embedding (ìˆìœ¼ë©´ í¬í•¨)
                    "sparse_embedding": sparse_emb,
                    "meta_doc_id": doc_metadata.get("doc_id"),
                    "meta_chunk_index": chunk_idx,
                    "meta_propositions": propositions,  # ëª…ì œëŠ” ë©”íƒ€ë°ì´í„°ë¡œ ì €ì¥
                    "meta_num_propositions": len(propositions),
                    "meta_title": doc_metadata.get("title"),
                    "meta_country": doc_metadata.get("country"),
                    "meta_jurisdiction": doc_metadata.get("jurisdiction"),
                    "meta_agency": doc_metadata.get("regulatory_body"),
                    "meta_law_type": doc_metadata.get("law_type"),
                    "meta_regulation_type": doc_metadata.get("regulation_type"),
                    "meta_date": doc_metadata.get("publication_date"),
                    "meta_external_id": doc_metadata.get("external_id"),
                    "meta_section": chunk.get("section"),
                    "meta_section_title": chunk.get("section_title"),
                    "meta_has_table": chunk.get("has_table", False),
                    "meta_cfr_citation": legal_hierarchy.get("full_citation") if legal_hierarchy else None,
                    "meta_regulation_hierarchy": legal_hierarchy.get("regulation_type") if legal_hierarchy else None,
                },
            }
            
            # Sparse embedding ì œê±° (Noneì´ë©´)
            if chroma_doc["metadata"]["sparse_embedding"] is None:
                del chroma_doc["metadata"]["sparse_embedding"]
            chroma_data.append(chroma_doc)
        
        logger.debug(f"Qdrant ì €ì¥ìš© ë°ì´í„° ì¤€ë¹„: {len(chroma_data)}ê°œ ì²­í¬ (ì¤‘ Sparse: {sparse_count}ê°œ)")
        return chroma_data
    
    def _generate_doc_id(self, pdf_path: str) -> str:
        """PDF ê²½ë¡œë¡œë¶€í„° ë¬¸ì„œ IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        path = Path(pdf_path)
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        return f"doc_{path.stem}_{timestamp}"
    
    def batch_process_pdfs(self, pdf_paths: List[str]) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ PDFë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            pdf_paths (List[str]): PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            List[Dict[str, Any]]: ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        for idx, pdf_path in enumerate(pdf_paths, start=1):
            logger.info(f"ì²˜ë¦¬ ì¤‘: {idx}/{len(pdf_paths)} - {pdf_path}")
            result = self.process_pdf(pdf_path)
            results.append(result)
        
        logger.info(f"âœ… {len(results)}ê°œ PDF ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
        return results
    
    def process_regulatory_document_ddh(
        self,
        document_text: str,
        document_metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        DDH í‘œì¤€ ê·œì œ ë¬¸ì„œë¥¼ Parent-Child ì²­í‚¹ìœ¼ë¡œ ì²˜ë¦¬.
        
        Args:
            document_text: ê·œì œ ë¬¸ì„œ í…ìŠ¤íŠ¸
            document_metadata: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
            
        Returns:
            Dict: ì²˜ë¦¬ ê²°ê³¼ (Parent-Child ì²­í¬ + Qdrant ì¤€ë¹„ ë°ì´í„°)
        """
        logger.info("ğŸ¦ DDH ê·œì œ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘ (Parent-Child ì²­í‚¹)")
        
        try:
            if not document_metadata:
                document_metadata = {}
                
            doc_id = document_metadata.get("doc_id", self._generate_doc_id("ddh_document"))
            
            # Step 1: DDH êµ¬ì¡° íŒŒì‹±
            logger.info("  ğŸ¦ DDH êµ¬ì¡° íŒŒì‹± ì¤‘...")
            hierarchy_extractor_ddh = HierarchyExtractor(use_ddh=True)
            legal_nodes = hierarchy_extractor_ddh.parse_ddh_structure(document_text)
            preamble_metadata = hierarchy_extractor_ddh.get_preamble_metadata()
            
            # Step 2: ê¸°ìˆ  ìš©ì–´ ì •ê·œí™”
            logger.info("  ğŸ“ ê¸°ìˆ  ìš©ì–´ ì •ê·œí™” ì¤‘...")
            definition_extractor_enhanced = DefinitionExtractor(use_domain_dict=True)
            
            # Step 3: Parent-Child ì²­í‚¹
            logger.info("  âœ‚ï¸ Parent-Child ì²­í‚¹ ì¤‘...")
            global_metadata = {
                **document_metadata,
                **preamble_metadata,
                "doc_id": doc_id,
                "processing_type": "ddh_parent_child",
                "processed_at": datetime.utcnow().isoformat() + "Z"
            }
            
            processed_chunks = self.semantic_chunker.create_parent_child_chunks(
                legal_nodes, global_metadata
            )
            
            # Step 4: ìš©ì–´ ì •ê·œí™” ì ìš©
            for chunk in processed_chunks:
                chunk.text = definition_extractor_enhanced.normalize_tobacco_terms(chunk.text)
                chunk.context_text = definition_extractor_enhanced.normalize_tobacco_terms(chunk.context_text)
            
            # Step 5: ì„ë² ë”© ìƒì„± (Child + Parent ë³„ë„, Dense + Sparse)
            logger.info(f"  ğŸ§® ì„ë² ë”© ìƒì„± ì¤‘ ({len(processed_chunks)}ê°œ ì²­í¬)...")
            
            child_texts = [chunk.text for chunk in processed_chunks if chunk.chunk_type == "child"]
            parent_texts = [chunk.text for chunk in processed_chunks if chunk.chunk_type == "parent"]
            
            # Child ì„ë² ë”© (Dense + Sparse)
            child_embeddings_result = self.embedding_pipeline.embed_texts(child_texts) if child_texts else {"dense": [], "sparse": []}
            if child_texts and child_embeddings_result.get("sparse"):
                logger.info(f"  âœ… Child Sparse ë²¡í„°: {len(child_embeddings_result['sparse'])}ê°œ")
            
            # Parent ì„ë² ë”© (Dense + Sparse)
            parent_embeddings_result = self.embedding_pipeline.embed_texts(parent_texts) if parent_texts else {"dense": [], "sparse": []}
            if parent_texts and parent_embeddings_result.get("sparse"):
                logger.info(f"  âœ… Parent Sparse ë²¡í„°: {len(parent_embeddings_result['sparse'])}ê°œ")
            
            # ì„ë² ë”©ì„ ì²­í¬ì— í• ë‹¹ (Dense + Sparse)
            child_idx = 0
            parent_idx = 0
            
            for chunk in processed_chunks:
                if chunk.chunk_type == "child":
                    # Dense ì„ë² ë”©
                    chunk.embedding = child_embeddings_result["dense"][child_idx].tolist() if hasattr(child_embeddings_result["dense"][child_idx], 'tolist') else child_embeddings_result["dense"][child_idx]
                    # Sparse ì„ë² ë”© (ìˆìœ¼ë©´)
                    if child_embeddings_result.get("sparse"):
                        chunk.sparse_embedding = child_embeddings_result["sparse"][child_idx]
                    child_idx += 1
                elif chunk.chunk_type == "parent":
                    # Dense ì„ë² ë”©
                    chunk.parent_embedding = parent_embeddings_result["dense"][parent_idx].tolist() if hasattr(parent_embeddings_result["dense"][parent_idx], 'tolist') else parent_embeddings_result["dense"][parent_idx]
                    # Sparse ì„ë² ë”© (ìˆìœ¼ë©´)
                    if parent_embeddings_result.get("sparse"):
                        chunk.parent_sparse_embedding = parent_embeddings_result["sparse"][parent_idx]
                    parent_idx += 1
            
            # Step 6: Qdrant ì €ì¥ìš© ë°ì´í„° ì¤€ë¹„
            qdrant_ready_data = self._prepare_parent_child_for_qdrant(
                processed_chunks, global_metadata
            )
            
            # Step 7: Qdrantì— ì €ì¥
            try:
                from app.vectorstore.vector_client import VectorClient
                logger.info("  ğŸ’¾ Qdrantì— Parent-Child ì €ì¥ ì¤‘...")
                
                vc = VectorClient()
                
                # ë°ì´í„° ì¶”ì¶œ
                texts = [d["text"] for d in qdrant_ready_data]
                embeddings = [d["embedding"] for d in qdrant_ready_data]
                metadatas = [d["metadata"] for d in qdrant_ready_data]
                
                # Sparse embedding ì¶”ì¶œ (ìˆìœ¼ë©´)
                sparse_embeddings = None
                if qdrant_ready_data and "sparse_embedding" in qdrant_ready_data[0]["metadata"]:
                    sparse_embeddings = [d["metadata"].pop("sparse_embedding") for d in qdrant_ready_data]
                
                vc.insert(
                    texts=texts,
                    dense_embeddings=embeddings,
                    metadatas=metadatas,
                    sparse_embeddings=sparse_embeddings
                )
                
                logger.info(f"  âœ… Qdrant ì €ì¥ ì™„ë£Œ: {len(texts)}ê°œ ì²­í¬")
                
            except Exception as e:
                logger.error(f"  âŒ Qdrant ì €ì¥ ì‹¤íŒ¨: {e}")
            
            # ê²°ê³¼ ë°˜í™˜
            result = {
                "status": "success",
                "doc_id": doc_id,
                "processing_type": "ddh_parent_child",
                "processed_at": datetime.utcnow().isoformat() + "Z",
                "ddh_parsing": {
                    "legal_nodes": len(legal_nodes),
                    "preamble_metadata": preamble_metadata,
                    "sections_parsed": [node.get("identifier") for node in legal_nodes]
                },
                "parent_child_chunking": {
                    "total_chunks": len(processed_chunks),
                    "parent_chunks": len([c for c in processed_chunks if c.chunk_type == "parent"]),
                    "child_chunks": len([c for c in processed_chunks if c.chunk_type == "child"])
                },
                "embeddings": {
                    "child_embeddings": len(child_texts),
                    "parent_embeddings": len(parent_texts),
                    "embedding_dim": len(processed_chunks[0].embedding) if processed_chunks and processed_chunks[0].embedding else 1024
                },
                "qdrant_ready_data": qdrant_ready_data,
                "summary": {
                    "sections": len(legal_nodes),
                    "total_chunks": len(processed_chunks),
                    "normalized_terms": len(definition_extractor_enhanced.get_all_terms())
                }
            }
            
            logger.info(f"âœ… DDH ì²˜ë¦¬ ì™„ë£Œ: {len(legal_nodes)}ê°œ ì„¹ì…˜ â†’ {len(processed_chunks)}ê°œ Parent-Child ì²­í¬")
            return result
            
        except Exception as e:
            logger.error(f"âŒ DDH ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "status": "error",
                "error": str(e),
                "processing_type": "ddh_parent_child"
            }
    
    def _prepare_parent_child_for_qdrant(
        self,
        processed_chunks: List,  # ProcessedChunk ë¦¬ìŠ¤íŠ¸
        global_metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Parent-Child ì²­í¬ë¥¼ Qdrant ì €ì¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (Dense + Sparse).
        
        Args:
            processed_chunks: ProcessedChunk ë¦¬ìŠ¤íŠ¸
            global_metadata: ì „ì—­ ë©”íƒ€ë°ì´í„°
            
        Returns:
            List[Dict]: Qdrant ì €ì¥ìš© ë°ì´í„°
        """
        qdrant_data = []
        sparse_count = 0
        
        for chunk in processed_chunks:
            # Dense ì„ë² ë”© ì„ íƒ (ChildëŠ” ê²€ìƒ‰ìš©, ParentëŠ” ë§¥ë½ìš©)
            embedding = chunk.embedding if chunk.chunk_type == "child" else chunk.parent_embedding
            
            if not embedding:
                logger.warning(f"ì„ë² ë”© ëˆ„ë½: {chunk.chunk_id}")
                continue
            
            # Sparse ì„ë² ë”© ì„ íƒ
            sparse_embedding = chunk.sparse_embedding if chunk.chunk_type == "child" else chunk.parent_sparse_embedding
            if sparse_embedding:
                sparse_count += 1
                
            qdrant_doc = {
                "id": chunk.chunk_id,
                "text": chunk.text,
                "embedding": embedding,
                "metadata": {
                    # Sparse ì„ë² ë”© (ìˆìœ¼ë©´ ë©”íƒ€ë°ì´í„°ì— í¬í•¨)
                    "sparse_embedding": sparse_embedding,
                    
                    # Parent-Child ê´€ê³„
                    "type": chunk.chunk_type,
                    "parent_id": chunk.parent_id,
                    "context_text": chunk.context_text[:500] + "..." if len(chunk.context_text) > 500 else chunk.context_text,
                    
                    # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
                    "meta_doc_id": global_metadata.get("doc_id"),
                    "meta_section_id": chunk.metadata.get("section_id"),
                    "meta_hierarchy_level": chunk.metadata.get("hierarchy_level"),
                    "meta_level": chunk.metadata.get("level"),
                    "meta_is_table": chunk.metadata.get("is_table", False),
                    
                    # DDH ë©”íƒ€ë°ì´í„°
                    "meta_agency": global_metadata.get("AGENCY"),
                    "meta_action": global_metadata.get("ACTION"),
                    "meta_summary": global_metadata.get("SUMMARY", "")[:200],
                    "meta_country": global_metadata.get("country", "US"),
                    "meta_jurisdiction": global_metadata.get("jurisdiction", "federal"),
                    "meta_processing_type": global_metadata.get("processing_type"),
                    "meta_processed_at": global_metadata.get("processed_at")
                }
            }
            
            qdrant_data.append(qdrant_doc)
            
        logger.debug(f"Qdrant Parent-Child ë°ì´í„° ì¤€ë¹„: {len(qdrant_data)}ê°œ (ì¤‘ Sparse: {sparse_count}ê°œ)")
        return qdrant_data
