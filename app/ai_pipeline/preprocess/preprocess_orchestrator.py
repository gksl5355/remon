"""
module: preprocess_orchestrator.py
description: ì „ì²´ Preprocess íŒŒì´í”„ë¼ì¸ ì¡°ìœ¨ (ëª¨ë“  ëª¨ë“ˆ í†µí•©)
author: AI Agent
created: 2025-11-12
updated: 2025-11-12
dependencies:
    - app.config.logger, app.ai_pipeline.preprocess.config
    - app.ai_pipeline.preprocess.[metadata_extractor, definition_extractor, ...]
    - typing, json, datetime
"""

from typing import List, Dict, Optional, Any
import logging
import json
from datetime import datetime
from pathlib import Path

from app.ai_pipeline.preprocess.config import PreprocessConfig
from app.ai_pipeline.preprocess.metadata_extractor import MetadataExtractor
from app.ai_pipeline.preprocess.definition_extractor import DefinitionExtractor
from app.ai_pipeline.preprocess.bm25_indexer import BM25Indexer
from app.ai_pipeline.preprocess.pdf_processor import PDFProcessor
from app.ai_pipeline.preprocess.table_detector import TableDetector
from app.ai_pipeline.preprocess.embedding_pipeline import EmbeddingPipeline
from app.ai_pipeline.preprocess.semantic_chunker import SemanticChunker
from app.ai_pipeline.preprocess.hybrid_search import HybridSearch
from app.ai_pipeline.preprocess.hierarchy_extractor import HierarchyExtractor
from app.ai_pipeline.preprocess.proposition_extractor import PropositionExtractor

logger = logging.getLogger(__name__)


class PreprocessOrchestrator:
    """
    ì „ì²´ Preprocess íŒŒì´í”„ë¼ì¸ì„ ì¡°ìœ¨í•˜ëŠ” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°.
    
    íŒŒì´í”„ë¼ì¸:
    1. PDF ë¡œë“œ & í…ìŠ¤íŠ¸ ì¶”ì¶œ (PDFProcessor)
    2. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (MetadataExtractor)
    3. ì •ì˜ & ê³„ì¸µ êµ¬ì¡° ì¶”ì¶œ (DefinitionExtractor, HierarchyExtractor)
    4. í…Œì´ë¸” ê°ì§€ (TableDetector)
    5. ì˜ë¯¸ ê¸°ë°˜ ì²­í¬ ë¶„í•  (SemanticChunker)
    6. ì„ë² ë”© ìƒì„± (EmbeddingPipeline)
    7. BM25 ì¸ë±ì‹± (BM25Indexer)
    8. Chroma VectorDBì— ì €ì¥í•  í˜•ì‹ ì¤€ë¹„
    
    ì¶œë ¥: ChromaDB ì €ì¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ (ë³„ë„ index_managerë¡œ ì €ì¥)
    """
    
    def __init__(self, config: PreprocessConfig = None):
        """
        ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”.
        
        Args:
            config (PreprocessConfig): ì„¤ì • ê°ì²´. Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        """
        self.config = config or PreprocessConfig()
        
        # ëª¨ë“  ëª¨ë“ˆ ì´ˆê¸°í™”
        self.metadata_extractor = MetadataExtractor()
        self.definition_extractor = DefinitionExtractor()
        self.bm25_indexer = BM25Indexer()
        self.pdf_processor = PDFProcessor()
        self.table_detector = TableDetector()
        self.embedding_pipeline = EmbeddingPipeline(
            model_name=self.config.EMBEDDING_MODEL,
            use_fp16=self.config.USE_FP16,
            batch_size=self.config.EMBEDDING_BATCH_SIZE,
        )
        self.semantic_chunker = SemanticChunker(
            chunk_size=self.config.MAX_CHUNK_SIZE,
        )
        self.hybrid_search = HybridSearch(
            embedding_pipeline=self.embedding_pipeline,
            bm25_indexer=self.bm25_indexer,
            alpha=self.config.HYBRID_ALPHA,
            table_boost=self.config.TABLE_BOOST,
            category_boost=self.config.CATEGORY_BOOST,
        )
        self.hierarchy_extractor = HierarchyExtractor()
        self.proposition_extractor = PropositionExtractor(
            api_key=self.config.OPENAI_API_KEY,
            model=self.config.OPENAI_MODEL_PROPOSITION,
            max_workers=3
        )
        
        # Parent-Child Hierarchy ì €ì¥ì†Œ
        self.parent_chunks: Dict[str, str] = {}
        self.processed_hashes: set = set()
        
        logger.info("âœ… PreprocessOrchestrator ì´ˆê¸°í™” ì™„ë£Œ")
    
    def process_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """
        PDF íŒŒì¼ ì „ì²´ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            pdf_path (str): PDF íŒŒì¼ ê²½ë¡œ
        
        Returns:
            Dict[str, Any]: {
                "status": "success" | "error",
                "doc_id": "ìƒì„±ëœ ë¬¸ì„œ ID",
                "pipeline_output": {
                    "raw_text": "ì›ë³¸ í…ìŠ¤íŠ¸",
                    "metadata": {...},
                    "definitions": {...},
                    "hierarchy": {...},
                    "tables": {...},
                    "chunks": [...],
                    "embeddings": [...],  # numpy ë°°ì—´ì€ listë¡œ ë³€í™˜
                    "search_index": {...},  # BM25 ì¸ë±ìŠ¤ ì •ë³´
                },
                "chroma_ready_data": [  # Chromaì— ë°”ë¡œ ì €ì¥ ê°€ëŠ¥
                    {
                        "id": "chunk_id",
                        "text": "ì²­í¬ í…ìŠ¤íŠ¸",
                        "metadata": {...},
                        "embedding": [0.1, 0.2, ...],
                    },
                    ...
                ]
            }
        """
        logger.info(f"ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
        
        try:
            # Step 1: PDF ë¡œë“œ
            pdf_result = self.pdf_processor.load_and_extract(pdf_path)
            if pdf_result["status"] != "success":
                raise RuntimeError(f"PDF ì¶”ì¶œ ì‹¤íŒ¨: {pdf_result.get('error')}")
            
            raw_text = pdf_result.get("full_text", "")
            doc_id = self._generate_doc_id(pdf_path)
            
            # Step 2: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = self.metadata_extractor.extract_metadata(raw_text, source_url=pdf_path)
            metadata["doc_id"] = doc_id
            
            # Step 3: ì •ì˜ ì¶”ì¶œ
            definitions = self.definition_extractor.extract_definitions(raw_text)
            
            # Step 4: ê³„ì¸µ êµ¬ì¡° ì¶”ì¶œ
            hierarchy = self.hierarchy_extractor.extract_hierarchy(raw_text)
            
            # Step 5: í…Œì´ë¸” ê°ì§€ ë° ì»¨í…ìŠ¤íŠ¸ ë°”ì¸ë”©
            tables_result = self.table_detector.detect_tables_in_text(raw_text)
            if tables_result.get("tables"):
                enriched_tables = self.table_detector.bind_table_context(
                    raw_text, tables_result["tables"]
                )
                tables_result["tables"] = enriched_tables
            
            # Step 6: ì˜ë¯¸ ê¸°ë°˜ ì²­í¬ ë¶„í• 
            chunking_result = self.semantic_chunker.chunk_document(raw_text, metadata)
            chunks = chunking_result["chunks"]
            
            # Step 7: ëª…ì œ ì¶”ì¶œ (ë³‘ë ¬ ì²˜ë¦¬)
            logger.info(f"  ëª…ì œ ì¶”ì¶œ ì¤‘ ({len(chunks)}ê°œ ì²­í¬)...")
            all_propositions = self.proposition_extractor.extract_propositions_batch(
                [{"content": c["text"]} for c in chunks]
            )
            
            # Step 8: Parent-Child ê´€ê³„ êµ¬ì„±
            for i, chunk in enumerate(chunks):
                chunk_id = f"{doc_id}_chunk_{i}"
                self.parent_chunks[chunk_id] = chunk["text"]
            
            # Step 9: ì„ë² ë”© ìƒì„± (ëª…ì œ ë‹¨ìœ„)
            all_proposition_texts = []
            proposition_metadata = []
            
            for chunk_idx, (chunk, propositions) in enumerate(zip(chunks, all_propositions)):
                chunk_id = f"{doc_id}_chunk_{chunk_idx}"
                for prop_idx, proposition in enumerate(propositions):
                    all_proposition_texts.append(proposition)
                    proposition_metadata.append({
                        "chunk_id": f"{chunk_id}_prop_{prop_idx}",
                        "parent_chunk_id": chunk_id,
                        "chunk_index": chunk_idx,
                        "proposition_index": prop_idx,
                        "parent_content": chunk["text"][:500]
                    })
            
            embeddings_result = self.embedding_pipeline.embed_texts(all_proposition_texts)
            embeddings_array = embeddings_result["dense"]
            sparse_embeddings = embeddings_result.get("sparse")
            
            # Step 10: BM25 ì¸ë±ì‹±
            bm25_result = self.bm25_indexer.build_index(raw_text, metadata)
            
            # Step 11: Qdrant ì €ì¥ìš© ë°ì´í„° êµ¬ì„± (ëª…ì œ ë‹¨ìœ„)
            qdrant_ready_data = self._prepare_for_qdrant_with_propositions(
                chunks, all_propositions, embeddings_array, proposition_metadata, metadata, tables_result, sparse_embeddings
            )
            
            # ìµœì¢… ê²°ê³¼
            result = {
                "status": "success",
                "doc_id": doc_id,
                "processed_at": datetime.utcnow().isoformat() + "Z",
                "pipeline_output": {
                    "raw_text": raw_text[:1000] + "..." if len(raw_text) > 1000 else raw_text,  # ìƒ˜í”Œ
                    "metadata": metadata,
                    "definitions": definitions,
                    "hierarchy": hierarchy,
                    "tables": tables_result,
                    "chunks": chunks[:3],
                    "propositions_sample": all_propositions[:3],
                    "embeddings_stats": {
                        "num_embeddings": len(embeddings_array),
                        "num_propositions": len(all_proposition_texts),
                        "embedding_dim": len(embeddings_array[0]) if embeddings_array and len(embeddings_array) > 0 else 1024,
                    },
                    "search_index": bm25_result,
                },
                "qdrant_ready_data": qdrant_ready_data,
                "summary": {
                    "num_chunks": len(chunks),
                    "num_propositions": len(all_proposition_texts),
                    "num_definitions": len(definitions.get("definitions", [])),
                    "num_tables": tables_result.get("num_tables", 0),
                    "total_text_chars": len(raw_text),
                }
            }
            
            # Qdrantì— ì´ì¤‘ ì €ì¥ (Docker + ë¡œì»¬)
            try:
                from app.vectorstore.vector_client import VectorClient
                logger.info(f"  ğŸ’¾ Qdrant VectorDBì— ì´ì¤‘ ì €ì¥ ì¤‘ (Docker + ë¡œì»¬)...")
                
                # Docker VectorClient
                vc_docker = VectorClient(use_local=False)
                # ë¡œì»¬ VectorClient  
                vc_local = VectorClient(use_local=True)
                
                # ë°ì´í„° ì¶”ì¶œ
                texts = [d["text"] for d in qdrant_ready_data]
                embeddings = [d["embedding"] for d in qdrant_ready_data]
                metadatas = [d["metadata"] for d in qdrant_ready_data]
                
                # Sparse embedding ì¶”ì¶œ (ìˆìœ¼ë©´)
                sparse_embeddings = None
                if qdrant_ready_data and "sparse_embedding" in qdrant_ready_data[0]["metadata"]:
                    sparse_embeddings = [d["metadata"].pop("sparse_embedding") for d in qdrant_ready_data]
                
                # Dockerì— ì €ì¥
                vc_docker.insert(
                    texts=texts,
                    dense_embeddings=embeddings,
                    metadatas=metadatas,
                    sparse_embeddings=sparse_embeddings
                )
                
                # ë¡œì»¬ì—ë„ ì €ì¥
                logger.info(f"  ğŸ’¾ ë¡œì»¬ VectorDBì—ë„ ì €ì¥ ì¤‘...")
                vc_local.insert(
                    texts=texts,
                    dense_embeddings=embeddings,
                    metadatas=metadatas,
                    sparse_embeddings=sparse_embeddings
                )
                
                logger.info(f"  âœ… Qdrant ì´ì¤‘ ì €ì¥ ì™„ë£Œ: {len(texts)}ê°œ ëª…ì œ (Docker + ë¡œì»¬)")
                result["qdrant_status"] = "saved_dual"
                result["qdrant_count"] = len(texts)
                result["storage_locations"] = {
                    "docker": "http://localhost:6333",
                    "local": "/home/minje/remon/data/qdrant"
                }
                
            except Exception as e:
                logger.error(f"  âŒ Qdrant ì´ì¤‘ ì €ì¥ ì‹¤íŒ¨: {e}")
                result["qdrant_status"] = "failed"
                result["qdrant_error"] = str(e)
            
            # ì¤‘ë³µ ë°©ì§€
            self.processed_hashes.add(metadata.get("document_hash"))
            
            logger.info(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±, {len(all_proposition_texts)}ê°œ ëª…ì œ ì„ë² ë”©")
            return result
        
        except Exception as e:
            logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "status": "error",
                "error": str(e),
            }
    
    def _prepare_for_qdrant_with_propositions(
        self,
        chunks: List[Dict[str, Any]],
        all_propositions: List[List[str]],
        embeddings_array,
        proposition_metadata: List[Dict[str, Any]],
        doc_metadata: Dict[str, Any],
        tables_result: Dict[str, Any],
        sparse_embeddings=None,
    ) -> List[Dict[str, Any]]:
        """Qdrant VectorDBì— ì €ì¥í•  í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì¤€ë¹„ (ëª…ì œ ë‹¨ìœ„)."""
        chroma_data = []
        legal_hierarchy = doc_metadata.get("legal_hierarchy", {})
        
        embedding_idx = 0
        for chunk_idx, (chunk, propositions) in enumerate(zip(chunks, all_propositions)):
            for prop_idx, proposition in enumerate(propositions):
                prop_meta = proposition_metadata[embedding_idx]
                
                # Dense embedding
                dense_emb = embeddings_array[embedding_idx].tolist() if hasattr(embeddings_array[embedding_idx], 'tolist') else embeddings_array[embedding_idx]
                
                chroma_doc = {
                    "id": prop_meta["chunk_id"],
                    "text": proposition,
                    "embedding": dense_emb,
                    "metadata": {
                        # Sparse embedding (ì„ íƒì )
                        "sparse_embedding": sparse_embeddings[embedding_idx] if sparse_embeddings else None,
                        "meta_doc_id": doc_metadata.get("doc_id"),
                        "meta_parent_chunk_id": prop_meta["parent_chunk_id"],
                        "meta_parent_content": prop_meta["parent_content"],
                        "meta_chunk_index": chunk_idx,
                        "meta_proposition_index": prop_idx,
                        "meta_title": doc_metadata.get("title"),
                        "meta_country": doc_metadata.get("country"),
                        "meta_jurisdiction": doc_metadata.get("jurisdiction"),
                        "meta_agency": doc_metadata.get("regulatory_body"),
                        "meta_law_type": doc_metadata.get("law_type"),
                        "meta_regulation_type": doc_metadata.get("regulation_type"),
                        "meta_date": doc_metadata.get("publication_date"),
                        "meta_external_id": doc_metadata.get("external_id"),
                        "meta_section": chunk.get("section"),
                        "meta_section_title": chunk.get("section_title"),
                        "meta_has_table": chunk.get("has_table", False),
                        "meta_cfr_citation": legal_hierarchy.get("full_citation") if legal_hierarchy else None,
                        "meta_regulation_hierarchy": legal_hierarchy.get("regulation_type") if legal_hierarchy else None,
                    },
                }
                
                # Sparse embedding ì œê±° (Noneì´ë©´)
                if chroma_doc["metadata"]["sparse_embedding"] is None:
                    del chroma_doc["metadata"]["sparse_embedding"]
                chroma_data.append(chroma_doc)
                embedding_idx += 1
        
        logger.debug(f"Qdrant ì €ì¥ìš© ë°ì´í„° ì¤€ë¹„: {len(chroma_data)}ê°œ ëª…ì œ")
        return chroma_data
    
    def _generate_doc_id(self, pdf_path: str) -> str:
        """PDF ê²½ë¡œë¡œë¶€í„° ë¬¸ì„œ IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        path = Path(pdf_path)
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        return f"doc_{path.stem}_{timestamp}"
    
    def batch_process_pdfs(self, pdf_paths: List[str]) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ PDFë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            pdf_paths (List[str]): PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            List[Dict[str, Any]]: ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        for idx, pdf_path in enumerate(pdf_paths, start=1):
            logger.info(f"ì²˜ë¦¬ ì¤‘: {idx}/{len(pdf_paths)} - {pdf_path}")
            result = self.process_pdf(pdf_path)
            results.append(result)
        
        logger.info(f"âœ… {len(results)}ê°œ PDF ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
        return results
