"""
module: vision_orchestrator.py
description: Vision-Centric Preprocessing Pipeline ì „ì²´ ì¡°ìœ¨ (ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)
author: AI Agent
created: 2025-01-14
updated: 2025-01-14 (ë³‘ë ¬ ì²˜ë¦¬ ë¦¬íŒ©í„°ë§)
"""

import asyncio
import logging
from typing import Dict, Any, List, Optional
from pathlib import Path
from dataclasses import dataclass, field

from .config import PreprocessConfig
from .vision_ingestion import PDFRenderer, ComplexityAnalyzer, VisionRouter, StructureExtractor, DocumentAnalyzer
from .semantic_processing import HierarchyChunker, ContextInjector, DualIndexer
from .graph_builder import EntityExtractor, GraphManager

logger = logging.getLogger(__name__)


@dataclass
class TokenTracker:
    """í† í° ì‚¬ìš©ëŸ‰ ì¶”ì ."""
    total_tokens: int = 0
    tokens_by_model: Dict[str, int] = field(default_factory=dict)
    tokens_by_page: Dict[int, int] = field(default_factory=dict)
    
    def add_usage(self, page_num: int, model: str, tokens: int) -> None:
        """í† í° ì‚¬ìš©ëŸ‰ ì¶”ê°€."""
        self.total_tokens += tokens
        self.tokens_by_model[model] = self.tokens_by_model.get(model, 0) + tokens
        self.tokens_by_page[page_num] = tokens
    
    def check_budget(self, budget: Optional[int]) -> bool:
        """ì˜ˆì‚° ì´ˆê³¼ ì—¬ë¶€ í™•ì¸."""
        if budget is None:
            return True
        return self.total_tokens <= budget
    
    def get_summary(self) -> Dict[str, Any]:
        """ì‚¬ìš©ëŸ‰ ìš”ì•½ ë°˜í™˜."""
        return {
            "total_tokens": self.total_tokens,
            "tokens_by_model": self.tokens_by_model.copy(),
            "page_count": len(self.tokens_by_page),
        }


class VisionOrchestrator:
    """Vision ê¸°ë°˜ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¡°ìœ¨ì (ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)."""
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        model_complex: Optional[str] = None,
        model_simple: Optional[str] = None,
        dpi: Optional[int] = None,
        complexity_threshold: Optional[float] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        enable_graph: Optional[bool] = None,
        analysis_pages: Optional[int] = None,
        max_concurrency: Optional[int] = None,
        token_budget: Optional[int] = None,
        request_timeout: Optional[int] = None,
        retry_max_attempts: Optional[int] = None,
        retry_backoff_seconds: Optional[float] = None,
    ):
        """
        VisionOrchestrator ì´ˆê¸°í™”.
        
        Args:
            api_key: OpenAI API í‚¤ (í•„ìˆ˜)
            model_complex: ë³µì¡í•œ í‘œ ì²˜ë¦¬ìš© ëª¨ë¸ (ê¸°ë³¸ê°’: gpt-4o)
            model_simple: ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì²˜ë¦¬ìš© ëª¨ë¸ (ê¸°ë³¸ê°’: gpt-4o-mini)
            dpi: PDF ì´ë¯¸ì§€ ë Œë”ë§ DPI (ê¸°ë³¸ê°’: 300)
            complexity_threshold: í‘œ ë³µì¡ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.3)
            max_tokens: Vision LLM ìµœëŒ€ ì¶œë ¥ í† í° (ê¸°ë³¸ê°’: 4096)
            temperature: Vision LLM ì˜¨ë„ (ê¸°ë³¸ê°’: 0.1)
            enable_graph: ì§€ì‹ ê·¸ë˜í”„ ì¶”ì¶œ í™œì„±í™” (ê¸°ë³¸ê°’: True)
            analysis_pages: ë¬¸ì„œ ê·œì¹™ íŒŒì•…ìš© ì´ˆê¸° ë¶„ì„ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 3)
            max_concurrency: í˜ì´ì§€ë³„ Vision LLM í˜¸ì¶œ ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜ (ê¸°ë³¸ê°’: 3)
            token_budget: ë¬¸ì„œ ë‹¨ìœ„ í† í° ì˜ˆì‚° (ê¸°ë³¸ê°’: None, ì œí•œ ì—†ìŒ)
            request_timeout: Vision API ìš”ì²­ íƒ€ì„ì•„ì›ƒ ì´ˆ (ê¸°ë³¸ê°’: 120)
            retry_max_attempts: Vision API ì‹¤íŒ¨ ì‹œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 2)
            retry_backoff_seconds: ì¬ì‹œë„ ê°„ ëŒ€ê¸° ì‹œê°„ ì´ˆ (ê¸°ë³¸ê°’: 1.0)
        
        Note:
            ì¸ìê°€ Noneì´ë©´ PreprocessConfigì—ì„œ ê¸°ë³¸ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        """
        # ê¸°ë³¸ê°’ ë¡œë“œ (ì¸ìê°€ Noneì¸ ê²½ìš°)
        vision_config = PreprocessConfig.get_vision_config()
        
        # ì¸ìë¡œ ë„˜ì–´ì˜¨ ê°’ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ config ê¸°ë³¸ê°’ ì‚¬ìš©
        self.api_key = api_key or vision_config["api_key"]
        model_complex = model_complex or vision_config["model_complex"]
        model_simple = model_simple or vision_config["model_simple"]
        dpi = dpi or vision_config["dpi"]
        complexity_threshold = complexity_threshold or vision_config["complexity_threshold"]
        max_tokens = max_tokens or vision_config["max_tokens"]
        temperature = temperature or vision_config["temperature"]
        request_timeout = request_timeout or vision_config["request_timeout"]
        retry_max_attempts = retry_max_attempts or vision_config["retry_max_attempts"]
        retry_backoff_seconds = retry_backoff_seconds or vision_config["retry_backoff_seconds"]
        self.enable_graph = enable_graph if enable_graph is not None else vision_config["enable_graph"]
        self.analysis_pages = analysis_pages or vision_config["analysis_pages"]
        self.max_concurrency = max_concurrency or 30
        self.token_budget = token_budget if token_budget is not None else vision_config["token_budget"]
        
        if not self.api_key:
            raise ValueError("api_keyëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤. ìƒì„±ì ì¸ìë¡œ ì „ë‹¬í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        
        self.renderer = PDFRenderer(dpi=dpi)
        self.complexity_analyzer = ComplexityAnalyzer()
        self.document_analyzer = DocumentAnalyzer(
            api_key=self.api_key,
            model="gpt-4o-mini"
        )
        self.vision_router = VisionRouter(
            api_key=self.api_key,
            model_complex=model_complex,
            model_simple=model_simple,
            complexity_threshold=complexity_threshold,
            max_tokens=max_tokens,
            temperature=temperature,
            request_timeout=request_timeout,
            retry_max_attempts=retry_max_attempts,
            retry_backoff_seconds=retry_backoff_seconds,
        )
        self.structure_extractor = StructureExtractor()
        self.hierarchy_chunker = HierarchyChunker(max_tokens=1024)
        self.context_injector = ContextInjector()
        self.dual_indexer = DualIndexer()
        self.entity_extractor = EntityExtractor()
        self.graph_manager = GraphManager()
        
    async def process_pdf_async(self, pdf_path: str, use_parallel: bool = True, language_code: str = None) -> Dict[str, Any]:
        """
        PDF ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (async).
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            use_parallel: ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            language_code: ë¬¸ì„œ ì–¸ì–´ ì½”ë“œ (Noneì´ë©´ ìë™ ê°ì§€)
            
        Returns:
            Dict: {
                "status": "success",
                "language_code": "en",
                "vision_extraction_result": [...],
                "graph_data": {...},
                "dual_index_summary": {...}
            }
        """
        logger.info(f"=== Vision Pipeline ì‹œì‘: {Path(pdf_path).name} (ë³‘ë ¬: {use_parallel}) ===")
        
        try:
            # ì–¸ì–´ ê°ì§€ (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
            if language_code is None:
                from app.crawler.pdf_language_detector import detect_pdf_language
                
                logger.info("ë¬¸ì„œ ì–¸ì–´ ê°ì§€ ì¤‘...")
                lang_result = await asyncio.to_thread(detect_pdf_language, pdf_path)
                
                if lang_result['success']:
                    language_code = lang_result['language_code'].lower()
                    logger.info(
                        f"ê°ì§€ëœ ì–¸ì–´: {lang_result['language_name']} ({language_code}), "
                        f"ì‹ ë¢°ë„: {lang_result['confidence']:.2f}"
                    )
                else:
                    language_code = "en"  # fallback
                    logger.warning(f"ì–¸ì–´ ê°ì§€ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {language_code}")
            else:
                logger.info(f"ì§€ì •ëœ ì–¸ì–´: {language_code}")
            
            # StructureExtractorì— ì–¸ì–´ ì„¤ì •
            self.structure_extractor = StructureExtractor(language_code=language_code)
            # Phase 1: Vision Ingestion
            if use_parallel and self.max_concurrency > 1:
                # ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬
                vision_results = await self._phase1_vision_ingestion_parallel(pdf_path)
            else:
                # ìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
                vision_results = await asyncio.to_thread(self._phase1_vision_ingestion, pdf_path)
            
            if not vision_results:
                logger.warning("ì²˜ë¦¬ëœ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    "status": "error",
                    "error": "No pages processed"
                }
            
            # Phase 2: Semantic Processing
            processing_results = await asyncio.to_thread(self._phase2_semantic_processing, vision_results, pdf_path)
            
            # Phase 3: Graph Building (ì„ íƒì )
            if self.enable_graph:
                graph_data = await asyncio.to_thread(self._phase3_graph_building, vision_results)
            else:
                graph_data = {"nodes": [], "edges": []}
            
            # Phase 4: Dual Indexing (ì„ë² ë”© ë¶„ê¸° ì²˜ë¦¬)
            index_summary = {"qdrant_chunks": 0, "skipped": True}
            # ì„ë² ë”©ì€ change_detection ê²°ê³¼ì— ë”°ë¼ ë¶„ê¸°ë¨
            # ì—¬ê¸°ì„œëŠ” ìŠ¤í‚µí•˜ê³ , graph.pyì—ì„œ needs_embedding í”Œë˜ê·¸ í™•ì¸ í›„ ì‹¤í–‰
            
            result = {
                "status": "success",
                "language_code": language_code,
                "vision_extraction_result": vision_results,
                "graph_data": graph_data,
                "dual_index_summary": index_summary
            }
            
            logger.info(f"âœ… Vision Pipeline ì™„ë£Œ: {len(vision_results)}ê°œ í˜ì´ì§€ ì²˜ë¦¬")
            
            return result
            
        except Exception as e:
            logger.exception(f"âŒ Vision Pipeline ì‹¤íŒ¨: {e}")
            return {
                "status": "error",
                "error": str(e)
            }
    
    def process_pdf(self, pdf_path: str, use_parallel: bool = True, language_code: str = None) -> Dict[str, Any]:
        """
        PDF ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ë™ê¸° ë˜í¼).
        
        Note: ì´ ë©”ì„œë“œëŠ” ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œ í˜¸ì¶œ ì‹œ ì‚¬ìš©. async ì»¨í…ìŠ¤íŠ¸ì—ì„œëŠ” process_pdf_async() ì‚¬ìš©.
        """
        return asyncio.run(self.process_pdf_async(pdf_path, use_parallel, language_code))
    
    def _phase1_vision_ingestion(self, pdf_path: str) -> List[Dict[str, Any]]:
        """
        Phase 1: PDF â†’ ì´ë¯¸ì§€ â†’ Vision LLM â†’ êµ¬ì¡°í™” (ë™ê¸°, ë™ì  DPI).
        
        ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì›í•˜ë©´ _phase1_vision_ingestion_parallel() ì‚¬ìš©.
        """
        logger.info("Phase 1: Vision Ingestion ì‹œì‘ (ìˆœì°¨ ì²˜ë¦¬, ë™ì  DPI, ë©”íƒ€ë°ì´í„° ëˆ„ì )")
        
        # PDF í˜ì´ì§€ ìˆ˜ í™•ì¸
        import pypdfium2 as pdfium
        pdf = pdfium.PdfDocument(pdf_path)
        total_pages = len(pdf)
        pdf.close()
        
        vision_results = []
        token_tracker = TokenTracker()
        
        # ë¬¸ì„œ ê³µí†µ ë©”íƒ€ë°ì´í„° ëˆ„ì  (í˜ì´ì§€ë³„ë¡œ ê°±ì‹ )
        accumulated_metadata = {}
        
        for page_idx in range(total_pages):
            page_num = page_idx + 1
            
            try:
                # 1. ë³µì¡ë„ ë¶„ì„
                complexity = self.complexity_analyzer.analyze_page(pdf_path, page_num)
                
                # 2. DPI ê²°ì • (ë³µì¡ë„ ê¸°ë°˜)
                if complexity["has_table"] or complexity["complexity_score"] >= 0.3:
                    dpi = 300  # í‘œ ìˆìŒ â†’ ê³ í•´ìƒë„
                    logger.info(f"í˜ì´ì§€ {page_num}: ë³µì¡ë„ {complexity['complexity_score']:.2f} â†’ 300 DPI")
                else:
                    dpi = 150  # ë‹¨ìˆœ í…ìŠ¤íŠ¸ â†’ ì €í•´ìƒë„
                    logger.info(f"í˜ì´ì§€ {page_num}: ë³µì¡ë„ {complexity['complexity_score']:.2f} â†’ 150 DPI")
                
                # 3. ë™ì  ë Œë”ë§
                page_data = self.renderer.render_page_with_dpi(pdf_path, page_idx, dpi)
                
                # 4. Vision ì¶”ì¶œ (ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
                extraction = self.vision_router.route_and_extract(
                    image_base64=page_data["image_base64"],
                    page_num=page_num,
                    complexity_score=complexity["complexity_score"],
                    system_prompt=self.structure_extractor.get_system_prompt()
                )
                
                # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
                tokens_used = extraction.get("tokens_used", 0)
                token_tracker.add_usage(page_num, extraction["model_used"], tokens_used)
                
                # ì˜ˆì‚° í™•ì¸
                if not token_tracker.check_budget(self.token_budget):
                    logger.warning(
                        f"í† í° ì˜ˆì‚° ì´ˆê³¼: {token_tracker.total_tokens}/{self.token_budget}. "
                        f"í˜ì´ì§€ {page_num}ë¶€í„° ì²˜ë¦¬ ì¤‘ë‹¨."
                    )
                    break
                
                # 5. êµ¬ì¡°í™”
                structure = self.structure_extractor.extract(
                    extraction["content"],
                    page_num
                )
                
                # 6. ë©”íƒ€ë°ì´í„° ëˆ„ì  ê°±ì‹  (ê³µí†µ í•„ë“œë§Œ)
                if structure.metadata:
                    page_meta = structure.metadata.dict()
                    for key, value in page_meta.items():
                        # nullì´ ì•„ë‹Œ ê°’ì´ ë°œê²¬ë˜ë©´ ëˆ„ì 
                        if value is not None:
                            if key not in accumulated_metadata or accumulated_metadata[key] is None:
                                accumulated_metadata[key] = value
                                logger.debug(f"í˜ì´ì§€ {page_num}: {key} = {value} (ëˆ„ì )")
                
                # 7. ëˆ„ì ëœ ë©”íƒ€ë°ì´í„°ë¥¼ í˜„ì¬ í˜ì´ì§€ì— ì ìš©
                if accumulated_metadata:
                    from .vision_ingestion.structure_extractor import DocumentMetadata
                    structure.metadata = DocumentMetadata(**accumulated_metadata)
                
                vision_results.append({
                    "page_num": page_num,
                    "model_used": extraction["model_used"],
                    "complexity_score": complexity["complexity_score"],
                    "has_table": complexity["has_table"],
                    "dpi": dpi,  # DPI ê¸°ë¡
                    "structure": structure.dict(),
                    "tokens_used": tokens_used
                })
                
                logger.debug(f"í˜ì´ì§€ {page_num} ì™„ë£Œ: {dpi} DPI, {tokens_used} í† í°")
                
            except Exception as e:
                logger.error(f"í˜ì´ì§€ {page_num} ì‹¤íŒ¨: {e}", exc_info=True)
                continue
        
        logger.info(
            f"Phase 1 ì™„ë£Œ: {len(vision_results)}/{total_pages}ê°œ í˜ì´ì§€. "
            f"ì´ í† í°: {token_tracker.total_tokens}"
        )
        logger.info(f"ëˆ„ì ëœ ê³µí†µ ë©”íƒ€ë°ì´í„°: {len([k for k, v in accumulated_metadata.items() if v is not None])}ê°œ í•„ë“œ")
        return vision_results
    
    async def _phase1_vision_ingestion_parallel(self, pdf_path: str) -> List[Dict[str, Any]]:
        """
        Phase 1: PDF â†’ ì´ë¯¸ì§€ â†’ Vision LLM â†’ êµ¬ì¡°í™” (ë³‘ë ¬ ì²˜ë¦¬, ë™ì  DPI).
        
        í˜ì´ì§€ë³„ Vision LLM í˜¸ì¶œì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•.
        """
        logger.info(f"Phase 1: Vision Ingestion ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬, ë™ì  DPI, max_concurrency={self.max_concurrency})")
        
        # PDF í˜ì´ì§€ ìˆ˜ í™•ì¸
        import pypdfium2 as pdfium
        pdf = pdfium.PdfDocument(pdf_path)
        total_pages = len(pdf)
        pdf.close()
        
        # 1. ëª¨ë“  í˜ì´ì§€ ì •ë³´ ìˆ˜ì§‘ (ë³µì¡ë„, DPI, ëª¨ë¸ ê²°ì •)
        page_infos = self._prepare_page_infos(pdf_path, total_pages)
        
        # 2. ëª¨ë¸ë³„ ë°°ì¹˜ ìƒì„±
        batches = self._create_model_batches(page_infos)
        
        # 3. ë°°ì¹˜ë³„ ë³‘ë ¬ ì²˜ë¦¬
        token_tracker = TokenTracker()
        token_tracker_lock = asyncio.Lock()
        
        # LangSmith ë¶€í•˜ ë°©ì§€: ìµœëŒ€ 10ê°œ ë™ì‹œ ìš”ì²­
        effective_concurrency = min(self.max_concurrency, 10)
        semaphore = asyncio.Semaphore(effective_concurrency)
        logger.info(f"ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ ì œí•œ: {effective_concurrency}ê°œ ë™ì‹œ ìš”ì²­")
        
        async def process_batch(batch: Dict[str, Any]) -> List[Dict[str, Any]]:
            """ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬ (ë¹„ë™ê¸°)."""
            async with semaphore:
                try:
                    model_name = batch["model_name"]
                    pages = batch["pages"]
                    
                    logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {model_name}, {len(pages)}í˜ì´ì§€ (í˜ì´ì§€ {[p['page_index']+1 for p in pages]})")
                    
                    # ë°°ì¹˜ ë‹¨ìœ„ Vision í˜¸ì¶œ
                    batch_results = await asyncio.to_thread(
                        self.structure_extractor.extract_batch,
                        pages,
                        model_name
                    )
                    
                    # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
                    total_batch_tokens = sum(r.get("tokens_used", 0) for r in batch_results)
                    async with token_tracker_lock:
                        for result in batch_results:
                            page_num = result["page_num"]
                            tokens_used = result.get("tokens_used", 0)
                            token_tracker.add_usage(page_num, model_name, tokens_used)
                        
                        # ì˜ˆì‚° í™•ì¸
                        if not token_tracker.check_budget(self.token_budget):
                            logger.warning(
                                f"í† í° ì˜ˆì‚° ì´ˆê³¼: {token_tracker.total_tokens}/{self.token_budget}. "
                                f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ë‹¨."
                            )
                            return []
                    
                    logger.info(f"ë°°ì¹˜ ì™„ë£Œ: {model_name}, {len(batch_results)}í˜ì´ì§€, {total_batch_tokens}í† í°")
                    return batch_results
                    
                except Exception as e:
                    logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ ({batch['model_name']}): {e}", exc_info=True)
                    return []
        
        # ëª¨ë“  ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬
        logger.info(f"ì´ {len(batches)}ê°œ ë°°ì¹˜ ìƒì„±: {[(b['model_name'], len(b['pages'])) for b in batches]}")
        
        batch_tasks = [process_batch(batch) for batch in batches]
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
        
        # ê²°ê³¼ ë³‘í•© ë° ì •ë¦¬
        all_results = []
        for batch_result in batch_results:
            if isinstance(batch_result, Exception):
                logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {batch_result}")
            elif batch_result:
                all_results.extend(batch_result)
        
        # í˜ì´ì§€ ë²ˆí˜¸ ìˆœ ì •ë ¬
        all_results.sort(key=lambda x: x["page_num"])
        
        # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì™¸ë¶€ í˜¸í™˜ì„± ìœ ì§€)
        vision_results = []
        for result in all_results:
            page_info = next((p for p in page_infos if p["page_index"] == result["page_index"]), {})
            
            vision_results.append({
                "page_num": result["page_num"],
                "model_used": result["model_used"],
                "complexity_score": result["complexity_score"],
                "has_table": page_info.get("has_table", False),
                "dpi": page_info.get("dpi", 150),
                "structure": result["structure"],
                "tokens_used": result["tokens_used"]
            })
        
        logger.info(
            f"Phase 1 ì™„ë£Œ: {len(vision_results)}/{total_pages}ê°œ í˜ì´ì§€ ì²˜ë¦¬. "
            f"ì´ í† í°: {token_tracker.total_tokens} "
            f"({token_tracker.get_summary()['tokens_by_model']})"
        )
        
        return vision_results
    
    def _prepare_page_infos(self, pdf_path: str, total_pages: int) -> List[Dict[str, Any]]:
        """ëª¨ë“  í˜ì´ì§€ ì •ë³´ ìˆ˜ì§‘ (ë³µì¡ë„, DPI, ëª¨ë¸ ê²°ì •)."""
        page_infos = []
        
        for page_idx in range(total_pages):
            page_num = page_idx + 1
            
            try:
                # ë³µì¡ë„ ë¶„ì„
                complexity = self.complexity_analyzer.analyze_page(pdf_path, page_num)
                
                # DPI ê²°ì •
                if complexity["has_table"] or complexity["complexity_score"] >= 0.3:
                    dpi = 300
                else:
                    dpi = 150
                
                # ëª¨ë¸ ê²°ì • (ê¸°ì¡´ VisionRouter ë¡œì§ ì¬ì‚¬ìš©)
                if complexity["complexity_score"] >= self.vision_router.complexity_threshold:
                    model_name = self.vision_router.model_complex
                else:
                    model_name = self.vision_router.model_simple
                
                # í˜ì´ì§€ ë Œë”ë§
                page_data = self.renderer.render_page_with_dpi(pdf_path, page_idx, dpi)
                
                page_info = {
                    "page_index": page_idx,
                    "page_num": page_num,
                    "image_base64": page_data["image_base64"],
                    "complexity": complexity["complexity_score"],
                    "has_table": complexity["has_table"],
                    "dpi": dpi,
                    "model_name": model_name
                }
                
                page_infos.append(page_info)
                
                logger.info(f"í˜ì´ì§€ {page_num}: ë³µì¡ë„ {complexity['complexity_score']:.2f} â†’ {dpi} DPI")
                
            except Exception as e:
                logger.error(f"í˜ì´ì§€ {page_num} ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                # Fallback ì •ë³´
                page_infos.append({
                    "page_index": page_idx,
                    "page_num": page_num,
                    "image_base64": "",
                    "complexity": 0.1,
                    "has_table": False,
                    "dpi": 150,
                    "model_name": self.vision_router.model_simple
                })
        
        return page_infos
    
    def _create_model_batches(self, page_infos: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ëª¨ë¸ë³„ ë°°ì¹˜ ìƒì„±."""
        from .config import PreprocessConfig
        
        config = PreprocessConfig.get_vision_config()
        batch_size_simple = config["batch_size_simple"]
        batch_size_complex = config["batch_size_complex"]
        
        # ëª¨ë¸ë³„ ê·¸ë£¹í•‘
        model_groups = {}
        for page_info in page_infos:
            model_name = page_info["model_name"]
            if model_name not in model_groups:
                model_groups[model_name] = []
            model_groups[model_name].append(page_info)
        
        # ë°°ì¹˜ ìƒì„±
        batches = []
        for model_name, pages in model_groups.items():
            # ë°°ì¹˜ í¬ê¸° ê²°ì •
            if "4o-mini" in model_name or "mini" in model_name:
                batch_size = batch_size_simple
            else:
                batch_size = batch_size_complex
            
            # í˜ì´ì§€ë¥¼ ë°°ì¹˜ í¬ê¸°ë¡œ ë¶„í• 
            for i in range(0, len(pages), batch_size):
                batch_pages = pages[i:i + batch_size]
                batches.append({
                    "model_name": model_name,
                    "pages": batch_pages
                })
        
        logger.info(
            f"ë°°ì¹˜ ìƒì„± ì™„ë£Œ: {len(batches)}ê°œ ë°°ì¹˜. "
            f"ëª¨ë¸ë³„ í˜ì´ì§€ ìˆ˜: {[(model, len(pages)) for model, pages in model_groups.items()]}"
        )
        
        return batches
    
    def _phase2_semantic_processing(
        self,
        vision_results: List[Dict[str, Any]],
        pdf_path: str
    ) -> Dict[str, Any]:
        """Phase 2: ì²­í‚¹ + ì»¨í…ìŠ¤íŠ¸ ì£¼ì…."""
        logger.info("Phase 2: Semantic Processing ì‹œì‘")
        
        all_chunks = []
        
        for page_result in vision_results:
            structure = page_result["structure"]
            markdown_content = structure["markdown_content"]
            page_num = page_result["page_num"]
            
            # ì²­í‚¹
            chunks = self.hierarchy_chunker.chunk_document(markdown_content, page_num)
            all_chunks.extend(chunks)
        
        # ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
        enriched_chunks = self.context_injector.inject_context(all_chunks)
        
        logger.info(f"Phase 2 ì™„ë£Œ: {len(enriched_chunks)}ê°œ ì²­í¬")
        
        return {"chunks": enriched_chunks}
    
    def _phase3_graph_building(self, vision_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Phase 3: ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•."""
        logger.info("Phase 3: Graph Building ì‹œì‘")
        
        # PageStructure ê°ì²´ ì¬êµ¬ì„±
        from .vision_ingestion.structure_extractor import PageStructure
        
        page_structures = [
            PageStructure(**result["structure"])
            for result in vision_results
        ]
        
        # ì—”í‹°í‹° ì¶”ì¶œ
        graph_data = self.entity_extractor.extract_from_pages(page_structures)
        
        # ê·¸ë˜í”„ êµ¬ì¶•
        self.graph_manager.build_graph(graph_data)
        
        logger.info(f"Phase 3 ì™„ë£Œ: {len(graph_data['nodes'])}ê°œ ë…¸ë“œ")
        
        return graph_data
    
    def _phase4_dual_indexing(
        self,
        chunks: List[Dict[str, Any]],
        graph_data: Dict[str, Any],
        source_file: str,
        vision_results: List[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Phase 4: Qdrant + Graph ì €ì¥ (regulation_id ìë™ ìƒì„±)."""
        logger.info("Phase 4: Dual Indexing ì‹œì‘")
        
        import re
        from pathlib import Path
        
        # regulation_id ìƒì„± ì „ëµ
        regulation_id = None
        
        # ì „ëµ 1: ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ì¶œ
        if vision_results:
            first_page = vision_results[0]
            metadata = first_page.get("structure", {}).get("metadata", {})
            
            title = metadata.get("title", "")
            country = metadata.get("country", "")
            regulation_type = metadata.get("regulation_type", "")
            
            # title + country + type ì¡°í•©ìœ¼ë¡œ ID ìƒì„±
            if title:
                # ì˜ˆ: "FDA-US-Required-Warnings-Cigarette"
                parts = []
                if regulation_type:
                    parts.append(regulation_type)
                if country:
                    parts.append(country)
                # titleì—ì„œ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ (3ê°œ)
                title_words = re.findall(r'\b[A-Z][a-z]+\b', title)[:3]
                parts.extend(title_words)
                
                if parts:
                    regulation_id = "-".join(parts)
                    logger.info(f"regulation_id ìƒì„± (ë©”íƒ€ë°ì´í„°): {regulation_id}")
        
        # ì „ëµ 2: íŒŒì¼ëª… ê¸°ë°˜ (fallback)
        if not regulation_id:
            file_stem = Path(source_file).stem
            regulation_id = re.sub(r'[^A-Za-z0-9-]', '-', file_stem)
            logger.info(f"regulation_id ìƒì„± (íŒŒì¼ëª…): {regulation_id}")
        
        summary = self.dual_indexer.index(
            chunks=chunks, 
            graph_data=graph_data, 
            source_file=source_file,
            regulation_id=regulation_id,
            vision_results=vision_results
        )
        
        logger.info(f"Phase 4 ì™„ë£Œ: {summary['qdrant_chunks']}ê°œ ì²­í¬ ì €ì¥ (ref_blocks: {summary.get('reference_blocks_count', 0)})")
        
        return summary
