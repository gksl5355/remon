"""
module: embedding_pipeline.py
description: BGE-M3 ê¸°ë°˜ ì„ë² ë”© ìƒì„± ë° ê´€ë¦¬ (Chroma VectorDB ì—°ë™ ì˜ˆë¹„)
author: AI Agent
created: 2025-11-12
updated: 2025-11-12
dependencies:
    - app.config.logger, app.ai_pipeline.preprocess.config
    - sentence-transformers (BGE-M3)
    - numpy, typing
"""

from typing import List, Dict, Tuple, Optional, Any
import logging
import numpy as np
from pathlib import Path

logger = logging.getLogger(__name__)

# BGE-M3 ì„ë² ë”© ëª¨ë¸ (FlagEmbedding ìš°ì„ , sentence-transformers í´ë°±)
try:
    from FlagEmbedding import BGEM3FlagModel
    HAS_FLAG_EMBEDDING = True
except ImportError:
    HAS_FLAG_EMBEDDING = False

try:
    from sentence_transformers import SentenceTransformer
    HAS_SENTENCE_TRANSFORMERS = True
except ImportError:
    HAS_SENTENCE_TRANSFORMERS = False

if not HAS_FLAG_EMBEDDING and not HAS_SENTENCE_TRANSFORMERS:
    logger.warning("âš ï¸ FlagEmbedding ë˜ëŠ” sentence-transformers ì„¤ì¹˜ í•„ìš”")


class EmbeddingPipeline:
    """
    BGE-M3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤.
    
    ì—­í• :
    - BGE-M3 ëª¨ë¸ ë¡œë“œ (1024ì°¨ì› ë²¡í„°)
    - ë°°ì¹˜ ì„ë² ë”© (íš¨ìœ¨ì ì¸ ì²˜ë¦¬)
    - ì„ë² ë”© ì •ê·œí™” (cosine ìœ ì‚¬ë„ ê³„ì‚°ìš©)
    - ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ Chroma ì—°ë™ ì¤€ë¹„
    
    íŠ¹ì§•:
    - ë‹¤ì–¸ì–´ ì§€ì› (í•œê¸€, ì˜ë¬¸, ì¤‘ë¬¸, ì¼ë¬¸ ë“±)
    - FP16 ì •ë°€ë„ ì§€ì› (ë©”ëª¨ë¦¬ ì ˆì•½)
    - ìºì‹± (ì¤‘ë³µ ì„ë² ë”© ë°©ì§€)
    - ë°°ì¹˜ ì²˜ë¦¬ (ëŒ€ê·œëª¨ ë¬¸ì„œ íš¨ìœ¨ ì²˜ë¦¬)
    """
    
    def __init__(self, model_name: str = "BAAI/bge-m3", use_fp16: bool = True, batch_size: int = 32, use_sparse: bool = True):
        """
        ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”.
        
        Args:
            model_name (str): Hugging Face ëª¨ë¸ëª…. ê¸°ë³¸ê°’: "BAAI/bge-m3"
            use_fp16 (bool): FP16 ì •ë°€ë„ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½). ê¸°ë³¸ê°’: True
            batch_size (int): ë°°ì¹˜ í¬ê¸°. ê¸°ë³¸ê°’: 32
            use_sparse (bool): Sparse vector ì‚¬ìš© (FlagEmbedding í•„ìš”). ê¸°ë³¸ê°’: True
        """
        self.model_name = model_name
        self.use_fp16 = use_fp16
        self.batch_size = batch_size
        self.use_sparse = use_sparse and HAS_FLAG_EMBEDDING
        self.model = None
        self.embedding_cache: Dict[str, Dict] = {}  # ìºì‹œ (dense + sparse)
        
        self._load_model()
        
        logger.info(
            f"âœ… EmbeddingPipeline ì´ˆê¸°í™”: model={model_name}, "
            f"fp16={use_fp16}, batch_size={batch_size}, sparse={self.use_sparse}"
        )
    
    def _load_model(self) -> None:
        """BGE-M3 ëª¨ë¸ ë¡œë“œ (FlagEmbedding ìš°ì„ )."""
        if HAS_FLAG_EMBEDDING and self.use_sparse:
            logger.info(f"ğŸ”„ FlagEmbedding ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
            self.model = BGEM3FlagModel(self.model_name, use_fp16=self.use_fp16)
            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (FlagEmbedding): {self.model_name}")
        elif HAS_SENTENCE_TRANSFORMERS:
            logger.info(f"ğŸ”„ SentenceTransformer ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
            from sentence_transformers import SentenceTransformer
            self.model = SentenceTransformer(self.model_name)
            self.use_sparse = False
            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (SentenceTransformer): {self.model_name}")
        else:
            raise RuntimeError("ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜ (FlagEmbedding ë˜ëŠ” sentence-transformers í•„ìš”)")
    
    def embed_texts(self, texts: List[str], normalize: bool = True) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤ (Qdrant í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹­ ì§€ì›).
        
        Args:
            texts (List[str]): ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            normalize (bool): ì„ë² ë”© ì •ê·œí™” (cosine ìœ ì‚¬ë„ìš©). ê¸°ë³¸ê°’: True
        
        Returns:
            Dict[str, Any]: {
                "dense": List[List[float]] (N x 1024),
                "sparse": List[Dict[int, float]] (ì„ íƒì , use_sparse=True ì‹œ)
            }
        
        Raises:
            RuntimeError: ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°
            ValueError: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
        """
        if self.model is None:
            raise RuntimeError("ëª¨ë¸ ë¯¸ë¡œë“œ")
        
        if not texts:
            raise ValueError("í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        
        # ìºì‹œ íˆíŠ¸ í™•ì¸
        uncached_texts = []
        uncached_indices = []
        dense_embeddings = [None] * len(texts)
        sparse_embeddings = [None] * len(texts) if self.use_sparse else None
        
        for idx, text in enumerate(texts):
            if text in self.embedding_cache:
                cached = self.embedding_cache[text]
                dense_embeddings[idx] = cached["dense"]
                if self.use_sparse:
                    sparse_embeddings[idx] = cached.get("sparse")
            else:
                uncached_texts.append(text)
                uncached_indices.append(idx)
        
        # ìºì‹œë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ ì„ë² ë”©
        if uncached_texts:
            logger.debug(f"ì„ë² ë”© ìƒì„± ì¤‘: {len(uncached_texts)}ê°œ í…ìŠ¤íŠ¸")
            
            if HAS_FLAG_EMBEDDING and self.use_sparse:
                # FlagEmbedding: dense + sparse
                embeddings_dict = self.model.encode(
                    uncached_texts,
                    batch_size=self.batch_size,
                    max_length=8192,
                    return_dense=True,
                    return_sparse=True,
                    return_colbert_vecs=False
                )
                
                for idx, text in enumerate(uncached_texts):
                    dense = embeddings_dict['dense_vecs'][idx]
                    sparse = embeddings_dict['lexical_weights'][idx]  # Dict[int, float]
                    
                    self.embedding_cache[text] = {"dense": dense, "sparse": sparse}
                    dense_embeddings[uncached_indices[idx]] = dense
                    sparse_embeddings[uncached_indices[idx]] = sparse
            
            else:
                # SentenceTransformer: dense only
                for i in range(0, len(uncached_texts), self.batch_size):
                    batch = uncached_texts[i:i + self.batch_size]
                    batch_embeddings = self.model.encode(
                        batch,
                        convert_to_numpy=True,
                        normalize_embeddings=normalize,
                        show_progress_bar=False,
                    )
                    
                    for j, text in enumerate(batch):
                        dense = batch_embeddings[j]
                        self.embedding_cache[text] = {"dense": dense}
                        dense_embeddings[uncached_indices[i + j]] = dense
        
        # Qdrant í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (List[List[float]])
        result = {
            "dense": [emb.tolist() if isinstance(emb, np.ndarray) else emb for emb in dense_embeddings]
        }
        
        if self.use_sparse and sparse_embeddings:
            result["sparse"] = sparse_embeddings  # List[Dict[int, float]]
        
        logger.info(
            f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(texts)}ê°œ í…ìŠ¤íŠ¸, "
            f"ìºì‹œ íˆíŠ¸ìœ¨ {(len(texts) - len(uncached_texts)) / len(texts) * 100:.1f}%"
        )
        
        return result
    
    def embed_single_text(self, text: str, normalize: bool = True) -> Dict[str, Any]:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            text (str): ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            normalize (bool): ì •ê·œí™” ì—¬ë¶€. ê¸°ë³¸ê°’: True
        
        Returns:
            Dict[str, Any]: {"dense": np.ndarray, "sparse": Dict (ì„ íƒì )}
        """
        embeddings = self.embed_texts([text], normalize=normalize)
        result = {"dense": embeddings["dense"][0]}
        if "sparse" in embeddings:
            result["sparse"] = embeddings["sparse"][0]
        return result
    
    def batch_embed_documents(
        self, documents: List[Dict[str, Any]], text_field: str = "text"
    ) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ê³  ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            documents (List[Dict[str, Any]]): 
                [
                    {
                        "id": "doc_1",
                        "text": "ë¬¸ì„œ í…ìŠ¤íŠ¸",
                        "metadata": {...}
                    },
                    ...
                ]
            text_field (str): í…ìŠ¤íŠ¸ í•„ë“œëª…. ê¸°ë³¸ê°’: "text"
        
        Returns:
            List[Dict[str, Any]]: 
                [
                    {
                        "id": "doc_1",
                        "text": "ë¬¸ì„œ í…ìŠ¤íŠ¸",
                        "embedding": [0.1, 0.2, ...],  # 1024ì°¨ì› ë²¡í„°
                        "metadata": {...}
                    },
                    ...
                ]
        """
        texts = [doc.get(text_field, "") for doc in documents]
        embeddings = self.embed_texts(texts)
        
        results = []
        for doc, embedding in zip(documents, embeddings):
            results.append({
                "id": doc.get("id"),
                "text": doc.get(text_field, ""),
                "embedding": embedding.tolist(),  # numpy â†’ list
                "metadata": doc.get("metadata", {}),
            })
        
        logger.info(f"âœ… {len(results)}ê°œ ë¬¸ì„œ ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
        return results
    
    def similarity_search(
        self, query_text: str, candidates: List[str], top_k: int = 5
    ) -> List[Tuple[int, float, str]]:
        """
        ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì™€ í›„ë³´ í…ìŠ¤íŠ¸ë“¤ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            query_text (str): ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            candidates (List[str]): í›„ë³´ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            top_k (int): ìƒìœ„ Kê°œ ë°˜í™˜. ê¸°ë³¸ê°’: 5
        
        Returns:
            List[Tuple[int, float, str]]: [(ì¸ë±ìŠ¤, ì ìˆ˜, í…ìŠ¤íŠ¸), ...]
                ì ìˆ˜ëŠ” 0~1 ì‚¬ì´ì˜ cosine ìœ ì‚¬ë„
        """
        if not candidates:
            return []
        
        # ì„ë² ë”©
        query_embedding = self.embed_single_text(query_text, normalize=True)
        candidate_embeddings = self.embed_texts(candidates, normalize=True)
        
        # Cosine ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for idx, cand_emb in enumerate(candidate_embeddings):
            # ì •ê·œí™”ëœ ì„ë² ë”©: cosine_similarity = dot product
            score = float(np.dot(query_embedding, cand_emb))
            similarities.append((idx, score, candidates[idx]))
        
        # ìƒìœ„ Kê°œ ì •ë ¬
        similarities.sort(key=lambda x: x[1], reverse=True)
        results = similarities[:top_k]
        
        logger.debug(f"ìœ ì‚¬ë„ ê²€ìƒ‰ ì™„ë£Œ: ìƒìœ„ {len(results)}ê°œ ê²°ê³¼")
        return results
    
    def get_cache_statistics(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        cache_size = len(self.embedding_cache)
        cache_memory_mb = (cache_size * 1024 * 4) / (1024 * 1024)  # 1024ì°¨ì› float32
        
        return {
            "cache_size": cache_size,
            "cache_memory_mb": round(cache_memory_mb, 2),
            "model_name": self.model_name,
            "embedding_dimension": 1024,
        }
    
    def clear_cache(self) -> None:
        """ìºì‹œë¥¼ ë¹„ì›ë‹ˆë‹¤."""
        self.embedding_cache.clear()
        logger.info("âœ… ì„ë² ë”© ìºì‹œ ì´ˆê¸°í™”")
