"""
score_impact.py
"""

from __future__ import annotations

import os
import json
from datetime import datetime
from dotenv import load_dotenv
from pathlib import Path
import httpx

from openai import OpenAI
from typing import Any, Dict, List

from app.ai_pipeline.state import (
    AppState,
    MappingResults,
    StrategyResults,
    ImpactScoreItem,
)
from app.ai_pipeline.prompts.impact_prompt import IMPACT_PROMPT

import logging

logger = logging.getLogger(__name__)

# -----------------------------------------------------
# ENV & OpenAI Client
# -----------------------------------------------------
ROOT_DIR = Path(__file__).resolve().parents[3]
ENV_PATH = ROOT_DIR / ".env"
load_dotenv(dotenv_path=ENV_PATH, override=True)

client_openai = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    http_client=httpx.Client(trust_env=False)
)


# -----------------------------------------------------
# Utility: months_left ê³„ì‚°
# -----------------------------------------------------
def calculate_months_left(effective_date: str, analysis_date: str):
    if not effective_date:
        return None

    try:
        ed = datetime.strptime(effective_date, "%Y-%m-%d").date()
        ad = datetime.strptime(analysis_date, "%Y-%m-%d").date()
        days = (ed - ad).days
        months = round(days / 30, 2)
        return max(months, 0)
    except Exception:
        return None


# -----------------------------------------------------
# LangGraph Node
# -----------------------------------------------------

async def score_impact_node(state: AppState) -> AppState:

    regulation = state.get("regulation", {})
    mapping: MappingResults | None = state.get("mapping")
    strategies_list = state.get("strategies", [])

    # ë§¤í•‘/ì „ëµ ì—†ìœ¼ë©´ ìŠ¤í‚µ
    if not mapping or not strategies_list:
        logger.warning("[Impact] Skip: mapping or strategies missing")
        return state

    logger.info("[Impact] Starting impact scoring...")
    logger.debug("[Impact] Mapping items: %s", mapping.get("items"))
    logger.debug("[Impact] Strategy items: %s", strategies_list)

    # -----------------------------
    # INPUT ì „ì²˜ë¦¬
    # -----------------------------
    regulation_text = (
        regulation.get("text")
        or (mapping.get("items") or [{}])[0].get("regulation_summary")
        or ""
    )

    effective_date = regulation.get("effective_date")
    analysis_date = datetime.today().strftime("%Y-%m-%d")
    months_left = calculate_months_left(effective_date, analysis_date)

    # ì œí’ˆ ë§¤í•‘ JSON êµ¬ì„±
    products_json_list = []
    for item in mapping["items"]:
        products_json_list.append({
            "product_id": item.get("product_id"),
            "feature_name": item.get("feature_name"),
            "current_value": item.get("current_value"),
            "required_value": item.get("required_value"),
            "gap": item.get("gap"),
        })

    strategy_text = " ".join(strategies_list).strip()

    # -----------------------------
    # í”„ë¡¬í”„íŠ¸ ìƒì„± + ë¡œê·¸
    # -----------------------------

    # refined prompt ìš°ì„  ì ìš©
    if state.get("refined_score_impact_prompt"):
        prompt = state["refined_score_impact_prompt"]
        logger.info("[Impact] Using REFINED IMPACT PROMPT from validator")
        logger.debug(f"[Impact] Refined prompt content: {prompt[:200]}...")  # ë””ë²„ê¹…ìš©
    else:
        prompt = IMPACT_PROMPT.format(
            regulation_text=regulation_text,
            products_json=json.dumps(products_json_list, ensure_ascii=False, indent=2),
            strategy_text=strategy_text,
            months_left=months_left,
        )

    logger.debug("\n\n[Impact Prompt]\n%s\n", prompt)

    # -----------------------------
    # LLM í˜¸ì¶œ
    # -----------------------------
    try:
        response = client_openai.chat.completions.create(
            model=os.getenv("LLM_MODEL", "gpt-4o-mini"),
            messages=[
                {"role": "system", "content": "Respond ONLY with valid JSON."},
                {"role": "user", "content": prompt},
            ],
            temperature=0,
        )
        raw_llm_output = response.choices[0].message.content
        logger.debug("\n[Impact Raw LLM Output]\n%s\n", raw_llm_output)

        llm_out = json.loads(raw_llm_output)

    except Exception as e:
        logger.error("[Impact] LLM JSON parsing failed: %s", e)
        return state

    # -----------------------------
    # ì ìˆ˜ ë¶„ë¦¬
    # -----------------------------
    reasoning = llm_out.pop("reasoning", "")
    raw_scores = llm_out

    logger.debug("[Impact] Raw score dict: %s", raw_scores)

    # -----------------------------
    # ê°€ì¤‘í•© ê³„ì‚° ë° HITL ê°•ì œ ë ˆë²¨ ì ìš©
    # -----------------------------
    weights = {
        "directness": 0.20,
        "legal_severity": 0.25,
        "scope": 0.20,
        "regulatory_urgency": 0.10,
        "operational_urgency": 0.10,
        "response_cost": 0.20,
    }

    weighted_score = sum(raw_scores.get(k, 0) * w for k, w in weights.items())
    
    # ğŸ”¥ HITL ëª…ì‹œì  ë©”íƒ€ë°ì´í„° ìš°ì„  ì²˜ë¦¬
    hitl_desired_level = state.get("hitl_desired_impact_level")
    
    if hitl_desired_level:
        # ğŸ¯ HITL ê°•ì œ ë ˆë²¨ ì ìš©
        impact_level = hitl_desired_level
        
        # ì ìˆ˜ ë§¤í•‘
        level_score_map = {
            "Low": 2.0,
            "Medium": 3.0,
            "High": 4.5
        }
        weighted_score = level_score_map.get(impact_level, weighted_score)
        
        logger.info(f"[Impact] ğŸ¯ HITL ê°•ì œ ì ìš©: {impact_level} (score={weighted_score})")
        
        # ì‚¬ìš© í›„ ì œê±° (ë‹¤ìŒ ì‹¤í–‰ ì‹œ ê°„ì„­ ë°©ì§€)
        state["hitl_desired_impact_level"] = None
    else:
        # ê¸°ë³¸ ë¡œì§
        impact_level = (
            "High" if weighted_score >= 4 else
            "Medium" if weighted_score >= 2.5 else
            "Low"
        )
        logger.debug(f"[Impact] ê¸°ë³¸ ë¡œì§ ì ìš©: {impact_level} (score={weighted_score:.2f})")

    # -----------------------------
    # ê²°ê³¼ ìƒì„± (HITL ê·¼ê±° ì²˜ë¦¬)
    # -----------------------------
    # HITLì—ì„œ ê·¼ê±°ë¥¼ 'Human in the loop'ìœ¼ë¡œ ëŒ€ì²´
    if hitl_desired_level:
        reasoning = "Human in the loop"
        logger.info("[Impact] HITL override: reasoning set to 'Human in the loop'")
    
    impact_item: ImpactScoreItem = {
        "raw_scores": raw_scores,
        "weighted_score": round(weighted_score, 2),
        "impact_level": impact_level,
        "reasoning": reasoning,
    }

    # HITL ì¬ì‹¤í–‰ ì‹œ ê¸°ì¡´ ê²°ê³¼ êµì²´
    state["impact_scores"] = [impact_item]

    logger.info("[Impact] Final Impact Score: %s (Level: %s, Score: %.2f)", 
                impact_item, impact_level, weighted_score)
    
    # HITL ì ìš© ì—¬ë¶€ ë¡œê·¸
    if state.get("refined_score_impact_prompt"):
        logger.info("[Impact] âœ… HITL refined prompt applied successfully")

    # ğŸ†• ì¤‘ê°„ ê²°ê³¼ë¬¼ ì €ì¥ (HITLìš©)
    regulation_id = None
    regulation = state.get("regulation", {})
    if regulation:
        regulation_id = regulation.get("regulation_id")
    
    if not regulation_id:
        preprocess_results = state.get("preprocess_results", [])
        if preprocess_results:
            regulation_id = preprocess_results[0].get("regulation_id")
    
    if regulation_id and state.get("impact_scores"):
        from app.core.repositories.intermediate_output_repository import IntermediateOutputRepository
        from app.core.database import AsyncSessionLocal
        
        logger.info(f"ğŸ’¾ ì˜í–¥ë„ ì¤‘ê°„ ê²°ê³¼ë¬¼ ì €ì¥ ì‹œì‘: regulation_id={regulation_id}")
        
        async with AsyncSessionLocal() as session:
            intermediate_repo = IntermediateOutputRepository()
            try:
                intermediate_data = {
                    "impact_scores": state["impact_scores"],
                    "raw_scores": impact_item.get("raw_scores"),
                    "weighted_score": impact_item.get("weighted_score"),
                    "impact_level": impact_item.get("impact_level"),
                    "reasoning": impact_item.get("reasoning"),
                }
                await intermediate_repo.save_intermediate(
                    session,
                    regulation_id=regulation_id,
                    node_name="score_impact",
                    data=intermediate_data
                )
                await session.commit()
                logger.info(f"âœ… ì˜í–¥ë„ ì¤‘ê°„ ê²°ê³¼ë¬¼ ì €ì¥ ì™„ë£Œ: regulation_id={regulation_id}")
            except Exception as db_err:
                await session.rollback()
                logger.error(f"âŒ ì˜í–¥ë„ ì¤‘ê°„ ê²°ê³¼ë¬¼ ì €ì¥ ì‹¤íŒ¨: {db_err}")
    else:
        logger.warning(f"âš ï¸ ì˜í–¥ë„ ì¤‘ê°„ ê²°ê³¼ë¬¼ ì €ì¥ ìŠ¤í‚µ: regulation_id={regulation_id}")

    return state
