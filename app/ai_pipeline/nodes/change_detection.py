# app/ai_pipeline/nodes/change_detection.py
"""
module: change_detection.py
description: ê·œì œ ë³€ê²½ ê°ì§€ ë…¸ë“œ (Reference ID ê¸°ë°˜, ì „ì²˜ë¦¬ í›„ ì„ë² ë”© ì „)
author: AI Agent
created: 2025-01-18
updated: 2025-01-22 (í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬)
dependencies:
    - openai
    - app.vectorstore.vector_client
    - app.ai_pipeline.state
    - app.ai_pipeline.prompts.change_detection_prompt
"""

import json
import logging
from typing import Dict, Any, List, Optional, Literal, Set
from datetime import datetime
from openai import AsyncOpenAI
from sqlalchemy import text

from app.ai_pipeline.state import AppState
from app.vectorstore.vector_client import VectorClient
from app.ai_pipeline.prompts.change_detection_prompt import (
    CHANGE_DETECTION_SYSTEM_PROMPT,
    SECTION_MATCHING_PROMPT,
    NEW_REGULATION_ANALYSIS_PROMPT
)

logger = logging.getLogger(__name__)


# ==================== Confidence Scorer ====================
class ConfidenceScorer:
    """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°."""

    @staticmethod
    def adjust_confidence(result: Dict[str, Any]) -> float:
        base_confidence = result.get("confidence_score", 0.5)

        if "adversarial_check" in result:
            base_confidence = result["adversarial_check"].get(
                "adjusted_confidence", base_confidence
            )

        if result.get("numerical_changes"):
            base_confidence = min(base_confidence + 0.1, 1.0)

        return base_confidence

    @staticmethod
    def get_confidence_level(
        confidence: float,
    ) -> Literal["HIGH", "MEDIUM", "LOW", "UNCERTAIN"]:
        if confidence >= 0.9:
            return "HIGH"
        elif confidence >= 0.7:
            return "MEDIUM"
        elif confidence >= 0.5:
            return "LOW"
        else:
            return "UNCERTAIN"


# ==================== Change Detection Node ====================
class ChangeDetectionNode:
    """ë…ë¦½ ë³€ê²½ ê°ì§€ ë…¸ë“œ (Reference ID ê¸°ë°˜)."""

    def __init__(
        self,
        llm_client: Optional[AsyncOpenAI] = None,
        vector_client: Optional[VectorClient] = None,
        model_name: Optional[str] = None,
    ):
        from app.ai_pipeline.preprocess.config import PreprocessConfig

        if llm_client:
            self.llm = llm_client
        else:
            client = AsyncOpenAI()
            self.llm = PreprocessConfig.wrap_openai_client(client)

        self.vector_client = vector_client or VectorClient()
        self.model_name = model_name or PreprocessConfig.CHANGE_DETECTION_MODEL
        self.confidence_scorer = ConfidenceScorer()

    def _build_keynote_data(
        self,
        detection_results: List[Dict[str, Any]],
        change_summary: Dict[str, Any],
        regulation_meta: Dict[str, Any],
        legacy_regulation_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Change Detection ê²°ê³¼ë¥¼ Keynote JSONìœ¼ë¡œ ë³€í™˜"""
        from datetime import datetime

        return {
            "regulation_id": regulation_meta.get("regulation_id"),
            "country": regulation_meta.get("country"),
            "citation_code": regulation_meta.get("citation_code"),
            "title": regulation_meta.get("title"),
            "effective_date": regulation_meta.get("effective_date"),
            "analysis_date": datetime.utcnow().isoformat() + "Z",
            "change_summary": {
                "total_sections_analyzed": change_summary.get(
                    "total_reference_blocks", 0
                ),
                "total_changes_detected": change_summary.get("total_changes", 0),
                "high_confidence_changes": change_summary.get(
                    "high_confidence_changes", 0
                ),
            },
            "section_changes": [
                {
                    "section_ref": r.get("section_ref"),
                    "change_detected": r.get("change_detected"),
                    "confidence_level": r.get("confidence_level"),
                    "confidence_score": r.get("confidence_score"),
                    "change_type": r.get("change_type"),
                    "comparison": {
                        "legacy_snippet": r.get("legacy_snippet", "")[:200],
                        "new_snippet": r.get("new_snippet", "")[:200],
                    },
                    "reasoning": r.get("reasoning", {}),
                    "numerical_changes": r.get("numerical_changes", []),
                    "keywords": r.get("keywords", []),
                }
                for r in detection_results
                if r.get("change_detected")
            ],
            "legacy_regulation": (
                {"regulation_id": legacy_regulation_id}
                if legacy_regulation_id
                else None
            ),
        }

    async def run(self, state: AppState, db_session=None) -> AppState:
        """ë³€ê²½ ê°ì§€ ë…¸ë“œ ì‹¤í–‰ (ì§§ì€ DB ì„¸ì…˜ ì‚¬ìš©)."""
        logger.info("=== Change Detection Node ì‹œì‘ (Reference ID ê¸°ë°˜) ===")
        # ì‹ ê·œ ê·œì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (preprocess_results ìš°ì„ )
        pre_results = state.get("preprocess_results") or []
        if not pre_results:
            logger.info("preprocess_results ì—†ìŒ, ë³€ê²½ ê°ì§€ ìŠ¤í‚µ")
            state["change_detection_results"] = []
            state["change_summary"] = {
                "status": "skipped",
                "reason": "no_preprocess_results",
            }
            # ì‹¤í–‰ ìƒíƒœ ë§ˆí‚¹
            self._mark_execution_state(state)
            return state

        new_regul_data = pre_results[0]
        if new_regul_data.get("status") != "success":
            logger.error("âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨, ë³€ê²½ ê°ì§€ ìŠ¤í‚µ")
            state["change_detection_results"] = []
            state["change_summary"] = {"status": "error", "reason": "preprocess_failed"}
            return state

        new_regulation_id = new_regul_data.get("regulation_id", "NEW")
        legacy_regul_data = None
        legacy_regulation_id = None  # ì´ˆê¸°í™”

        # citation_code ê¸°ë°˜ìœ¼ë¡œ Legacy ê²€ìƒ‰ (ìƒˆ DB ì„¸ì…˜ ìƒì„±)
        if not legacy_regul_data:
            from app.core.repositories.regulation_repository import RegulationRepository
            from app.core.database import AsyncSessionLocal

            repo = RegulationRepository()
            # ìƒˆ ì„¸ì…˜ ìƒì„± (ì´ì „ ì„¸ì…˜ ì—°ê²° ëŠê¹€ ë°©ì§€)
            async with AsyncSessionLocal() as session:
                if not new_regul_data:
                    if not new_regulation_id:
                        logger.error("new_regulation_id ì—†ìŒ")
                        state["change_detection_results"] = []
                        state["change_summary"] = {
                            "status": "error",
                            "reason": "no_new_regulation_id",
                        }
                        self._mark_execution_state(state)
                        return state

                    new_regul_data = await repo.get_regul_data(
                        session, new_regulation_id
                    )
                    if not new_regul_data:
                        logger.warning(
                            f"ì‹ ê·œ regul_data ì—†ìŒ: regulation_id={new_regulation_id}"
                        )
                        state["change_detection_results"] = []
                        state["change_summary"] = {
                            "status": "error",
                            "reason": "no_new_regul_data",
                        }
                        self._mark_execution_state(state)
                        return state

                if not legacy_regul_data:
                    # citation_code ê¸°ë°˜ìœ¼ë¡œ Legacy ê²€ìƒ‰ (regulation_id ë¬´ì‹œ)
                    logger.info(f"ğŸ” new_regul_data í™•ì¸: {bool(new_regul_data)}")
                    if new_regul_data:
                        logger.info(
                            f"   new_regul_data keys: {list(new_regul_data.keys())}"
                        )

                    vision_pages = (
                        new_regul_data.get("vision_extraction_result", [])
                        if new_regul_data
                        else []
                    )
                    logger.info(f"   vision_pages ê°œìˆ˜: {len(vision_pages)}")

                    if vision_pages:
                        structure = vision_pages[0].get("structure", {})
                        new_metadata = structure.get("metadata") or {}
                        new_citation = new_metadata.get("citation_code")
                        new_country = new_metadata.get("jurisdiction_code")

                        if new_citation and new_country:
                            logger.info(
                                f"ğŸ” citation_codeë¡œ Legacy ê²€ìƒ‰: {new_citation} ({new_country})"
                            )

                            # citation_code + countryë¡œ Legacy ì§ì ‘ ì¡°íšŒ (ì›”-ì¼ ê¸°ì¤€)
                            try:
                                result = await session.execute(
                                    text(
                                        """
                                        SELECT regul_data FROM regulations
                                        WHERE citation_code = :citation
                                        AND country_code = :country
                                        AND TO_CHAR(created_at, 'MMDD') < TO_CHAR(CURRENT_TIMESTAMP, 'MMDD')
                                        ORDER BY created_at DESC LIMIT 1
                                    """
                                    ),
                                    {"citation": new_citation, "country": new_country},
                                )
                                row = result.fetchone()
                                if row:
                                    legacy_regul_data = row[0]
                                    logger.info(
                                        f"âœ… Legacy ë°œê²¬ (ì›”-ì¼ ê¸°ì¤€): citation={new_citation}"
                                    )
                            except Exception as db_err:
                                logger.error(f"âŒ DB ì¿¼ë¦¬ ì‹¤íŒ¨ (ì—°ê²° ëŠê¹€): {db_err}")
                                logger.info("âš ï¸ Legacy ê²€ìƒ‰ ì‹¤íŒ¨ - ì‹ ê·œ ê·œì œë¡œ ì²˜ë¦¬")

                    if not legacy_regul_data:
                        logger.warning("âš ï¸ Legacy ê²€ìƒ‰ ì‹¤íŒ¨ - ì‹ ê·œ ê·œì œë¡œ ì²˜ë¦¬")
                        logger.info(
                            "âœ… ì™„ì „íˆ ìƒˆë¡œìš´ ê·œì œ (Legacy ì—†ìŒ) - ì‹ ê·œ ë¶„ì„ ì‹¤í–‰"
                        )

                        # ì‹ ê·œ ê·œì œ ë¶„ì„ (LLM)
                        analysis_hints = await self._analyze_new_regulation(
                            new_regul_data
                        )
                        state["regulation_analysis_hints"] = analysis_hints
                        logger.info(
                            f"âœ… ì‹ ê·œ ê·œì œ ë¶„ì„ ì™„ë£Œ: {len(analysis_hints.get('key_requirements', []))}ê°œ ìš”êµ¬ì‚¬í•­"
                        )
                        logger.info(
                            f"   affected_areas: {analysis_hints.get('affected_areas', [])}"
                        )

                        # ğŸ†• ì‹ ê·œ ê·œì œ Keynote ë°ì´í„° ìƒì„±
                        keynote_data = {
                            "regulation_id": new_regulation_id,
                            "country": new_country,
                            "citation_code": new_citation,
                            "title": new_metadata.get("title", "Unknown Regulation"),
                            "effective_date": new_metadata.get("effective_date"),
                            "analysis_date": datetime.utcnow().isoformat() + "Z",
                            "change_summary": {
                                "total_sections_analyzed": 0,
                                "total_changes_detected": 0,
                                "high_confidence_changes": 0,
                            },
                            "section_changes": [],  # ì‹ ê·œ ê·œì œëŠ” ë³€ê²½ ì‚¬í•­ ì—†ìŒ
                            "new_regulation_analysis": analysis_hints,  # ì‹ ê·œ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                            "legacy_regulation": None,
                        }
                        state["change_keynote_data"] = keynote_data
                        logger.info("ğŸ“ ì‹ ê·œ ê·œì œ Keynote ë°ì´í„° ìƒì„± ì™„ë£Œ")
                        logger.info(f"   - regulation_id: {new_regulation_id}")
                        logger.info(f"   - country: {new_country}")
                        logger.info(f"   - citation_code: {new_citation}")
                        logger.info(f"   - key_requirements: {len(analysis_hints.get('key_requirements', []))}ê°œ")
                        
                        state["change_detection_results"] = []
                        state["change_summary"] = {
                            "status": "new_regulation",
                            "total_changes": 0,
                        }
                        state["needs_embedding"] = True
                        self._mark_execution_state(state)
                        return state
                # end session block

        # legacy_regulation_id ì—†ì§€ë§Œ legacy_regul_data ì£¼ì…ëœ ê²½ìš° ê¸°ë³¸ê°’ ì„¸íŒ…
        if legacy_regul_data and not legacy_regulation_id:
            legacy_regulation_id = (
                legacy_regul_data.get("regulation_id")
                or legacy_regul_data.get("regulation", {}).get("regulation_id")
                or "LEGACY"
            )
        # new_regulation_id ì—†ì„ ë•Œë„ ê¸°ë³¸ê°’ ì„¸íŒ…
        if not new_regulation_id:
            new_regulation_id = (
                new_regul_data.get("regulation_id")
                or new_regul_data.get("regulation", {}).get("regulation_id")
                or "INLINE_NEW"
            )

        # ========== ìµœì¢… ê²€ì¦: legacy_regul_data í™•ì¸ ==========
        if not legacy_regul_data:
            logger.error("âŒ legacy_regul_dataê°€ Noneì…ë‹ˆë‹¤")
            logger.error(
                "ğŸ’¡ í•´ê²°: python scripts/run_full_pipeline.py --mode legacy ì‹¤í–‰ í•„ìš”"
            )
            state["change_detection_results"] = []
            state["change_summary"] = {
                "status": "error",
                "reason": "legacy_data_is_none",
                "message": "Legacy ê·œì œë¥¼ ë¨¼ì € DBì— ì €ì¥í•˜ì„¸ìš” (--mode legacy)",
            }
            return state

        # ========== Reference Blocks ì¶”ì¶œ (ì„¸ì…˜ ë¶ˆí•„ìš”) ==========
        new_ref_blocks = self._extract_reference_blocks(new_regul_data)
        legacy_ref_blocks = self._extract_reference_blocks(legacy_regul_data)

        logger.info(
            f"Reference Blocks: ì‹ ê·œ {len(new_ref_blocks)}ê°œ, Legacy {len(legacy_ref_blocks)}ê°œ"
        )

        # ========== Section ë§¤ì¹­ (ì„¸ì…˜ ë¶ˆí•„ìš”) ==========
        matched_pairs = await self._match_reference_blocks(
            new_ref_blocks, legacy_ref_blocks
        )
        logger.info(f"Section ë§¤ì¹­ ì™„ë£Œ: {len(matched_pairs)}ê°œ ìŒ")

        # ========== LLM ë³€ê²½ ê°ì§€ (ë³‘ë ¬ ì²˜ë¦¬, 10ê°œ ë‹¨ìœ„) ==========
        import asyncio

        semaphore = asyncio.Semaphore(10)  # LangSmith ë¶€í•˜ ë°©ì§€

        async def detect_single_pair(pair):
            async with semaphore:
                return await self._detect_change_by_ref_id(
                    pair, new_regulation_id, legacy_regulation_id
                )

        logger.info(
            f"ğŸ”„ ë³€ê²½ ê°ì§€ ë³‘ë ¬ ì²˜ë¦¬: {len(matched_pairs)}ê°œ ì„¹ì…˜ (10ê°œ ë™ì‹œ ì œí•œ)"
        )

        detection_results_raw = await asyncio.gather(
            *[detect_single_pair(pair) for pair in matched_pairs],
            return_exceptions=True,
        )

        detection_results = []
        for result in detection_results_raw:
            if isinstance(result, Exception):
                logger.error(f"âŒ ë³€ê²½ ê°ì§€ ì‹¤íŒ¨: {result}")
                continue
            if result:
                detection_results.append(result)

        # ì‹ ë¢°ë„ ì¡°ì •
        for result in detection_results:
            result["confidence_score"] = self.confidence_scorer.adjust_confidence(
                result
            )
            result["confidence_level"] = self.confidence_scorer.get_confidence_level(
                result["confidence_score"]
            )

        total_changes = sum(1 for r in detection_results if r.get("change_detected"))
        high_confidence = sum(
            1 for r in detection_results if r.get("confidence_level") == "HIGH"
        )

        # ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“‹ ë³€ê²½ ê°ì§€ ìƒì„¸ ê²°ê³¼")
        logger.info("=" * 80)
        for idx, result in enumerate(detection_results, 1):
            section = result.get("section_ref", "Unknown")
            detected = result.get("change_detected", False)
            confidence = result.get("confidence_level", "UNKNOWN")
            change_type = result.get("change_type", "N/A")

            logger.info(f"\n[{idx}] Section: {section}")
            logger.info(f"  ë³€ê²½ ê°ì§€: {detected}")
            logger.info(
                f"  ì‹ ë¢°ë„: {confidence} ({result.get('confidence_score', 0):.2f})"
            )
            logger.info(f"  ë³€ê²½ ìœ í˜•: {change_type}")

            if detected:
                logger.info(f"  Legacy: {result.get('legacy_snippet', '')[:100]}...")
                logger.info(f"  New: {result.get('new_snippet', '')[:100]}...")

                numerical = result.get("numerical_changes", [])
                if numerical:
                    logger.info(f"  ìˆ˜ì¹˜ ë³€ê²½: {len(numerical)}ê°œ")
                    for num_change in numerical[:3]:
                        logger.info(
                            f"    - {num_change.get('field')}: {num_change.get('legacy_value')} â†’ {num_change.get('new_value')}"
                        )

        logger.info("\n" + "=" * 80)

        state["change_detection_results"] = detection_results
        state["change_summary"] = {
            "status": "completed",
            "total_reference_blocks": len(matched_pairs),
            "total_changes": total_changes,
            "high_confidence_changes": high_confidence,
            "legacy_regulation_id": legacy_regulation_id,
            "new_regulation_id": new_regulation_id,
        }
        # ğŸ”‘ Section ê¸°ë°˜ ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
        change_index = {}
        for result in detection_results:
            section = self._normalize_section_ref(result.get("section_ref", ""))
            if section and result.get("change_detected"):
                change_index[section] = result
        state["change_detection_index"] = change_index
        logger.info(f"ğŸ“š Change Index ìƒì„±: {len(change_index)}ê°œ ì„¹ì…˜")

        logger.info(
            f"âœ… ë³€ê²½ ê°ì§€ ì™„ë£Œ: {total_changes}ê°œ ë³€ê²½ ê°ì§€ (HIGH: {high_confidence})"
        )

        # ========== Keynote ë°ì´í„° ìƒì„± ==========
        regulation_meta = state.get("regulation", {})
        keynote_data = self._build_keynote_data(
            detection_results=detection_results,
            change_summary=state["change_summary"],
            regulation_meta=regulation_meta,
            legacy_regulation_id=legacy_regulation_id,
        )
        state["change_keynote_data"] = keynote_data
        logger.info("ğŸ“ Change Keynote ë°ì´í„° ìƒì„± ì™„ë£Œ")
        logger.info(f"   - ë°ì´í„° í¬ê¸°: {len(str(keynote_data))} bytes")
        logger.info(f"   - section_changes: {len(keynote_data.get('section_changes', []))}ê°œ")
        logger.info(f"   - regulation_id: {keynote_data.get('regulation_id')}")

        # ========== ì„ë² ë”© í•„ìš” ì—¬ë¶€ í”Œë˜ê·¸ ==========
        needs_embedding = total_changes > 0
        state["needs_embedding"] = needs_embedding
        logger.info(f"ğŸ“¦ ì„ë² ë”© í•„ìš”: {needs_embedding}")

        # ì‹¤í–‰ ìƒíƒœ ë§ˆí‚¹ (ì •ìƒ ì™„ë£Œ)
        self._mark_execution_state(state)
        
        # âœ… ìµœì¢… í™•ì¸: stateì— change_keynote_dataê°€ ì œëŒ€ë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if "change_keynote_data" not in state:
            logger.error("âŒ change_keynote_dataê°€ stateì— ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        else:
            logger.info(f"âœ… state['change_keynote_data'] í™•ì¸ ì™„ë£Œ: {len(str(state['change_keynote_data']))} bytes")
        
        return state

    def _extract_reference_blocks(
        self, regul_data: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Reference Block ì¶”ì¶œ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜)."""
        ref_blocks = []
        vision_pages = regul_data.get("vision_extraction_result", [])
        doc_id = regul_data.get("regulation_id") or regul_data.get("regulation", {}).get("regulation_id")

        for page in vision_pages:
            structure = page.get("structure", {})
            page_num = page.get("page_num", 0)
            markdown_content = structure.get("markdown_content", "")
            reference_blocks_meta = structure.get("reference_blocks", [])

            if reference_blocks_meta:
                lines = markdown_content.splitlines()
                for ref in reference_blocks_meta:
                    start = max(0, ref.get("start_line", 0))
                    end = ref.get("end_line", len(lines))
                    if end <= start:
                        end = min(len(lines), start + 20)
                    snippet = "\n".join(lines[start:end]) if lines else markdown_content
                    ref_blocks.append({
                        "section_ref": ref.get("section_ref", ""),
                        "text": snippet,
                        "keywords": ref.get("keywords") or self._extract_keywords(snippet),
                        "page_num": page_num,
                        "doc_id": doc_id,
                        "meta_doc_id": doc_id,
                    })
            else:
                ref_blocks.append({
                    "section_ref": f"Page {page_num}",
                    "text": markdown_content[:500],
                    "keywords": self._extract_keywords(markdown_content),
                    "page_num": page_num,
                    "doc_id": doc_id,
                    "meta_doc_id": doc_id,
                })

        logger.info(f"Reference Blocks ì¶”ì¶œ: {len(ref_blocks)}ê°œ")
        return ref_blocks

    def _extract_keywords(self, text: str, max_keywords: int = 5) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ í† í° ê¸°ë°˜)."""
        import re

        if not text:
            return []

        # ìˆ«ì í¬í•¨ ë‹¨ì–´ ìš°ì„  (ì˜ˆ: 20mg, Â§ 1141.1)
        numeric_words = re.findall(r"\b\w*\d+\w*\b", text)

        # ëŒ€ë¬¸ì ì‹œì‘ ë‹¨ì–´ (ê³ ìœ ëª…ì‚¬)
        capitalized = re.findall(r"\b[A-Z][a-z]+\b", text)

        # ê²°í•© ë° ì¤‘ë³µ ì œê±°
        keywords = list(dict.fromkeys(numeric_words[:3] + capitalized[:3]))

        return keywords[:max_keywords]

    async def _find_legacy_regulation_db(
        self, regul_data: Dict[str, Any], db_session, exclude_regulation_id: int = None
    ) -> Optional[int]:
        """DBì—ì„œ Legacy ê·œì œ ê²€ìƒ‰ (ê°•í™”ëœ ê²€ìƒ‰ ë¡œì§ + Citation Code ì •ê·œí™” + ê°™ì€ ë‚ ì§œ í•„í„°ë§)."""
        if not regul_data:
            logger.warning("regul_dataê°€ Noneì…ë‹ˆë‹¤")
            return None

        # vision_extraction_resultì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        vision_pages = regul_data.get("vision_extraction_result", [])
        if not vision_pages:
            logger.warning("vision_extraction_resultê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return None

        first_page = vision_pages[0]
        metadata = first_page.get("structure", {}).get("metadata", {})

        title = metadata.get("title", "")
        country = metadata.get("jurisdiction_code", "")
        citation_code = metadata.get("citation_code", "")
        version = metadata.get("version", "")
        effective_date = metadata.get("effective_date", "")

        # Citation Code ì •ê·œí™” (í•˜ì´í”ˆ ì œê±°, ëŒ€ë¬¸ì ë³€í™˜)
        def normalize_citation(code: str) -> str:
            if not code:
                return ""
            return code.upper().replace("-", "").replace(" ", "")

        normalized_citation = normalize_citation(citation_code)

        logger.info(f"DB Legacy ê²€ìƒ‰: title={title}, country={country}")
        # noisy print ì œê±°, loggerë¡œë§Œ ê¸°ë¡

        try:
            from app.core.repositories.regulation_repository import RegulationRepository

            repo = RegulationRepository()

            # ê°™ì€ ë‚ ì§œ í•„í„°ë§ì„ ìœ„í•œ ì‹ ê·œ ê·œì œ created_at ì¡°íšŒ
            exclude_date = None
            if exclude_regulation_id:
                result = await db_session.execute(
                    text(
                        "SELECT DATE(created_at) FROM regulations WHERE regulation_id = :rid"
                    ),
                    {"rid": exclude_regulation_id},
                )
                row = result.fetchone()
                if row:
                    exclude_date = row[0]

            # 1ìˆœìœ„: citation_code + country (ì •ê·œí™”ëœ ì½”ë“œë¡œ ê²€ìƒ‰)
            if normalized_citation and country:
                # ì›ë³¸ citation_codeë¡œ ê²€ìƒ‰ (ê°™ì€ ë‚ ì§œ ì œì™¸)
                if exclude_date:
                    result = await db_session.execute(
                        text(
                            """
                            SELECT regulation_id FROM regulations
                            WHERE citation_code = :citation
                            AND country_code = :country
                            AND DATE(created_at) < :exclude_date
                            AND (:exclude_id IS NULL OR regulation_id != :exclude_id)
                            ORDER BY created_at DESC LIMIT 1
                        """
                        ),
                        {
                            "citation": citation_code,
                            "country": country,
                            "exclude_date": exclude_date,
                            "exclude_id": exclude_regulation_id,
                        },
                    )
                    row = result.fetchone()
                    regulation = await repo.get(db_session, row[0]) if row else None
                else:
                    regulation = await repo.find_by_citation_and_country(
                        db_session, citation_code, country, exclude_regulation_id
                    )

                if regulation:
                    logger.info(
                        f"DB Legacy ë°œê²¬ (citation ì›ë³¸): regulation_id={regulation.regulation_id}"
                    )
                    return regulation.regulation_id

                # ì •ê·œí™”ëœ citation_codeë¡œ ì¬ê²€ìƒ‰ (fallback)
                regulation = await repo.find_by_citation_normalized(
                    db_session, normalized_citation, country, exclude_regulation_id
                )
                if regulation:
                    logger.info(
                        f"DB Legacy ë°œê²¬ (citation ì •ê·œí™”): regulation_id={regulation.regulation_id}"
                    )
                    return regulation.regulation_id

            # 2ìˆœìœ„: title + country + version
            if title and country and version:
                regulation = await repo.find_by_title_country_version(
                    db_session, title, country, version, exclude_regulation_id
                )
                if regulation:
                    logger.info(
                        f"DB Legacy ë°œê²¬ (title+version): regulation_id={regulation.regulation_id}"
                    )
                    return regulation.regulation_id

            # 3ìˆœìœ„: title + country (ê¸°ì¡´ ë¡œì§)
            if title and country:
                regulation = await repo.find_by_title_and_country(
                    db_session, title, country, exclude_regulation_id
                )
                if regulation:
                    logger.info(
                        f"DB Legacy ë°œê²¬ (title): regulation_id={regulation.regulation_id}"
                    )
                    return regulation.regulation_id
            regulation = await repo.find_by_title_and_country(
                db_session, title, country, exclude_regulation_id
            )

            if regulation:
                logger.info(f"DB Legacy ë°œê²¬: regulation_id={regulation.regulation_id}")
                return regulation.regulation_id

            logger.info("DB Legacy ë¯¸ë°œê²¬")
            return None

        except Exception as e:
            logger.error(f"DB Legacy ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def _normalize_section_ref(self, section_ref: str) -> str:
        """ì¡°í•­ ë²ˆí˜¸ ì •ê·œí™” (Â§1160.5, 1160.5, Â§ 1160.5 â†’ 1160.5)."""
        import re

        normalized = re.sub(r"[Â§\s]", "", section_ref)
        match = re.search(r"(\d+\.\d+)", normalized)
        return match.group(1) if match else normalized

    async def _match_reference_blocks(
        self, new_blocks: List[Dict[str, Any]], legacy_blocks: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Strict Section Matching: ì¡°í•­ ë²ˆí˜¸ ê¸°ë°˜ ì •í™•í•œ 1:1 ë§¤ì¹­ (í…ìŠ¤íŠ¸ ë³‘í•©).
        """
        logger.info("ğŸ” Strict Section Matching ì‹œì‘ (í…ìŠ¤íŠ¸ ë³‘í•©)")

        # ì¤‘ë³µ ì œê±° + í…ìŠ¤íŠ¸ ë³‘í•©: ê°™ì€ section_refì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ë³‘í•©
        def deduplicate_blocks(blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
            section_map = {}  # {section_ref: merged_block}

            for block in blocks:
                section = self._normalize_section_ref(block.get("section_ref", ""))
                if not section:
                    continue

                if section not in section_map:
                    # ì²« ë°œê²¬: ì´ˆê¸°í™”
                    section_map[section] = block.copy()
                    section_map[section]["end_page"] = block.get("page_num")
                    section_map[section]["page_range"] = [block.get("page_num")]
                else:
                    # ì¬ë°œê²¬: í…ìŠ¤íŠ¸ ë³‘í•©
                    existing_text = section_map[section].get("text", "")
                    new_text = block.get("text", "")
                    section_map[section]["text"] = existing_text + "\n\n" + new_text
                    section_map[section]["end_page"] = block.get("page_num")
                    section_map[section]["page_range"].append(block.get("page_num"))

            return list(section_map.values())

        new_blocks_unique = deduplicate_blocks(new_blocks)
        legacy_blocks_unique = deduplicate_blocks(legacy_blocks)

        logger.info(
            f"ğŸ§¹ ì¤‘ë³µ ì œê±°: New {len(new_blocks)} â†’ {len(new_blocks_unique)}, "
            f"Legacy {len(legacy_blocks)} â†’ {len(legacy_blocks_unique)}"
        )

        matched_pairs = []
        matched_legacy_sections = set()

        # ì •ê·œí™”ëœ ì¡°í•­ ë²ˆí˜¸ ê¸°ë°˜ 1:1 ë§¤ì¹­
        for new_block in new_blocks_unique:
            new_section = new_block.get("section_ref", "")
            new_normalized = self._normalize_section_ref(new_section)

            if not new_normalized:
                continue

            for legacy_block in legacy_blocks_unique:
                legacy_section = legacy_block.get("section_ref", "")
                legacy_normalized = self._normalize_section_ref(legacy_section)

                if legacy_normalized in matched_legacy_sections:
                    continue

                if new_normalized == legacy_normalized:
                    matched_pairs.append(
                        {
                            "new_block": new_block,
                            "legacy_block": legacy_block,
                            "match_confidence": 1.0,
                            "match_reason": f"Exact section: {new_normalized}",
                        }
                    )
                    matched_legacy_sections.add(legacy_normalized)
                    logger.debug(f"âœ… Matched: {new_section} â†” {legacy_section}")
                    break

        # ë§¤ì¹­ ì‹¤íŒ¨í•œ ì„¹ì…˜ ë¡œê·¸
        unmatched_new = [
            b.get("section_ref")
            for b in new_blocks_unique
            if not any(p["new_block"] == b for p in matched_pairs)
        ]
        if unmatched_new:
            logger.warning(f"âš ï¸ ë§¤ì¹­ ì‹¤íŒ¨í•œ ì‹ ê·œ ì„¹ì…˜: {unmatched_new[:5]}...")

        logger.info(
            f"âœ… ë§¤ì¹­ ì™„ë£Œ: {len(matched_pairs)}ê°œ ìŒ "
            f"(Exact: {sum(1 for p in matched_pairs if p['match_confidence'] == 1.0)})"
        )
        return matched_pairs

    async def _detect_change_by_ref_id(
        self, pair: Dict[str, Any], new_regulation_id: str, legacy_regulation_id: str
    ) -> Optional[Dict[str, Any]]:
        """CoT Step 2-4: Reference ID ê¸°ë°˜ ì •ë°€ ë³€ê²½ ê°ì§€ (Agentic)."""
        new_block = pair["new_block"]
        legacy_block = pair["legacy_block"]

        section_ref = new_block["section_ref"]
        new_text = new_block["text"]
        legacy_text = legacy_block["text"]
        new_doc_id = new_block.get("doc_id")
        legacy_doc_id = legacy_block.get("doc_id")

        # Reference ID ìƒì„±
        new_ref_id = (
            f"{new_regulation_id}-{section_ref}-P{new_block.get('page_num', 0)}"
        )
        legacy_ref_id = (
            f"{legacy_regulation_id}-{section_ref}-P{legacy_block.get('page_num', 0)}"
        )

        # LLM í˜¸ì¶œ (ref_id ê¸°ë°˜ ì •ë°€ ë¹„êµ)
        try:
            prompt = f"""Perform PRECISE comparison using Reference IDs for context-aware analysis.

**Reference IDs:**
- Legacy: {legacy_ref_id}
- New: {new_ref_id}

**Legacy Regulation (Section {section_ref}):**
{legacy_text}

**New Regulation (Section {section_ref}):**
{new_text}

**Task**: 
1. Use Reference IDs to understand document context and hierarchy
2. Detect ALL substantive changes (numerical, wording, scope)
3. Follow Chain of Thought (4 steps)
4. Apply Adversarial Validation
5. Extract numerical changes with full context
"""

            # GPT-5: Chat Completions API (SDK ë²„ì „ í˜¸í™˜ì„±)
            response = await self.llm.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": CHANGE_DETECTION_SYSTEM_PROMPT},
                    {"role": "user", "content": prompt},
                ],
                response_format={"type": "json_object"},
            )
            result = json.loads(response.choices[0].message.content)
            result["section_ref"] = section_ref
            result["new_ref_id"] = new_ref_id
            result["legacy_ref_id"] = legacy_ref_id
            result["doc_id"] = new_doc_id
            result["meta_doc_id"] = new_doc_id
            result.setdefault("new_snippet", new_text[:1000])
            result.setdefault("legacy_snippet", legacy_text[:1000])

            # í‚¤ì›Œë“œ/í•„ë“œ ë³´ê°• (ê²€ìƒ‰/ë§¤í•‘ íŒíŠ¸ìš©)
            kw: Set[str] = set(result.get("keywords") or [])
            kw |= set(self._extract_keywords(new_text, max_keywords=5))
            kw |= set(self._extract_keywords(legacy_text, max_keywords=5))
            for num_change in result.get("numerical_changes", []) or []:
                field = num_change.get("field")
                if field:
                    kw.add(str(field))
            if kw:
                result["keywords"] = list(kw)

            return result

        except Exception as e:
            logger.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨ (Section {section_ref}): {e}")
            return {
                "section_ref": section_ref,
                "new_ref_id": new_ref_id,
                "legacy_ref_id": legacy_ref_id,
                "change_detected": False,
                "confidence_score": 0.0,
                "error": str(e),
            }

    def _mark_execution_state(self, state: AppState) -> None:
        """ì‹¤í–‰ ìƒíƒœ ë§ˆí‚¹ (ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€)."""
        state["change_detection_ran_inline"] = True
    
    async def _analyze_new_regulation(
        self, regul_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì‹ ê·œ ê·œì œ ë¶„ì„ (Legacy ì—†ì„ ë•Œ LLMìœ¼ë¡œ í•µì‹¬ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ)."""
        vision_pages = regul_data.get("vision_extraction_result", [])
        if not vision_pages:
            return {
                "regulation_summary": "",
                "key_requirements": [],
                "affected_areas": [],
            }

        # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìµœëŒ€ 5000ì)
        full_text = ""
        for page in vision_pages[:10]:  # ìµœëŒ€ 10í˜ì´ì§€
            markdown = page.get("structure", {}).get("markdown_content", "")
            full_text += markdown + "\n\n"

        full_text = full_text[:5000]

        user_prompt = f"""**Regulation Text:**
{full_text}
"""

        try:
            # GPT-5: Chat Completions API (SDK ë²„ì „ í˜¸í™˜ì„±)
            response = await self.llm.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": NEW_REGULATION_ANALYSIS_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={"type": "json_object"},
            )
            result = json.loads(response.choices[0].message.content)
            return result

        except Exception as e:
            logger.error(f"ì‹ ê·œ ê·œì œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "regulation_summary": "",
                "key_requirements": [],
                "affected_areas": [],
            }


# ==================== ë…¸ë“œ í•¨ìˆ˜ ====================
_default_node: Optional[ChangeDetectionNode] = None


async def change_detection_node(
    state: AppState, config: Dict[str, Any] = None
) -> AppState:
    """LangGraph ë…¸ë“œ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ (ë‚´ë¶€ì—ì„œ ì§§ì€ ì„¸ì…˜ ìƒì„±)."""
    global _default_node
    if _default_node is None:
        _default_node = ChangeDetectionNode()

    # ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: ì´ë¯¸ ê²°ê³¼ê°€ ìˆê³  ê°•ì œ ì¬ì‹¤í–‰ì´ ì•„ë‹Œ ê²½ìš° skip
    if (
        state.get("change_detection_ran_inline")
        or state.get("change_detection_results")
    ) and not state.get("force_rerun_change_detection"):
        logger.info("change_detection ì´ë¯¸ ì‹¤í–‰ë¨. ì¬ì‹¤í–‰ ê±´ë„ˆëœ€.")
        return state

    return await _default_node.run(state, db_session=None)


__all__ = ["ChangeDetectionNode", "change_detection_node", "ConfidenceScorer"]
