"""
module: change_detection.py
description: ê·œì œ ë³€ê²½ ê°ì§€ ë…¸ë“œ (Reference ID ê¸°ë°˜, ì „ì²˜ë¦¬ í›„ ì„ë² ë”© ì „)
author: AI Agent
created: 2025-01-18
updated: 2025-01-18 (Reference ID ìµœì í™”)
dependencies:
    - openai
    - app.vectorstore.vector_client
    - app.ai_pipeline.state
"""

import json
import logging
from typing import Dict, Any, List, Optional, Literal
from openai import AsyncOpenAI

from app.ai_pipeline.state import AppState
from app.vectorstore.vector_client import VectorClient

logger = logging.getLogger(__name__)


# ==================== System Prompts ====================
CHANGE_DETECTION_SYSTEM_PROMPT = """You are a regulatory change detection expert with Reference ID-based context awareness.

**CRITICAL INSTRUCTIONS:**

1. **Complete Recall**: 
   - ì‚¬ì†Œí•´ ë³´ì´ëŠ” ìˆ˜ì¹˜ ë³€ê²½(ì˜ˆ: 18mg â†’ 20mg)ë„ ë°˜ë“œì‹œ ê°ì§€í•˜ì‹­ì‹œì˜¤.
   - ë‹¨ì–´ í•˜ë‚˜ì˜ ì°¨ì´(ì˜ˆ: 'ê¶Œê³ ' â†’ 'ì˜ë¬´', 'may' â†’ 'shall')ë„ ë†“ì¹˜ì§€ ë§ˆì‹­ì‹œì˜¤.

2. **Context Preservation with Reference IDs**:
   - Reference IDë¥¼ í™œìš©í•˜ì—¬ ë¬¸ì„œ ê³„ì¸µ êµ¬ì¡°ì™€ ë§¥ë½ì„ íŒŒì•…í•˜ì‹­ì‹œì˜¤.
   - ìˆ˜ì¹˜ë¥¼ ì¶”ì¶œí•  ë•ŒëŠ” ë°˜ë“œì‹œ ì ìš© ëŒ€ìƒê³¼ ì¡°ê±´ì„ í•¨ê»˜ ëª…ì‹œí•˜ì‹­ì‹œì˜¤.
   - Reference ID í˜•ì‹: {regulation_id}-{section_ref}-P{page_num}

3. **Chain of Thought (4 Steps)**:
   Step 1: Reference ID ê¸°ë°˜ ë§¥ë½ íŒŒì•… (ë¬¸ì„œ êµ¬ì¡°, ê³„ì¸µ)
   Step 2: í•µì‹¬ ìš©ì–´ ë¹„êµ (ìˆ˜ì¹˜, ì˜ë¬´ í‘œí˜„, ì¡°ê±´ì ˆ)
   Step 3: ì˜ë¯¸ ë³€í™” í‰ê°€ (ì‹¤ì§ˆì  ì˜í–¥ë„)
   Step 4: ìµœì¢… íŒë‹¨ (ë³€ê²½ ìœ í˜•, ì‹ ë¢°ë„)

4. **Adversarial Validation**:
   - ìì‹ ì˜ íŒë‹¨ì„ ë°˜ë°•í•˜ëŠ” ê·¼ê±°ë¥¼ ì°¾ìœ¼ì‹­ì‹œì˜¤.
   - ìµœì¢… íŒë‹¨ ì‹œ ë°˜ë°• ê·¼ê±°ë¥¼ ê³ ë ¤í•˜ì—¬ confidenceë¥¼ ì¡°ì •í•˜ì‹­ì‹œì˜¤.

**OUTPUT FORMAT (JSON):**
{
  "change_detected": true/false,
  "confidence_score": 0.0-1.0,
  "change_type": "value_change" | "scope_change" | "new_clause" | "removed" | "wording_only",
  "legacy_snippet": "ì›ë¬¸ ë°œì·Œ (ìµœëŒ€ 200ì)",
  "new_snippet": "ì›ë¬¸ ë°œì·Œ (ìµœëŒ€ 200ì)",
  "reasoning": {
    "step1_context_analysis": "Reference ID ê¸°ë°˜ ë§¥ë½ ë¶„ì„...",
    "step2_term_comparison": "í•µì‹¬ ìš©ì–´ ë¹„êµ...",
    "step3_semantic_evaluation": "ì˜ë¯¸ ë³€í™” í‰ê°€...",
    "step4_final_judgment": "ìµœì¢… íŒë‹¨..."
  },
  "adversarial_check": {
    "counter_argument": "...",
    "rebuttal": "...",
    "adjusted_confidence": 0.0-1.0
  },
  "keywords": ["keyword1", "keyword2"],
  "numerical_changes": [
    {
      "field": "í•„ë“œëª…",
      "legacy_value": "ì´ì „ ê°’",
      "new_value": "ìƒˆ ê°’",
      "context": "ì ìš© ë§¥ë½",
      "impact": "HIGH" | "MEDIUM" | "LOW"
    }
  ]
}
"""

SECTION_MATCHING_PROMPT = """Match new reference blocks with legacy reference blocks based on section numbers and keywords.

Return JSON array of matches:
{
  "matches": [
    {
      "new_section_ref": "1114.5(a)(3)",
      "legacy_section_ref": "1114.5(a)(3)",
      "match_confidence": 0.98
    }
  ]
}
"""


# ==================== Confidence Scorer ====================
class ConfidenceScorer:
    """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°."""

    @staticmethod
    def adjust_confidence(result: Dict[str, Any]) -> float:
        base_confidence = result.get("confidence_score", 0.5)

        if "adversarial_check" in result:
            base_confidence = result["adversarial_check"].get(
                "adjusted_confidence", base_confidence
            )

        if result.get("numerical_changes"):
            base_confidence = min(base_confidence + 0.1, 1.0)

        return base_confidence

    @staticmethod
    def get_confidence_level(
        confidence: float,
    ) -> Literal["HIGH", "MEDIUM", "LOW", "UNCERTAIN"]:
        if confidence >= 0.9:
            return "HIGH"
        elif confidence >= 0.7:
            return "MEDIUM"
        elif confidence >= 0.5:
            return "LOW"
        else:
            return "UNCERTAIN"


# ==================== Change Detection Node ====================
class ChangeDetectionNode:
    """ë…ë¦½ ë³€ê²½ ê°ì§€ ë…¸ë“œ (Reference ID ê¸°ë°˜)."""

    def __init__(
        self,
        llm_client: Optional[AsyncOpenAI] = None,
        vector_client: Optional[VectorClient] = None,
        model_name: str = "gpt-4o-mini",
    ):
        if llm_client:
            self.llm = llm_client
        else:
            from app.ai_pipeline.preprocess.config import PreprocessConfig

            client = AsyncOpenAI()
            self.llm = PreprocessConfig.wrap_openai_client(client)

        self.vector_client = vector_client or VectorClient()
        self.model_name = model_name
        self.confidence_scorer = ConfidenceScorer()

    async def run(self, state: AppState, db_session=None) -> AppState:
        """ë³€ê²½ ê°ì§€ ë…¸ë“œ ì‹¤í–‰ (DBì—ì„œ ì‹ ê·œ/ê¸°ì¡´ ê·œì œ ì¡°íšŒ)."""
        logger.info("=== Change Detection Node ì‹œì‘ (DB ê¸°ë°˜) ===")
        
        from app.core.repositories.regulation_repository import RegulationRepository
        from app.core.database import AsyncSessionLocal
        
        repo = RegulationRepository()
        
        # ========== DBì—ì„œ ì‹ ê·œ ê·œì œ ì¡°íšŒ ==========
        async with AsyncSessionLocal() as session:
            # 1. preprocess ê²°ê³¼ì—ì„œ ì‹ ê·œ regulation_id ì¶”ì¶œ
            preprocess_results = state.get("preprocess_results", [])
            if not preprocess_results:
                logger.warning("âš ï¸ preprocess_results ì—†ìŒ - ë³€ê²½ ê°ì§€ ìŠ¤í‚µ")
                state["change_detection_results"] = []
                state["change_summary"] = {"status": "skipped", "reason": "no_preprocess_results"}
                return state
            
            new_regulation_id = preprocess_results[0].get("regulation_id")
            if not new_regulation_id:
                logger.error("âŒ preprocess_resultsì— regulation_id ì—†ìŒ")
                state["change_detection_results"] = []
                state["change_summary"] = {"status": "error", "reason": "no_regulation_id"}
                return state
            
            logger.info(f"âœ… ì‹ ê·œ ê·œì œ ID: {new_regulation_id}")
            
            # 2. DBì—ì„œ ì‹ ê·œ ê·œì œ ë°ì´í„° ì¡°íšŒ
            new_regul_data = await repo.get_regul_data(session, new_regulation_id)
            if not new_regul_data:
                logger.warning(f"ì‹ ê·œ regul_data ì—†ìŒ: regulation_id={new_regulation_id}")
                state["change_detection_results"] = []
                state["change_summary"] = {"status": "error", "reason": "no_new_regul_data"}
                return state
            
            # 3. Legacy ê·œì œ ì¡°íšŒ (DBì—ì„œ ìë™ ê²€ìƒ‰)
            change_context = state.get("change_context", {})
            legacy_regulation_id = change_context.get("legacy_regulation_id")
            
            if not legacy_regulation_id:
                # DBì—ì„œ ë™ì¼ citation_codeì˜ ì´ì „ ë²„ì „ ì°¾ê¸°
                legacy_regulation_id = await self._find_legacy_regulation_db(
                    new_regul_data, session, new_regulation_id
                )
                if not legacy_regulation_id:
                    logger.info("âœ… ì™„ì „íˆ ìƒˆë¡œìš´ ê·œì œ (Legacy ì—†ìŒ)")
                    state["change_detection_results"] = []
                    state["change_summary"] = {"status": "new_regulation", "total_changes": 0}
                    return state
            
            logger.info(f"âœ… Legacy ê·œì œ ID: {legacy_regulation_id}")
            
            # 4. DBì—ì„œ Legacy ê·œì œ ë°ì´í„° ì¡°íšŒ
            legacy_regul_data = await repo.get_regul_data(session, legacy_regulation_id)
            if not legacy_regul_data:
                logger.warning(f"Legacy regul_data ì—†ìŒ: regulation_id={legacy_regulation_id}")
                state["change_detection_results"] = []
                state["change_summary"] = {"status": "error", "reason": "legacy_not_found"}
                return state

        # ========== Reference Blocks ì¶”ì¶œ (ì„¸ì…˜ ë¶ˆí•„ìš”) ==========
        new_ref_blocks = self._extract_reference_blocks(new_regul_data)
        legacy_ref_blocks = self._extract_reference_blocks(legacy_regul_data)
        
        logger.info(f"Reference Blocks: ì‹ ê·œ {len(new_ref_blocks)}ê°œ, Legacy {len(legacy_ref_blocks)}ê°œ")

        # ========== Section ë§¤ì¹­ (ì„¸ì…˜ ë¶ˆí•„ìš”) ==========
        matched_pairs = await self._match_reference_blocks(new_ref_blocks, legacy_ref_blocks)
        logger.info(f"Section ë§¤ì¹­ ì™„ë£Œ: {len(matched_pairs)}ê°œ ìŒ")

        # ========== LLM ë³€ê²½ ê°ì§€ (ì„¸ì…˜ ë¶ˆí•„ìš”) ==========
        detection_results = []
        for pair in matched_pairs:
            result = await self._detect_change_by_ref_id(pair, new_regulation_id, legacy_regulation_id)
            if result:
                detection_results.append(result)

        # ì‹ ë¢°ë„ ì¡°ì •
        for result in detection_results:
            result["confidence_score"] = self.confidence_scorer.adjust_confidence(result)
            result["confidence_level"] = self.confidence_scorer.get_confidence_level(result["confidence_score"])

        total_changes = sum(1 for r in detection_results if r.get("change_detected"))
        high_confidence = sum(1 for r in detection_results if r.get("confidence_level") == "HIGH")

        # ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“‹ ë³€ê²½ ê°ì§€ ìƒì„¸ ê²°ê³¼")
        logger.info("=" * 80)
        for idx, result in enumerate(detection_results, 1):
            section = result.get("section_ref", "Unknown")
            detected = result.get("change_detected", False)
            confidence = result.get("confidence_level", "UNKNOWN")
            change_type = result.get("change_type", "N/A")
            
            logger.info(f"\n[{idx}] Section: {section}")
            logger.info(f"  ë³€ê²½ ê°ì§€: {detected}")
            logger.info(f"  ì‹ ë¢°ë„: {confidence} ({result.get('confidence_score', 0):.2f})")
            logger.info(f"  ë³€ê²½ ìœ í˜•: {change_type}")
            
            if detected:
                logger.info(f"  Legacy: {result.get('legacy_snippet', '')[:100]}...")
                logger.info(f"  New: {result.get('new_snippet', '')[:100]}...")
                
                numerical = result.get("numerical_changes", [])
                if numerical:
                    logger.info(f"  ìˆ˜ì¹˜ ë³€ê²½: {len(numerical)}ê°œ")
                    for num_change in numerical[:3]:
                        logger.info(f"    - {num_change.get('field')}: {num_change.get('legacy_value')} â†’ {num_change.get('new_value')}")
        
        logger.info("\n" + "=" * 80)

        state["change_detection_results"] = detection_results
        state["change_summary"] = {
            "status": "completed",
            "total_reference_blocks": len(matched_pairs),
            "total_changes": total_changes,
            "high_confidence_changes": high_confidence,
            "legacy_regulation_id": legacy_regulation_id,
            "new_regulation_id": new_regulation_id,
        }

        logger.info(f"âœ… ë³€ê²½ ê°ì§€ ì™„ë£Œ: {total_changes}ê°œ ë³€ê²½ ê°ì§€ (HIGH: {high_confidence})")
        return state

    def _extract_reference_blocks(
        self, regul_data: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """regul_dataì—ì„œ reference_blocks ì¶”ì¶œ (Vision Pipeline êµ¬ì¡° ëŒ€ì‘)."""
        ref_blocks = []

        # Vision Pipeline ì¶œë ¥ êµ¬ì¡°
        vision_pages = regul_data.get("vision_extraction_result", [])

        for page in vision_pages:
            structure = page.get("structure", {})
            page_num = page.get("page_num", 0)
            markdown_content = structure.get("markdown_content", "")
            reference_blocks = structure.get("reference_blocks", [])

            # reference_blocksê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if reference_blocks:
                for ref in reference_blocks:
                    ref_blocks.append(
                        {
                            "section_ref": ref.get("section_ref", ""),
                            "text": "",  # í…ìŠ¤íŠ¸ëŠ” markdown_contentì—ì„œ ì¶”ì¶œ
                            "keywords": ref.get("keywords", []),
                            "page_num": page_num,
                            "start_line": ref.get("start_line", 0),
                            "end_line": ref.get("end_line", 0),
                            "hierarchy": [],  # ê³„ì¸µ ì •ë³´ (í•„ìš”ì‹œ ì¶”ê°€)
                        }
                    )
            else:
                # reference_blocksê°€ ì—†ìœ¼ë©´ í˜ì´ì§€ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë¸”ë¡ìœ¼ë¡œ
                ref_blocks.append(
                    {
                        "section_ref": f"Page {page_num}",
                        "text": markdown_content[:500],  # ì²˜ìŒ 500ì
                        "keywords": self._extract_keywords(markdown_content),
                        "page_num": page_num,
                        "start_line": 0,
                        "end_line": len(markdown_content.splitlines()),
                        "hierarchy": [],
                    }
                )

        return ref_blocks

    def _extract_keywords(self, text: str, max_keywords: int = 5) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ í† í° ê¸°ë°˜)."""
        import re

        if not text:
            return []

        # ìˆ«ì í¬í•¨ ë‹¨ì–´ ìš°ì„  (ì˜ˆ: 20mg, Â§ 1141.1)
        numeric_words = re.findall(r"\b\w*\d+\w*\b", text)

        # ëŒ€ë¬¸ì ì‹œì‘ ë‹¨ì–´ (ê³ ìœ ëª…ì‚¬)
        capitalized = re.findall(r"\b[A-Z][a-z]+\b", text)

        # ê²°í•© ë° ì¤‘ë³µ ì œê±°
        keywords = list(dict.fromkeys(numeric_words[:3] + capitalized[:3]))

        return keywords[:max_keywords]

    async def _find_legacy_regulation_db(
        self, regul_data: Dict[str, Any], db_session, exclude_regulation_id: int = None
    ) -> Optional[int]:
        """DBì—ì„œ Legacy ê·œì œ ê²€ìƒ‰ (vision_extraction_result ê¸°ë°˜)."""
        if not regul_data:
            logger.warning("regul_dataê°€ Noneì…ë‹ˆë‹¤")
            return None

        # vision_extraction_resultì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        vision_pages = regul_data.get("vision_extraction_result", [])
        if not vision_pages:
            logger.warning("vision_extraction_resultê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return None
        
        first_page = vision_pages[0]
        metadata = first_page.get("structure", {}).get("metadata", {})
        
        title = metadata.get("title", "")
        country = metadata.get("jurisdiction_code", "")
        
        logger.info(f"DB Legacy ê²€ìƒ‰: title={title}, country={country}")
        print(f"DB Legacy ê²€ìƒ‰: title={title}, country={country}")

        try:
            from app.core.repositories.regulation_repository import RegulationRepository

            repo = RegulationRepository()
            regulation = await repo.find_by_title_and_country(
                db_session, title, country, exclude_regulation_id
            )

            if regulation:
                logger.info(f"DB Legacy ë°œê²¬: regulation_id={regulation.regulation_id}")
                print(f"DB Legacy ë°œê²¬: regulation_id={regulation.regulation_id}")
                return regulation.regulation_id

            logger.info("DB Legacy ë¯¸ë°œê²¬")
            print("DB Legacy ë¯¸ë°œê²¬")
            return None

        except Exception as e:
            logger.error(f"DB Legacy ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None



    async def _match_reference_blocks(
        self, new_blocks: List[Dict[str, Any]], legacy_blocks: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        CoT Step 1: ê³„ì¸µ êµ¬ì¡° ê¸°ë°˜ ì •í™• ë§¤ì¹­ (ë°€ë¦¼ í˜„ìƒ ë°©ì§€).

        ì „ëµ:
        1. ê³„ì¸µ êµ¬ì¡° ì™„ì „ ì¼ì¹˜ (hierarchy ë°°ì—´ ë¹„êµ)
        2. section_ref ì¼ì¹˜ (fallback)
        3. í‚¤ì›Œë“œ ìœ ì‚¬ë„ (fuzzy matching)
        """
        logger.info("ê³„ì¸µ êµ¬ì¡° ê¸°ë°˜ ë§¤ì¹­ ì‹œì‘ (ê·œì¹™ ê¸°ë°˜)")

        matched_pairs = []
        matched_legacy_indices = set()

        # ì „ëµ 1: ê³„ì¸µ êµ¬ì¡° ì™„ì „ ì¼ì¹˜
        for new_block in new_blocks:
            new_hierarchy = new_block.get("hierarchy", [])

            if not new_hierarchy:
                continue

            for idx, legacy_block in enumerate(legacy_blocks):
                if idx in matched_legacy_indices:
                    continue

                legacy_hierarchy = legacy_block.get("hierarchy", [])

                # ê³„ì¸µ êµ¬ì¡° ì™„ì „ ì¼ì¹˜
                if new_hierarchy == legacy_hierarchy:
                    matched_pairs.append(
                        {
                            "new_block": new_block,
                            "legacy_block": legacy_block,
                            "match_confidence": 1.0,
                            "match_reason": f"Exact hierarchy match: {' > '.join(new_hierarchy)}",
                        }
                    )
                    matched_legacy_indices.add(idx)
                    break

        # ì „ëµ 2: section_ref ì¼ì¹˜ (fallback)
        for new_block in new_blocks:
            # ì´ë¯¸ ë§¤ì¹­ëœ ê²½ìš° ìŠ¤í‚µ
            if any(p["new_block"] == new_block for p in matched_pairs):
                continue

            new_section = new_block["section_ref"]

            for idx, legacy_block in enumerate(legacy_blocks):
                if idx in matched_legacy_indices:
                    continue

                legacy_section = legacy_block["section_ref"]

                if new_section == legacy_section:
                    matched_pairs.append(
                        {
                            "new_block": new_block,
                            "legacy_block": legacy_block,
                            "match_confidence": 0.9,
                            "match_reason": f"Section ref match: {new_section}",
                        }
                    )
                    matched_legacy_indices.add(idx)
                    break

        # ì „ëµ 3: í‚¤ì›Œë“œ ìœ ì‚¬ë„ (fuzzy matching)
        for new_block in new_blocks:
            if any(p["new_block"] == new_block for p in matched_pairs):
                continue

            new_keywords = set(new_block.get("keywords", []))

            if not new_keywords:
                continue

            best_match = None
            best_score = 0.0

            for idx, legacy_block in enumerate(legacy_blocks):
                if idx in matched_legacy_indices:
                    continue

                legacy_keywords = set(legacy_block.get("keywords", []))

                if not legacy_keywords:
                    continue

                # Jaccard ìœ ì‚¬ë„
                intersection = len(new_keywords & legacy_keywords)
                union = len(new_keywords | legacy_keywords)
                score = intersection / union if union > 0 else 0.0

                if score > best_score and score >= 0.5:  # ì„ê³„ê°’
                    best_score = score
                    best_match = (idx, legacy_block)

            if best_match:
                idx, legacy_block = best_match
                matched_pairs.append(
                    {
                        "new_block": new_block,
                        "legacy_block": legacy_block,
                        "match_confidence": best_score,
                        "match_reason": f"Keyword similarity: {best_score:.2f}",
                    }
                )
                matched_legacy_indices.add(idx)

        logger.info(
            f"ë§¤ì¹­ ì™„ë£Œ: {len(matched_pairs)}ê°œ ìŒ "
            f"(ì •í™•: {sum(1 for p in matched_pairs if p['match_confidence'] == 1.0)}, "
            f"section: {sum(1 for p in matched_pairs if p['match_confidence'] == 0.9)}, "
            f"fuzzy: {sum(1 for p in matched_pairs if p['match_confidence'] < 0.9)})"
        )
        return matched_pairs

    async def _detect_change_by_ref_id(
        self, pair: Dict[str, Any], new_regulation_id: str, legacy_regulation_id: str
    ) -> Optional[Dict[str, Any]]:
        """CoT Step 2-4: Reference ID ê¸°ë°˜ ì •ë°€ ë³€ê²½ ê°ì§€ (Agentic)."""
        new_block = pair["new_block"]
        legacy_block = pair["legacy_block"]

        section_ref = new_block["section_ref"]
        new_text = new_block["text"]
        legacy_text = legacy_block["text"]

        # Reference ID ìƒì„±
        new_ref_id = (
            f"{new_regulation_id}-{section_ref}-P{new_block.get('page_num', 0)}"
        )
        legacy_ref_id = (
            f"{legacy_regulation_id}-{section_ref}-P{legacy_block.get('page_num', 0)}"
        )

        # LLM í˜¸ì¶œ (ref_id ê¸°ë°˜ ì •ë°€ ë¹„êµ)
        try:
            prompt = f"""Perform PRECISE comparison using Reference IDs for context-aware analysis.

**Reference IDs:**
- Legacy: {legacy_ref_id}
- New: {new_ref_id}

**Legacy Regulation (Section {section_ref}):**
{legacy_text}

**New Regulation (Section {section_ref}):**
{new_text}

**Task**: 
1. Use Reference IDs to understand document context and hierarchy
2. Detect ALL substantive changes (numerical, wording, scope)
3. Follow Chain of Thought (4 steps)
4. Apply Adversarial Validation
5. Extract numerical changes with full context
"""

            response = await self.llm.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": CHANGE_DETECTION_SYSTEM_PROMPT},
                    {"role": "user", "content": prompt},
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
            )

            result = json.loads(response.choices[0].message.content)
            result["section_ref"] = section_ref
            result["new_ref_id"] = new_ref_id
            result["legacy_ref_id"] = legacy_ref_id

            return result

        except Exception as e:
            logger.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨ (Section {section_ref}): {e}")
            return {
                "section_ref": section_ref,
                "new_ref_id": new_ref_id,
                "legacy_ref_id": legacy_ref_id,
                "change_detected": False,
                "confidence_score": 0.0,
                "error": str(e),
            }


# ==================== ë…¸ë“œ í•¨ìˆ˜ ====================
_default_node: Optional[ChangeDetectionNode] = None


async def change_detection_node(state: AppState, config: Dict[str, Any] = None) -> AppState:
    """LangGraph ë…¸ë“œ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ (ë‚´ë¶€ì—ì„œ ì§§ì€ ì„¸ì…˜ ìƒì„±)."""
    global _default_node
    if _default_node is None:
        _default_node = ChangeDetectionNode()
    
    return await _default_node.run(state, db_session=None)


__all__ = ["ChangeDetectionNode", "change_detection_node", "ConfidenceScorer"]
