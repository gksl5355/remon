"""
module: change_detection.py
description: ê·œì œ ë³€ê²½ ê°ì§€ ë…¸ë“œ (Reference ID ê¸°ë°˜, ì „ì²˜ë¦¬ í›„ ì„ë² ë”© ì „)
author: AI Agent
created: 2025-01-18
updated: 2025-01-21 (ì¤‘ë³µ run() ë©”ì„œë“œ í†µí•©, ì‹ ê·œ ê·œì œ ë¶„ì„ ë¡œì§ ì¶”ê°€)
dependencies:
    - openai
    - app.vectorstore.vector_client
    - app.ai_pipeline.state
"""

import json
import logging
from typing import Dict, Any, List, Optional, Literal, Set
from openai import AsyncOpenAI

from app.ai_pipeline.state import AppState
from app.vectorstore.vector_client import VectorClient

logger = logging.getLogger(__name__)


# ==================== System Prompts ====================
CHANGE_DETECTION_SYSTEM_PROMPT = """You are a regulatory change detection expert with Reference ID-based context awareness.

**CRITICAL INSTRUCTIONS:**

1. **Complete Recall**: 
   - ì‚¬ì†Œí•´ ë³´ì´ëŠ” ìˆ˜ì¹˜ ë³€ê²½(ì˜ˆ: ê°’ A â†’ ê°’ B)ë„ ë°˜ë“œì‹œ ê°ì§€í•˜ì‹­ì‹œì˜¤. ë‹¨, ë°˜ë“œì‹œ ì œê³µëœ í…ìŠ¤íŠ¸ ë‚´ì— ì¡´ì¬í•˜ëŠ” ìˆ˜ì¹˜ë§Œ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
   - ë‹¨ì–´ í•˜ë‚˜ì˜ ì°¨ì´(ì˜ˆ: 'ê¶Œê³ ' â†’ 'ì˜ë¬´', 'may' â†’ 'shall')ë„ ë†“ì¹˜ì§€ ë§ˆì‹­ì‹œì˜¤.

2. **Context Preservation with Reference IDs**:
   - Reference IDë¥¼ í™œìš©í•˜ì—¬ ë¬¸ì„œ ê³„ì¸µ êµ¬ì¡°ì™€ ë§¥ë½ì„ íŒŒì•…í•˜ì‹­ì‹œì˜¤.
   - ìˆ˜ì¹˜ë¥¼ ì¶”ì¶œí•  ë•ŒëŠ” ë°˜ë“œì‹œ ì ìš© ëŒ€ìƒê³¼ ì¡°ê±´ì„ í•¨ê»˜ ëª…ì‹œí•˜ì‹­ì‹œì˜¤.
   - Reference ID í˜•ì‹: {regulation_id}-{section_ref}-P{page_num}

3. **Chain of Thought (4 Steps)**:
   Step 1: Reference ID ê¸°ë°˜ ë§¥ë½ íŒŒì•… (ë¬¸ì„œ êµ¬ì¡°, ê³„ì¸µ)
   Step 2: í•µì‹¬ ìš©ì–´ ë¹„êµ (ìˆ˜ì¹˜, ì˜ë¬´ í‘œí˜„, ì¡°ê±´ì ˆ)
   Step 3: ì˜ë¯¸ ë³€í™” í‰ê°€ (ì‹¤ì§ˆì  ì˜í–¥ë„)
   Step 4: ìµœì¢… íŒë‹¨ (ë³€ê²½ ìœ í˜•, ì‹ ë¢°ë„)

4. **Adversarial Validation**:
   - ìì‹ ì˜ íŒë‹¨ì„ ë°˜ë°•í•˜ëŠ” ê·¼ê±°ë¥¼ ì°¾ìœ¼ì‹­ì‹œì˜¤.
   - ìµœì¢… íŒë‹¨ ì‹œ ë°˜ë°• ê·¼ê±°ë¥¼ ê³ ë ¤í•˜ì—¬ confidenceë¥¼ ì¡°ì •í•˜ì‹­ì‹œì˜¤.

**OUTPUT FORMAT (JSON):**
{
  "change_detected": true/false,
  "confidence_score": 0.0-1.0,
  "change_type": "value_change" | "scope_change" | "new_clause" | "removed" | "wording_only",
  "legacy_snippet": "ì›ë¬¸ ë°œì·Œ (ìµœëŒ€ 200ì)",
  "new_snippet": "ì›ë¬¸ ë°œì·Œ (ìµœëŒ€ 200ì)",
  "reasoning": {
    "step1_context_analysis": "Reference ID ê¸°ë°˜ ë§¥ë½ ë¶„ì„...",
    "step2_term_comparison": "í•µì‹¬ ìš©ì–´ ë¹„êµ...",
    "step3_semantic_evaluation": "ì˜ë¯¸ ë³€í™” í‰ê°€...",
    "step4_final_judgment": "ìµœì¢… íŒë‹¨..."
  },
  "adversarial_check": {
    "counter_argument": "...",
    "rebuttal": "...",
    "adjusted_confidence": 0.0-1.0
  },
  "keywords": ["keyword1", "keyword2"],
  "numerical_changes": [
    {
      "field": "í•„ë“œëª…",
      "legacy_value": "ì´ì „ ê°’",
      "new_value": "ìƒˆ ê°’",
      "context": "ì ìš© ë§¥ë½",
      "impact": "HIGH" | "MEDIUM" | "LOW"
    }
  ]
}
"""

SECTION_MATCHING_PROMPT = """Match new reference blocks with legacy reference blocks based on section numbers and keywords.

Return JSON array of matches:
{
  "matches": [
    {
      "new_section_ref": "1114.5(a)(3)",
      "legacy_section_ref": "1114.5(a)(3)",
      "match_confidence": 0.98
    }
  ]
}
"""

NEW_REGULATION_ANALYSIS_PROMPT = """You are a regulatory compliance expert analyzing a NEW regulation.

**TASK:**
Extract key requirements and identify affected product areas for compliance mapping.

**INSTRUCTIONS:**
1. Summarize the regulation's main purpose (1-2 sentences)
2. Extract ALL key requirements:
   - Numerical limits (e.g., "nicotine â‰¤ 20mg/ml")
   - Mandatory features (e.g., "child-resistant packaging")
   - Prohibited substances
   - Labeling requirements
   - Testing/certification requirements
3. Identify affected product areas using normalized names:
   - Use snake_case (e.g., "nicotine_content", "package_volume")
   - Be specific (e.g., "warning_label_size" not just "labeling")

**OUTPUT FORMAT (JSON):**
{
  "regulation_summary": "Brief 1-2 sentence summary",
  "key_requirements": [
    {
      "requirement": "Descriptive name",
      "value": "Specific value or limit",
      "unit": "Unit if applicable (or null)",
      "context": "When/where this applies"
    }
  ],
  "affected_areas": ["snake_case_area_1", "snake_case_area_2"]
}
"""


# ==================== Confidence Scorer ====================
class ConfidenceScorer:
    """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°."""

    @staticmethod
    def adjust_confidence(result: Dict[str, Any]) -> float:
        base_confidence = result.get("confidence_score", 0.5)

        if "adversarial_check" in result:
            base_confidence = result["adversarial_check"].get(
                "adjusted_confidence", base_confidence
            )

        if result.get("numerical_changes"):
            base_confidence = min(base_confidence + 0.1, 1.0)

        return base_confidence

    @staticmethod
    def get_confidence_level(
        confidence: float,
    ) -> Literal["HIGH", "MEDIUM", "LOW", "UNCERTAIN"]:
        if confidence >= 0.9:
            return "HIGH"
        elif confidence >= 0.7:
            return "MEDIUM"
        elif confidence >= 0.5:
            return "LOW"
        else:
            return "UNCERTAIN"


# ==================== Change Detection Node ====================
class ChangeDetectionNode:
    """ë…ë¦½ ë³€ê²½ ê°ì§€ ë…¸ë“œ (Reference ID ê¸°ë°˜)."""

    def __init__(
        self,
        llm_client: Optional[AsyncOpenAI] = None,
        vector_client: Optional[VectorClient] = None,
        model_name: Optional[str] = None,
    ):
        from app.ai_pipeline.preprocess.config import PreprocessConfig

        if llm_client:
            self.llm = llm_client
        else:
            client = AsyncOpenAI()
            self.llm = PreprocessConfig.wrap_openai_client(client)

        self.vector_client = vector_client or VectorClient()
        self.model_name = model_name or PreprocessConfig.CHANGE_DETECTION_MODEL
        self.confidence_scorer = ConfidenceScorer()

    async def run(self, state: AppState, db_session=None) -> AppState:
        """ë³€ê²½ ê°ì§€ ë…¸ë“œ ì‹¤í–‰ (ì§§ì€ DB ì„¸ì…˜ ì‚¬ìš©)."""
        logger.info("=== Change Detection Node ì‹œì‘ (Reference ID ê¸°ë°˜) ===")
        change_context = state.get("change_context", {})
        if not change_context:
            logger.info("change_context ì—†ìŒ, ë³€ê²½ ê°ì§€ ìŠ¤í‚µ")
            state["change_detection_results"] = []
            state["change_summary"] = {
                "status": "skipped",
                "reason": "no_change_context",
            }
            return state

        new_regulation_id = change_context.get("new_regulation_id")

        # ìš°ì„ ìˆœìœ„: 1) change_context.new_regul_data 2) preprocess_results[0] 3) DB
        new_regul_data = change_context.get("new_regul_data")
        if not new_regul_data:
            pre_results = state.get("preprocess_results") or []
            if pre_results:
                new_regul_data = pre_results[0]
                if not new_regulation_id:
                    new_regulation_id = (
                        new_regul_data.get("regulation_id")
                        or new_regul_data.get("regulation", {}).get("regulation_id")
                        or "INLINE_NEW"
                    )

        legacy_regulation_id = change_context.get("legacy_regulation_id")
        legacy_regul_data = change_context.get("legacy_regul_data")

        # DBê°€ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì„¸ì…˜ì„ ì—°ë‹¤
        if not new_regul_data or (not legacy_regul_data and legacy_regulation_id):
            from app.core.repositories.regulation_repository import RegulationRepository
            from app.core.database import AsyncSessionLocal

            repo = RegulationRepository()
            async with AsyncSessionLocal() as session:
                if not new_regul_data:
                    if not new_regulation_id:
                        logger.error("new_regulation_id ì—†ìŒ")
                        state["change_detection_results"] = []
                        state["change_summary"] = {
                            "status": "error",
                            "reason": "no_new_regulation_id",
                        }
                        return state

                    new_regul_data = await repo.get_regul_data(
                        session, new_regulation_id
                    )
                    if not new_regul_data:
                        logger.warning(
                            f"ì‹ ê·œ regul_data ì—†ìŒ: regulation_id={new_regulation_id}"
                        )
                        state["change_detection_results"] = []
                        state["change_summary"] = {
                            "status": "error",
                            "reason": "no_new_regul_data",
                        }
                        return state

                if not legacy_regul_data:
                    if not legacy_regulation_id:
                        legacy_regulation_id = await self._find_legacy_regulation_db(
                            new_regul_data, session, new_regulation_id
                        )
                        if not legacy_regulation_id:
                            logger.info(
                                "âœ… ì™„ì „íˆ ìƒˆë¡œìš´ ê·œì œ (Legacy ì—†ìŒ) - ì‹ ê·œ ë¶„ì„ ì‹¤í–‰"
                            )

                            # ì‹ ê·œ ê·œì œ ë¶„ì„ (LLM)
                            analysis_hints = await self._analyze_new_regulation(
                                new_regul_data
                            )
                            state["regulation_analysis_hints"] = analysis_hints
                            logger.info(
                                f"âœ… ì‹ ê·œ ê·œì œ ë¶„ì„ ì™„ë£Œ: {len(analysis_hints.get('key_requirements', []))}ê°œ ìš”êµ¬ì‚¬í•­"
                            )
                            logger.info(
                                f"   affected_areas: {analysis_hints.get('affected_areas', [])}"
                            )

                            state["change_detection_results"] = []
                            state["change_summary"] = {
                                "status": "new_regulation",
                                "total_changes": 0,
                            }
                            state["needs_embedding"] = True
                            return state

                    legacy_regul_data = await repo.get_regul_data(
                        session, legacy_regulation_id
                    )
                    if not legacy_regul_data:
                        logger.warning(
                            f"Legacy regul_data ì—†ìŒ: regulation_id={legacy_regulation_id}"
                        )
                        state["change_detection_results"] = []
                        state["change_summary"] = {
                            "status": "error",
                            "reason": "legacy_not_found",
                        }
                        return state
                # end session block

        # legacy_regulation_id ì—†ì§€ë§Œ legacy_regul_data ì£¼ì…ëœ ê²½ìš° ê¸°ë³¸ê°’ ì„¸íŒ…
        if legacy_regul_data and not legacy_regulation_id:
            legacy_regulation_id = (
                legacy_regul_data.get("regulation_id")
                or legacy_regul_data.get("regulation", {}).get("regulation_id")
                or "LEGACY"
            )
        # new_regulation_id ì—†ì„ ë•Œë„ ê¸°ë³¸ê°’ ì„¸íŒ…
        if not new_regulation_id:
            new_regulation_id = (
                new_regul_data.get("regulation_id")
                or new_regul_data.get("regulation", {}).get("regulation_id")
                or "INLINE_NEW"
            )

        # ========== Reference Blocks ì¶”ì¶œ (ì„¸ì…˜ ë¶ˆí•„ìš”) ==========
        new_ref_blocks = self._extract_reference_blocks(new_regul_data)
        legacy_ref_blocks = self._extract_reference_blocks(legacy_regul_data)

        logger.info(
            f"Reference Blocks: ì‹ ê·œ {len(new_ref_blocks)}ê°œ, Legacy {len(legacy_ref_blocks)}ê°œ"
        )

        # ========== Section ë§¤ì¹­ (ì„¸ì…˜ ë¶ˆí•„ìš”) ==========
        matched_pairs = await self._match_reference_blocks(
            new_ref_blocks, legacy_ref_blocks
        )
        logger.info(f"Section ë§¤ì¹­ ì™„ë£Œ: {len(matched_pairs)}ê°œ ìŒ")

        # ========== LLM ë³€ê²½ ê°ì§€ (ë³‘ë ¬ ì²˜ë¦¬, 10ê°œ ë‹¨ìœ„) ==========
        import asyncio

        semaphore = asyncio.Semaphore(10)  # LangSmith ë¶€í•˜ ë°©ì§€

        async def detect_single_pair(pair):
            async with semaphore:
                return await self._detect_change_by_ref_id(
                    pair, new_regulation_id, legacy_regulation_id
                )

        logger.info(
            f"ğŸ”„ ë³€ê²½ ê°ì§€ ë³‘ë ¬ ì²˜ë¦¬: {len(matched_pairs)}ê°œ ì„¹ì…˜ (10ê°œ ë™ì‹œ ì œí•œ)"
        )

        detection_results_raw = await asyncio.gather(
            *[detect_single_pair(pair) for pair in matched_pairs],
            return_exceptions=True,
        )

        detection_results = []
        for result in detection_results_raw:
            if isinstance(result, Exception):
                logger.error(f"âŒ ë³€ê²½ ê°ì§€ ì‹¤íŒ¨: {result}")
                continue
            if result:
                detection_results.append(result)

        # ì‹ ë¢°ë„ ì¡°ì •
        for result in detection_results:
            result["confidence_score"] = self.confidence_scorer.adjust_confidence(
                result
            )
            result["confidence_level"] = self.confidence_scorer.get_confidence_level(
                result["confidence_score"]
            )

        total_changes = sum(1 for r in detection_results if r.get("change_detected"))
        high_confidence = sum(
            1 for r in detection_results if r.get("confidence_level") == "HIGH"
        )

        # ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“‹ ë³€ê²½ ê°ì§€ ìƒì„¸ ê²°ê³¼")
        logger.info("=" * 80)
        for idx, result in enumerate(detection_results, 1):
            section = result.get("section_ref", "Unknown")
            detected = result.get("change_detected", False)
            confidence = result.get("confidence_level", "UNKNOWN")
            change_type = result.get("change_type", "N/A")

            logger.info(f"\n[{idx}] Section: {section}")
            logger.info(f"  ë³€ê²½ ê°ì§€: {detected}")
            logger.info(
                f"  ì‹ ë¢°ë„: {confidence} ({result.get('confidence_score', 0):.2f})"
            )
            logger.info(f"  ë³€ê²½ ìœ í˜•: {change_type}")

            if detected:
                logger.info(f"  Legacy: {result.get('legacy_snippet', '')[:100]}...")
                logger.info(f"  New: {result.get('new_snippet', '')[:100]}...")

                numerical = result.get("numerical_changes", [])
                if numerical:
                    logger.info(f"  ìˆ˜ì¹˜ ë³€ê²½: {len(numerical)}ê°œ")
                    for num_change in numerical[:3]:
                        logger.info(
                            f"    - {num_change.get('field')}: {num_change.get('legacy_value')} â†’ {num_change.get('new_value')}"
                        )

        logger.info("\n" + "=" * 80)

        state["change_detection_results"] = detection_results
        state["change_summary"] = {
            "status": "completed",
            "total_reference_blocks": len(matched_pairs),
            "total_changes": total_changes,
            "high_confidence_changes": high_confidence,
            "legacy_regulation_id": legacy_regulation_id,
            "new_regulation_id": new_regulation_id,
        }

        # ğŸ”‘ Section ê¸°ë°˜ ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
        change_index = {}
        for result in detection_results:
            section = self._normalize_section_ref(result.get("section_ref", ""))
            if section and result.get("change_detected"):
                change_index[section] = result
        state["change_detection_index"] = change_index
        logger.info(f"ğŸ“š Change Index ìƒì„±: {len(change_index)}ê°œ ì„¹ì…˜")

        logger.info(
            f"âœ… ë³€ê²½ ê°ì§€ ì™„ë£Œ: {total_changes}ê°œ ë³€ê²½ ê°ì§€ (HIGH: {high_confidence})"
        )

        # ========== ì„ë² ë”© í•„ìš” ì—¬ë¶€ í”Œë˜ê·¸ ==========
        needs_embedding = total_changes > 0
        state["needs_embedding"] = needs_embedding
        logger.info(f"ğŸ“¦ ì„ë² ë”© í•„ìš”: {needs_embedding}")

        return state

    def _extract_reference_blocks(
        self, regul_data: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """regul_dataì—ì„œ reference_blocks ì¶”ì¶œ (Vision Pipeline êµ¬ì¡° ëŒ€ì‘)."""
        ref_blocks = []
        # Vision Pipeline ì¶œë ¥ êµ¬ì¡°
        vision_pages = regul_data.get("vision_extraction_result", [])

        doc_id = regul_data.get("regulation_id") or regul_data.get(
            "regulation", {}
        ).get("regulation_id")

        for page in vision_pages:
            structure = page.get("structure", {})
            page_num = page.get("page_num", 0)
            markdown_content = structure.get("markdown_content", "")
            reference_blocks = structure.get("reference_blocks", [])

            # reference_blocksê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if reference_blocks:
                lines = markdown_content.splitlines()
                for ref in reference_blocks:
                    start = max(0, ref.get("start_line", 0))
                    end = ref.get("end_line", len(lines))
                    if end <= start:
                        end = min(len(lines), start + 20)
                    snippet = "\n".join(lines[start:end]) if lines else markdown_content

                    kw = ref.get("keywords") or self._extract_keywords(snippet)

                    ref_blocks.append(
                        {
                            "section_ref": ref.get("section_ref", ""),
                            "text": snippet,
                            "keywords": kw,
                            "page_num": page_num,
                            "start_line": ref.get("start_line", 0),
                            "end_line": ref.get("end_line", 0),
                            "hierarchy": [],  # ê³„ì¸µ ì •ë³´ (í•„ìš”ì‹œ ì¶”ê°€)
                            "doc_id": doc_id,
                            "meta_doc_id": doc_id,
                        }
                    )
            else:
                # reference_blocksê°€ ì—†ìœ¼ë©´ í˜ì´ì§€ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë¸”ë¡ìœ¼ë¡œ
                ref_blocks.append(
                    {
                        "section_ref": f"Page {page_num}",
                        "text": markdown_content[:500],  # ì²˜ìŒ 500ì
                        "keywords": self._extract_keywords(markdown_content),
                        "page_num": page_num,
                        "start_line": 0,
                        "end_line": len(markdown_content.splitlines()),
                        "hierarchy": [],
                        "doc_id": doc_id,
                        "meta_doc_id": doc_id,
                    }
                )

        return ref_blocks

    def _extract_keywords(self, text: str, max_keywords: int = 5) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ í† í° ê¸°ë°˜)."""
        import re

        if not text:
            return []

        # ìˆ«ì í¬í•¨ ë‹¨ì–´ ìš°ì„  (ì˜ˆ: 20mg, Â§ 1141.1)
        numeric_words = re.findall(r"\b\w*\d+\w*\b", text)

        # ëŒ€ë¬¸ì ì‹œì‘ ë‹¨ì–´ (ê³ ìœ ëª…ì‚¬)
        capitalized = re.findall(r"\b[A-Z][a-z]+\b", text)

        # ê²°í•© ë° ì¤‘ë³µ ì œê±°
        keywords = list(dict.fromkeys(numeric_words[:3] + capitalized[:3]))

        return keywords[:max_keywords]

    async def _find_legacy_regulation_db(
        self, regul_data: Dict[str, Any], db_session, exclude_regulation_id: int = None
    ) -> Optional[int]:
        """DBì—ì„œ Legacy ê·œì œ ê²€ìƒ‰ (ê°•í™”ëœ ê²€ìƒ‰ ë¡œì§ + Citation Code ì •ê·œí™”)."""
        if not regul_data:
            logger.warning("regul_dataê°€ Noneì…ë‹ˆë‹¤")
            return None

        # vision_extraction_resultì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        vision_pages = regul_data.get("vision_extraction_result", [])
        if not vision_pages:
            logger.warning("vision_extraction_resultê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return None

        first_page = vision_pages[0]
        metadata = first_page.get("structure", {}).get("metadata", {})

        title = metadata.get("title", "")
        country = metadata.get("jurisdiction_code", "")
        citation_code = metadata.get("citation_code", "")
        version = metadata.get("version", "")
        effective_date = metadata.get("effective_date", "")

        # Citation Code ì •ê·œí™” (í•˜ì´í”ˆ ì œê±°, ëŒ€ë¬¸ì ë³€í™˜)
        def normalize_citation(code: str) -> str:
            if not code:
                return ""
            return code.upper().replace("-", "").replace(" ", "")

        normalized_citation = normalize_citation(citation_code)

        logger.info(f"DB Legacy ê²€ìƒ‰: title={title}, country={country}")
        # noisy print ì œê±°, loggerë¡œë§Œ ê¸°ë¡

        try:
            from app.core.repositories.regulation_repository import RegulationRepository

            repo = RegulationRepository()

            # 1ìˆœìœ„: citation_code + country (ì •ê·œí™”ëœ ì½”ë“œë¡œ ê²€ìƒ‰)
            if normalized_citation and country:
                # ì›ë³¸ citation_codeë¡œ ê²€ìƒ‰
                regulation = await repo.find_by_citation_and_country(
                    db_session, citation_code, country, exclude_regulation_id
                )
                if regulation:
                    logger.info(
                        f"DB Legacy ë°œê²¬ (citation ì›ë³¸): regulation_id={regulation.regulation_id}"
                    )
                    return regulation.regulation_id

                # ì •ê·œí™”ëœ citation_codeë¡œ ì¬ê²€ìƒ‰ (fallback)
                regulation = await repo.find_by_citation_normalized(
                    db_session, normalized_citation, country, exclude_regulation_id
                )
                if regulation:
                    logger.info(
                        f"DB Legacy ë°œê²¬ (citation ì •ê·œí™”): regulation_id={regulation.regulation_id}"
                    )
                    return regulation.regulation_id

            # 2ìˆœìœ„: title + country + version
            if title and country and version:
                regulation = await repo.find_by_title_country_version(
                    db_session, title, country, version, exclude_regulation_id
                )
                if regulation:
                    logger.info(
                        f"DB Legacy ë°œê²¬ (title+version): regulation_id={regulation.regulation_id}"
                    )
                    return regulation.regulation_id

            # 3ìˆœìœ„: title + country (ê¸°ì¡´ ë¡œì§)
            if title and country:
                regulation = await repo.find_by_title_and_country(
                    db_session, title, country, exclude_regulation_id
                )
                if regulation:
                    logger.info(
                        f"DB Legacy ë°œê²¬ (title): regulation_id={regulation.regulation_id}"
                    )
                    return regulation.regulation_id
            regulation = await repo.find_by_title_and_country(
                db_session, title, country, exclude_regulation_id
            )

            if regulation:
                logger.info(f"DB Legacy ë°œê²¬: regulation_id={regulation.regulation_id}")
                return regulation.regulation_id

            logger.info("DB Legacy ë¯¸ë°œê²¬")
            return None

        except Exception as e:
            logger.error(f"DB Legacy ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def _normalize_section_ref(self, section_ref: str) -> str:
        """ì¡°í•­ ë²ˆí˜¸ ì •ê·œí™” (Â§1160.5, 1160.5, Â§ 1160.5 â†’ 1160.5)."""
        import re

        normalized = re.sub(r"[Â§\s]", "", section_ref)
        match = re.search(r"(\d+\.\d+)", normalized)
        return match.group(1) if match else normalized

    async def _match_reference_blocks(
        self, new_blocks: List[Dict[str, Any]], legacy_blocks: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Strict Section Matching: ì¡°í•­ ë²ˆí˜¸ ê¸°ë°˜ ì •í™•í•œ 1:1 ë§¤ì¹­ (ì¤‘ë³µ ì œê±°).
        """
        logger.info("ğŸ” Strict Section Matching ì‹œì‘ (ì¤‘ë³µ ì œê±°)")

        # ì¤‘ë³µ ì œê±°: ê°™ì€ section_refëŠ” ì²˜ìŒ í•˜ë‚˜ë§Œ ì‚¬ìš©
        def deduplicate_blocks(blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
            seen_sections = set()
            unique_blocks = []
            for block in blocks:
                section = self._normalize_section_ref(block.get("section_ref", ""))
                if section and section not in seen_sections:
                    seen_sections.add(section)
                    unique_blocks.append(block)
            return unique_blocks

        new_blocks_unique = deduplicate_blocks(new_blocks)
        legacy_blocks_unique = deduplicate_blocks(legacy_blocks)

        logger.info(
            f"ğŸ§¹ ì¤‘ë³µ ì œê±°: New {len(new_blocks)} â†’ {len(new_blocks_unique)}, "
            f"Legacy {len(legacy_blocks)} â†’ {len(legacy_blocks_unique)}"
        )

        matched_pairs = []
        matched_legacy_sections = set()

        # ì •ê·œí™”ëœ ì¡°í•­ ë²ˆí˜¸ ê¸°ë°˜ 1:1 ë§¤ì¹­
        for new_block in new_blocks_unique:
            new_section = new_block.get("section_ref", "")
            new_normalized = self._normalize_section_ref(new_section)

            if not new_normalized:
                continue

            for legacy_block in legacy_blocks_unique:
                legacy_section = legacy_block.get("section_ref", "")
                legacy_normalized = self._normalize_section_ref(legacy_section)

                if legacy_normalized in matched_legacy_sections:
                    continue

                if new_normalized == legacy_normalized:
                    matched_pairs.append(
                        {
                            "new_block": new_block,
                            "legacy_block": legacy_block,
                            "match_confidence": 1.0,
                            "match_reason": f"Exact section: {new_normalized}",
                        }
                    )
                    matched_legacy_sections.add(legacy_normalized)
                    logger.debug(f"âœ… Matched: {new_section} â†” {legacy_section}")
                    break

        # ë§¤ì¹­ ì‹¤íŒ¨í•œ ì„¹ì…˜ ë¡œê·¸
        unmatched_new = [
            b.get("section_ref")
            for b in new_blocks_unique
            if not any(p["new_block"] == b for p in matched_pairs)
        ]
        if unmatched_new:
            logger.warning(f"âš ï¸ ë§¤ì¹­ ì‹¤íŒ¨í•œ ì‹ ê·œ ì„¹ì…˜: {unmatched_new[:5]}...")

        logger.info(
            f"âœ… ë§¤ì¹­ ì™„ë£Œ: {len(matched_pairs)}ê°œ ìŒ "
            f"(Exact: {sum(1 for p in matched_pairs if p['match_confidence'] == 1.0)})"
        )
        return matched_pairs

    async def _detect_change_by_ref_id(
        self, pair: Dict[str, Any], new_regulation_id: str, legacy_regulation_id: str
    ) -> Optional[Dict[str, Any]]:
        """CoT Step 2-4: Reference ID ê¸°ë°˜ ì •ë°€ ë³€ê²½ ê°ì§€ (Agentic)."""
        new_block = pair["new_block"]
        legacy_block = pair["legacy_block"]

        section_ref = new_block["section_ref"]
        new_text = new_block["text"]
        legacy_text = legacy_block["text"]
        new_doc_id = new_block.get("doc_id")
        legacy_doc_id = legacy_block.get("doc_id")

        # Reference ID ìƒì„±
        new_ref_id = (
            f"{new_regulation_id}-{section_ref}-P{new_block.get('page_num', 0)}"
        )
        legacy_ref_id = (
            f"{legacy_regulation_id}-{section_ref}-P{legacy_block.get('page_num', 0)}"
        )

        # LLM í˜¸ì¶œ (ref_id ê¸°ë°˜ ì •ë°€ ë¹„êµ)
        try:
            prompt = f"""Perform PRECISE comparison using Reference IDs for context-aware analysis.

**Reference IDs:**
- Legacy: {legacy_ref_id}
- New: {new_ref_id}

**Legacy Regulation (Section {section_ref}):**
{legacy_text}

**New Regulation (Section {section_ref}):**
{new_text}

**Task**: 
1. Use Reference IDs to understand document context and hierarchy
2. Detect ALL substantive changes (numerical, wording, scope)
3. Follow Chain of Thought (4 steps)
4. Apply Adversarial Validation
5. Extract numerical changes with full context
"""

            # GPT-5 nanoëŠ” temperature íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›
            call_params = {
                "model": self.model_name,
                "messages": [
                    {"role": "system", "content": CHANGE_DETECTION_SYSTEM_PROMPT},
                    {"role": "user", "content": prompt},
                ],
                "response_format": {"type": "json_object"},
            }

            # gpt-5-nanoê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ temperature ì¶”ê°€
            if "gpt-5-nano" not in self.model_name.lower():
                call_params["temperature"] = 0.1

            response = await self.llm.chat.completions.create(**call_params)

            result = json.loads(response.choices[0].message.content)
            result["section_ref"] = section_ref
            result["new_ref_id"] = new_ref_id
            result["legacy_ref_id"] = legacy_ref_id
            result["doc_id"] = new_doc_id
            result["meta_doc_id"] = new_doc_id
            result.setdefault("new_snippet", new_text[:1000])
            result.setdefault("legacy_snippet", legacy_text[:1000])

            # í‚¤ì›Œë“œ/í•„ë“œ ë³´ê°• (ê²€ìƒ‰/ë§¤í•‘ íŒíŠ¸ìš©)
            kw: Set[str] = set(result.get("keywords") or [])
            kw |= set(self._extract_keywords(new_text, max_keywords=5))
            kw |= set(self._extract_keywords(legacy_text, max_keywords=5))
            for num_change in result.get("numerical_changes", []) or []:
                field = num_change.get("field")
                if field:
                    kw.add(str(field))
            if kw:
                result["keywords"] = list(kw)

            return result

        except Exception as e:
            logger.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨ (Section {section_ref}): {e}")
            return {
                "section_ref": section_ref,
                "new_ref_id": new_ref_id,
                "legacy_ref_id": legacy_ref_id,
                "change_detected": False,
                "confidence_score": 0.0,
                "error": str(e),
            }

    async def _analyze_new_regulation(
        self, regul_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì‹ ê·œ ê·œì œ ë¶„ì„ (Legacy ì—†ì„ ë•Œ LLMìœ¼ë¡œ í•µì‹¬ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ)."""
        vision_pages = regul_data.get("vision_extraction_result", [])
        if not vision_pages:
            return {
                "regulation_summary": "",
                "key_requirements": [],
                "affected_areas": [],
            }

        # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìµœëŒ€ 5000ì)
        full_text = ""
        for page in vision_pages[:10]:  # ìµœëŒ€ 10í˜ì´ì§€
            markdown = page.get("structure", {}).get("markdown_content", "")
            full_text += markdown + "\n\n"

        full_text = full_text[:5000]

        user_prompt = f"""**Regulation Text:**
{full_text}
"""

        try:
            # GPT-5 nanoëŠ” temperature íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›
            call_params = {
                "model": self.model_name,
                "messages": [
                    {"role": "system", "content": NEW_REGULATION_ANALYSIS_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                "response_format": {"type": "json_object"},
            }

            # gpt-5-nanoê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ temperature ì¶”ê°€
            if "gpt-5-nano" not in self.model_name.lower():
                call_params["temperature"] = 0.1

            response = await self.llm.chat.completions.create(**call_params)

            result = json.loads(response.choices[0].message.content)
            return result

        except Exception as e:
            logger.error(f"ì‹ ê·œ ê·œì œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "regulation_summary": "",
                "key_requirements": [],
                "affected_areas": [],
            }


# ==================== ë…¸ë“œ í•¨ìˆ˜ ====================
_default_node: Optional[ChangeDetectionNode] = None


async def change_detection_node(
    state: AppState, config: Dict[str, Any] = None
) -> AppState:
    """LangGraph ë…¸ë“œ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ (ë‚´ë¶€ì—ì„œ ì§§ì€ ì„¸ì…˜ ìƒì„±)."""
    global _default_node
    if _default_node is None:
        _default_node = ChangeDetectionNode()

    # ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: ì´ë¯¸ ê²°ê³¼ê°€ ìˆê³  ê°•ì œ ì¬ì‹¤í–‰ì´ ì•„ë‹Œ ê²½ìš° skip
    if (
        state.get("change_detection_ran_inline")
        or state.get("change_detection_results")
    ) and not state.get("force_rerun_change_detection"):
        logger.info("change_detection ì´ë¯¸ ì‹¤í–‰ë¨. ì¬ì‹¤í–‰ ê±´ë„ˆëœ€.")
        return state

    return await _default_node.run(state, db_session=None)


__all__ = ["ChangeDetectionNode", "change_detection_node", "ConfidenceScorer"]
