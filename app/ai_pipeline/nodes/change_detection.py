# app/ai_pipeline/nodes/change_detection.py
"""
module: change_detection.py
description: ê·œì œ ë³€ê²½ ê°ì§€ ë…¸ë“œ (Reference ID ê¸°ë°˜, ì „ì²˜ë¦¬ í›„ ì„ë² ë”© ì „)
author: AI Agent
created: 2025-01-18
updated: 2025-01-23 (HITL ê¸°ëŠ¥ ì¶”ê°€ - refined_change_detection_prompt ì§€ì›)
dependencies:
    - openai
    - app.vectorstore.vector_client
    - app.ai_pipeline.state
    - app.ai_pipeline.prompts.change_detection_prompt
    - app.core.repositories.regulation_keynote_repository
"""

import json
import logging
from typing import Dict, Any, List, Optional, Literal, Set
from datetime import datetime
from openai import AsyncOpenAI
from sqlalchemy import text

from app.ai_pipeline.state import AppState
from app.vectorstore.vector_client import VectorClient
from app.ai_pipeline.prompts.change_detection_prompt import (
    CHANGE_DETECTION_SYSTEM_PROMPT,
    SECTION_MATCHING_PROMPT,
    NEW_REGULATION_ANALYSIS_PROMPT,
)

logger = logging.getLogger(__name__)


# ==================== Confidence Scorer ====================
class ConfidenceScorer:
    """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°."""

    @staticmethod
    def adjust_confidence(result: Dict[str, Any]) -> float:
        base_confidence = result.get("confidence_score", 0.5)

        if "adversarial_check" in result:
            base_confidence = result["adversarial_check"].get(
                "adjusted_confidence", base_confidence
            )

        if result.get("numerical_changes"):
            base_confidence = min(base_confidence + 0.1, 1.0)

        return base_confidence

    @staticmethod
    def get_confidence_level(
        confidence: float,
    ) -> Literal["HIGH", "MEDIUM", "LOW", "UNCERTAIN"]:
        if confidence >= 0.8:  # ì™„í™”: 0.9 â†’ 0.8
            return "HIGH"
        elif confidence >= 0.5:  # ì™„í™”: 0.7 â†’ 0.5
            return "MEDIUM"
        elif confidence >= 0.4:  # ì™„í™”: 0.5 â†’ 0.4
            return "LOW"
        else:
            return "UNCERTAIN"


##sa-mj í†µí•© (160 - 236)
# ==================== Change Detection Node ====================
class ChangeDetectionNode:
    """ë…ë¦½ ë³€ê²½ ê°ì§€ ë…¸ë“œ (Reference ID ê¸°ë°˜)."""

    def __init__(
        self,
        llm_client: Optional[AsyncOpenAI] = None,
        vector_client: Optional[VectorClient] = None,
        model_name: Optional[str] = None,
    ):
        from app.ai_pipeline.preprocess.config import PreprocessConfig

        if llm_client:
            self.llm = llm_client
        else:
            client = AsyncOpenAI()
            self.llm = PreprocessConfig.wrap_openai_client(client)

        self.vector_client = vector_client or VectorClient()
        self.model_name = model_name or PreprocessConfig.CHANGE_DETECTION_MODEL
        self.confidence_scorer = ConfidenceScorer()

    def _build_keynote_data(
        self,
        detection_results: List[Dict[str, Any]],
        change_summary: Dict[str, Any],
        regulation_meta: Dict[str, Any],
        legacy_regulation_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Change Detection ê²°ê³¼ë¥¼ Keynote JSONìœ¼ë¡œ ë³€í™˜"""
        from datetime import datetime

        return {
            "regulation_id": regulation_meta.get("regulation_id"),
            "country": regulation_meta.get("country"),
            "citation_code": regulation_meta.get("citation_code"),
            "title": regulation_meta.get("title"),
            "effective_date": regulation_meta.get("effective_date"),
            "analysis_date": datetime.utcnow().isoformat() + "Z",
            "change_summary": {
                "total_sections_analyzed": change_summary.get(
                    "total_reference_blocks", 0
                ),
                "total_changes_detected": change_summary.get("total_changes", 0),
                "high_confidence_changes": change_summary.get(
                    "high_confidence_changes", 0
                ),
            },
            "section_changes": [
                {
                    "section_ref": r.get("section_ref"),
                    "change_detected": r.get("change_detected"),
                    "confidence_level": r.get("confidence_level"),
                    "confidence_score": r.get("confidence_score"),
                    "change_type": r.get("change_type"),
                    "comparison": {
                        "legacy_snippet": r.get("legacy_snippet", ""),
                        "new_snippet": r.get("new_snippet", ""),
                    },
                    "reasoning": r.get("reasoning", {}),
                    "numerical_changes": r.get("numerical_changes", []),
                    "keywords": r.get("keywords", []),
                    "new_ref_id": r.get("new_ref_id"),
                    "legacy_ref_id": r.get("legacy_ref_id"),
                }
                for r in detection_results
            ],
            "legacy_regulation": (
                {"regulation_id": legacy_regulation_id}
                if legacy_regulation_id
                else None
            ),
        }

    async def run(self, state: AppState, db_session=None) -> AppState:
        """ë³€ê²½ ê°ì§€ ë…¸ë“œ ì‹¤í–‰ (ì§§ì€ DB ì„¸ì…˜ ì‚¬ìš©)."""
        logger.info("=== Change Detection Node ì‹œì‘ (Reference ID ê¸°ë°˜) ===")
        # ì‹ ê·œ ê·œì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (preprocess_results ìš°ì„ )
        pre_results = state.get("preprocess_results") or []
        if not pre_results:
            logger.info("preprocess_results ì—†ìŒ, ë³€ê²½ ê°ì§€ ìŠ¤í‚µ")
            state["change_detection_results"] = []
            state["change_summary"] = {
                "status": "skipped",
                "reason": "no_preprocess_results",
            }
            # ì‹¤í–‰ ìƒíƒœ ë§ˆí‚¹
            self._mark_execution_state(state)
            return state

        new_regul_data = pre_results[0]
        if new_regul_data.get("status") != "success":
            logger.error("âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨, ë³€ê²½ ê°ì§€ ìŠ¤í‚µ")
            state["change_detection_results"] = []
            state["change_summary"] = {"status": "error", "reason": "preprocess_failed"}
            return state

        new_regulation_id = new_regul_data.get("regulation_id", "NEW")
        legacy_regul_data = None
        legacy_regulation_id = None  # ì´ˆê¸°í™”

        # citation_code ê¸°ë°˜ìœ¼ë¡œ Legacy ê²€ìƒ‰ (ìƒˆ DB ì„¸ì…˜ ìƒì„±)
        if not legacy_regul_data:
            from app.core.repositories.regulation_repository import RegulationRepository
            from app.core.database import AsyncSessionLocal

            repo = RegulationRepository()
            # ìƒˆ ì„¸ì…˜ ìƒì„± (ì´ì „ ì„¸ì…˜ ì—°ê²° ëŠê¹€ ë°©ì§€)
            async with AsyncSessionLocal() as session:
                if not new_regul_data:
                    if not new_regulation_id:
                        logger.error("new_regulation_id ì—†ìŒ")
                        state["change_detection_results"] = []
                        state["change_summary"] = {
                            "status": "error",
                            "reason": "no_new_regulation_id",
                        }
                        self._mark_execution_state(state)
                        return state

                    new_regul_data = await repo.get_regul_data(
                        session, new_regulation_id
                    )
                    if not new_regul_data:
                        logger.warning(
                            f"ì‹ ê·œ regul_data ì—†ìŒ: regulation_id={new_regulation_id}"
                        )
                        state["change_detection_results"] = []
                        state["change_summary"] = {
                            "status": "error",
                            "reason": "no_new_regul_data",
                        }
                        self._mark_execution_state(state)
                        return state

                if not legacy_regul_data:
                    # citation_code ê¸°ë°˜ìœ¼ë¡œ Legacy ê²€ìƒ‰ (regulation_id ë¬´ì‹œ)
                    logger.info(f"ğŸ” new_regul_data í™•ì¸: {bool(new_regul_data)}")
                    if new_regul_data:
                        logger.info(
                            f"   new_regul_data keys: {list(new_regul_data.keys())}"
                        )

                    vision_pages = (
                        new_regul_data.get("vision_extraction_result", [])
                        if new_regul_data
                        else []
                    )
                    logger.info(f"   vision_pages ê°œìˆ˜: {len(vision_pages)}")

                    if vision_pages:
                        structure = vision_pages[0].get("structure", {})
                        new_metadata = structure.get("metadata") or {}
                        new_citation = new_metadata.get("citation_code")
                        new_country = new_metadata.get("jurisdiction_code")

                        if vision_pages:
                            new_metadata = (
                                vision_pages[0].get("structure", {}).get("metadata", {})
                            )
                            new_citation = new_metadata.get("citation_code")
                            new_country = new_metadata.get("jurisdiction_code")

                            # citation_code + countryë¡œ Legacy ì§ì ‘ ì¡°íšŒ (ì›”-ì¼ ê¸°ì¤€)
                            try:
                                result = await session.execute(
                                    text(
                                        """
                                        SELECT regul_data FROM regulations
                                        WHERE citation_code = :citation
                                        AND country_code = :country
                                        AND TO_CHAR(created_at, 'MMDD') < TO_CHAR(CURRENT_TIMESTAMP, 'MMDD')
                                        ORDER BY created_at DESC LIMIT 1
                                    """
                                    ),
                                    {"citation": new_citation, "country": new_country},
                                )
                                row = result.fetchone()
                                if row:
                                    legacy_regul_data = row[0]
                                    logger.info(
                                        f"âœ… Legacy ë°œê²¬ (ì›”-ì¼ ê¸°ì¤€): citation={new_citation}"
                                    )
                            except Exception as db_err:
                                logger.error(f"âŒ DB ì¿¼ë¦¬ ì‹¤íŒ¨ (ì—°ê²° ëŠê¹€): {db_err}")
                                logger.info("âš ï¸ Legacy ê²€ìƒ‰ ì‹¤íŒ¨ - ì‹ ê·œ ê·œì œë¡œ ì²˜ë¦¬")

                    # ì—¬ì „íˆ legacy_regul_data ì—†ìœ¼ë©´ â†’ ì™„ì „ ì‹ ê·œ ê·œì œ ì²˜ë¦¬
                    if not legacy_regul_data:
                        logger.warning("âš ï¸ Legacy ê²€ìƒ‰ ì‹¤íŒ¨ - ì‹ ê·œ ê·œì œë¡œ ì²˜ë¦¬")
                        logger.info(
                            "âœ… ì™„ì „íˆ ìƒˆë¡œìš´ ê·œì œ (Legacy ì—†ìŒ) - ì‹ ê·œ ë¶„ì„ ì‹¤í–‰"
                        )

                        # ì‹ ê·œ ê·œì œ ë¶„ì„ (LLM)
                        analysis_hints = await self._analyze_new_regulation(
                            new_regul_data
                        )
                        state["regulation_analysis_hints"] = analysis_hints
                        logger.info(
                            f"âœ… ì‹ ê·œ ê·œì œ ë¶„ì„ ì™„ë£Œ: {len(analysis_hints.get('key_requirements', []))}ê°œ ìš”êµ¬ì‚¬í•­"
                        )
                        logger.info(
                            f"   affected_areas: {analysis_hints.get('affected_areas', [])}"
                        )

                        # ğŸ†• ì‹ ê·œ ê·œì œ Keynote ë°ì´í„° ìƒì„±
                        keynote_data = {
                            "regulation_id": new_regulation_id,
                            "country": new_country,
                            "citation_code": new_citation,
                            "title": new_metadata.get("title", "Unknown Regulation"),
                            "effective_date": new_metadata.get("effective_date"),
                            "analysis_date": datetime.utcnow().isoformat() + "Z",
                            "change_summary": {
                                "total_sections_analyzed": 0,
                                "total_changes_detected": 0,
                                "high_confidence_changes": 0,
                            },
                            "section_changes": [],  # ì‹ ê·œ ê·œì œëŠ” ë³€ê²½ ì‚¬í•­ ì—†ìŒ
                            "new_regulation_analysis": analysis_hints,  # ì‹ ê·œ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                            "legacy_regulation": None,
                        }
                        state["change_keynote_data"] = keynote_data
                        logger.info("ğŸ“ ì‹ ê·œ ê·œì œ Keynote ë°ì´í„° ìƒì„± ì™„ë£Œ (report ë…¸ë“œì—ì„œ ì €ì¥ ì˜ˆì •)")
                        logger.info(f"   - regulation_id: {new_regulation_id}")
                        logger.info(f"   - country: {new_country}")
                        logger.info(f"   - citation_code: {new_citation}")
                        logger.info(
                            f"   - key_requirements: {len(analysis_hints.get('key_requirements', []))}ê°œ"
                        )

                        state["change_detection_results"] = []
                        state["change_summary"] = {
                            "status": "new_regulation",
                            "total_changes": 0,
                        }
                        state["needs_embedding"] = True
                        self._mark_execution_state(state)
                        return state
                # end session block

        # legacy_regulation_id ì—†ì§€ë§Œ legacy_regul_data ì£¼ì…ëœ ê²½ìš° ê¸°ë³¸ê°’ ì„¸íŒ…
        if legacy_regul_data and not legacy_regulation_id:
            legacy_regulation_id = (
                legacy_regul_data.get("regulation_id")
                or legacy_regul_data.get("regulation", {}).get("regulation_id")
                or "LEGACY"
            )
        # new_regulation_id ì—†ì„ ë•Œë„ ê¸°ë³¸ê°’ ì„¸íŒ…
        if not new_regulation_id:
            new_regulation_id = (
                new_regul_data.get("regulation_id")
                or new_regul_data.get("regulation", {}).get("regulation_id")
                or "INLINE_NEW"
            )

        # ========== ìµœì¢… ê²€ì¦: legacy_regul_data í™•ì¸ ==========
        if not legacy_regul_data:
            logger.error("âŒ legacy_regul_dataê°€ Noneì…ë‹ˆë‹¤")
            logger.error(
                "ğŸ’¡ í•´ê²°: python scripts/run_full_pipeline.py --mode legacy ì‹¤í–‰ í•„ìš”"
            )
            state["change_detection_results"] = []
            state["change_summary"] = {
                "status": "error",
                "reason": "legacy_data_is_none",
                "message": "Legacy ê·œì œë¥¼ ë¨¼ì € DBì— ì €ì¥í•˜ì„¸ìš” (--mode legacy)",
            }
            return state

        # ========== Reference Blocks ì¶”ì¶œ (ì„¸ì…˜ ë¶ˆí•„ìš”) ==========
        new_ref_blocks = self._extract_reference_blocks(new_regul_data)
        legacy_ref_blocks = self._extract_reference_blocks(legacy_regul_data)

        logger.info(
            f"Reference Blocks: ì‹ ê·œ {len(new_ref_blocks)}ê°œ, Legacy {len(legacy_ref_blocks)}ê°œ"
        )

        # ========== Section ë§¤ì¹­ (ì„¸ì…˜ ë¶ˆí•„ìš”) ==========
        matched_pairs = await self._match_reference_blocks(
            new_ref_blocks, legacy_ref_blocks
        )
        logger.info(f"Section ë§¤ì¹­ ì™„ë£Œ: {len(matched_pairs)}ê°œ ìŒ")

        # ========== LLM ë³€ê²½ ê°ì§€ (ë³‘ë ¬ ì²˜ë¦¬, 10ê°œ ë‹¨ìœ„) ==========
        import asyncio

        semaphore = asyncio.Semaphore(10)  # LangSmith ë¶€í•˜ ë°©ì§€

        async def detect_single_pair(pair):
            async with semaphore:
                return await self._detect_change_by_ref_id(
                    pair, new_regulation_id, legacy_regulation_id, state
                )

        logger.info(
            f"ğŸ”„ ë³€ê²½ ê°ì§€ ë³‘ë ¬ ì²˜ë¦¬: {len(matched_pairs)}ê°œ ì„¹ì…˜ (10ê°œ ë™ì‹œ ì œí•œ)"
        )

        detection_results_raw = await asyncio.gather(
            *[detect_single_pair(pair) for pair in matched_pairs],
            return_exceptions=True,
        )

        detection_results = []
        for result in detection_results_raw:
            if isinstance(result, Exception):
                logger.error(f"âŒ ë³€ê²½ ê°ì§€ ì‹¤íŒ¨: {result}")
                continue
            if result:
                detection_results.append(result)

        # ì¤‘ë³µ ì œê±° (Section ê¸°ì¤€)
        seen_sections = {}
        for result in detection_results:
            section = self._normalize_section_ref(result.get("section_ref", ""))
            if not section:
                continue
            
            # ê°™ì€ Sectionì´ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ë†’ì€ ê²ƒë§Œ ìœ ì§€
            if section in seen_sections:
                existing = seen_sections[section]
                if result.get("confidence_score", 0) > existing.get("confidence_score", 0):
                    seen_sections[section] = result
            else:
                seen_sections[section] = result
        
        detection_results = list(seen_sections.values())
        logger.info(f"ğŸ”„ ì¤‘ë³µ ì œê±° í›„: {len(detection_results)}ê°œ ìœ ë‹ˆí¬ ì„¹ì…˜")
        
        # ì‹ ë¢°ë„ ì¡°ì • ë° í•„í„°ë§
        filtered_results = []
        for result in detection_results:
            result["confidence_score"] = self.confidence_scorer.adjust_confidence(result)
            result["confidence_level"] = self.confidence_scorer.get_confidence_level(
                result["confidence_score"]
            )
            
            # LOW/UNCERTAIN í•„í„°ë§ (ì™„í™”ëœ ì¡°ê±´)
            if result.get("change_detected"):
                if result["confidence_score"] >= 0.5:  # ì™„í™”: 0.65 â†’ 0.5
                    filtered_results.append(result)
                else:
                    logger.debug(f"âš ï¸ ë‚®ì€ ì‹ ë¢°ë„ë¡œ ì œì™¸: {result.get('section_ref')} ({result['confidence_score']:.2f})")
            else:
                # ë³€ê²½ ì—†ìŒë„ ì™„í™”
                if result["confidence_score"] >= 0.55:  # ì™„í™”: 0.7 â†’ 0.55
                    filtered_results.append(result)
        
        # í•„í„°ë§ ì „ ì „ì²´ ê²°ê³¼ ë°±ì—… (Keynote ì €ì¥ìš©)
        all_detection_results = detection_results.copy()
        
        detection_results = filtered_results
        logger.info(f"âœ… ì‹ ë¢°ë„ í•„í„°ë§ í›„: {len(detection_results)}ê°œ")
        
        total_changes = sum(1 for r in detection_results if r.get("change_detected"))
        high_confidence = sum(
            1 for r in detection_results if r.get("confidence_level") == "HIGH"
        )

        # ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“‹ ë³€ê²½ ê°ì§€ ìƒì„¸ ê²°ê³¼")
        logger.info("=" * 80)
        for idx, result in enumerate(detection_results, 1):
            section = result.get("section_ref", "Unknown")
            detected = result.get("change_detected", False)
            confidence = result.get("confidence_level", "UNKNOWN")
            change_type = result.get("change_type", "N/A")

            logger.info(f"\n[{idx}] Section: {section}")
            logger.info(f"  ë³€ê²½ ê°ì§€: {detected}")
            logger.info(
                f"  ì‹ ë¢°ë„: {confidence} ({result.get('confidence_score', 0):.2f})"
            )
            logger.info(f"  ë³€ê²½ ìœ í˜•: {change_type}")

            if detected:
                logger.info(f"  Legacy: {result.get('legacy_snippet', '')[:100]}...")
                logger.info(f"  New: {result.get('new_snippet', '')[:100]}...")

                numerical = result.get("numerical_changes", [])
                if numerical:
                    logger.info(f"  ìˆ˜ì¹˜ ë³€ê²½: {len(numerical)}ê°œ")
                    for num_change in numerical[:3]:
                        logger.info(
                            f"    - {num_change.get('field')}: {num_change.get('legacy_value')} â†’ {num_change.get('new_value')}"
                        )

        logger.info("\n" + "=" * 80)

        state["change_detection_results"] = detection_results
        state["change_summary"] = {
            "status": "completed",
            "total_reference_blocks": len(matched_pairs),
            "total_changes": total_changes,
            "high_confidence_changes": high_confidence,
            "legacy_regulation_id": legacy_regulation_id,
            "new_regulation_id": new_regulation_id,
        }
        # ğŸ”‘ Section ê¸°ë°˜ ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
        change_index = {}
        for result in detection_results:
            section = self._normalize_section_ref(result.get("section_ref", ""))
            if section and result.get("change_detected"):
                change_index[section] = result
        state["change_detection_index"] = change_index
        logger.info(f"ğŸ“š Change Index ìƒì„±: {len(change_index)}ê°œ ì„¹ì…˜")

        logger.info(
            f"âœ… ë³€ê²½ ê°ì§€ ì™„ë£Œ: {total_changes}ê°œ ë³€ê²½ ê°ì§€ (HIGH: {high_confidence})"
        )

        # ========== Keynote ë°ì´í„° ìƒì„± ==========
        regulation_meta = state.get("regulation", {})
        keynote_data = self._build_keynote_data(
            detection_results=all_detection_results,
            change_summary=state["change_summary"],
            regulation_meta=regulation_meta,
            legacy_regulation_id=legacy_regulation_id,
        )
        state["change_keynote_data"] = keynote_data
        logger.info("ğŸ“ Change Keynote ë°ì´í„° ìƒì„± ì™„ë£Œ (report ë…¸ë“œì—ì„œ ì €ì¥ ì˜ˆì •)")
        logger.info(f"   - ë°ì´í„° í¬ê¸°: {len(str(keynote_data))} bytes")
        logger.info(
            f"   - section_changes: {len(keynote_data.get('section_changes', []))}ê°œ"
        )
        logger.info(f"   - regulation_id: {keynote_data.get('regulation_id')}")

        # ========== ì¤‘ê°„ ê²°ê³¼ë¬¼ ì €ì¥ (HITLìš©) ==========
        from app.core.repositories.intermediate_output_repository import IntermediateOutputRepository
        from app.core.database import AsyncSessionLocal

        async with AsyncSessionLocal() as session:
            intermediate_repo = IntermediateOutputRepository()
            
            # ğŸ†• ì¤‘ê°„ ê²°ê³¼ë¬¼ ì €ì¥ (HITLìš©)
            try:
                intermediate_data = {
                    "change_detection_results": detection_results,
                    "change_summary": state["change_summary"],
                    "change_detection_index": change_index,
                    "regulation_analysis_hints": state.get("regulation_analysis_hints", {})
                }
                await intermediate_repo.save_intermediate(
                    session,
                    regulation_id=new_regulation_id,
                    node_name="change_detection",
                    data=intermediate_data
                )
                await session.commit()
                logger.info(f"âœ… ë³€ê²½ ê°ì§€ ì¤‘ê°„ ê²°ê³¼ë¬¼ ì €ì¥ ì™„ë£Œ: regulation_id={new_regulation_id}")
            except Exception as db_err:
                await session.rollback()
                logger.error(f"âŒ ì¤‘ê°„ ê²°ê³¼ë¬¼ ì €ì¥ ì‹¤íŒ¨: {db_err}")
                import traceback
                traceback.print_exc()

        # ========== ì„ë² ë”© í•„ìš” ì—¬ë¶€ í”Œë˜ê·¸ ==========
        needs_embedding = total_changes > 0
        state["needs_embedding"] = needs_embedding
        logger.info(f"ğŸ“¦ ì„ë² ë”© í•„ìš”: {needs_embedding}")

        # ì‹¤í–‰ ìƒíƒœ ë§ˆí‚¹ (ì •ìƒ ì™„ë£Œ)
        self._mark_execution_state(state)

        return state

    def _extract_reference_blocks(
        self, regul_data: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Reference Block ì¶”ì¶œ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜)."""
        ref_blocks = []
        vision_pages = regul_data.get("vision_extraction_result", [])
        doc_id = regul_data.get("regulation_id") or regul_data.get(
            "regulation", {}
        ).get("regulation_id")

        for page in vision_pages:
            structure = page.get("structure", {})
            page_num = page.get("page_num", 0)
            markdown_content = structure.get("markdown_content", "")
            reference_blocks_meta = structure.get("reference_blocks", [])

            if reference_blocks_meta:
                lines = markdown_content.splitlines()
                for ref in reference_blocks_meta:
                    start = max(0, ref.get("start_line", 0))
                    end = ref.get("end_line", len(lines))
                    if end <= start:
                        end = min(len(lines), start + 20)
                    snippet = "\n".join(lines[start:end]) if lines else markdown_content
                    ref_blocks.append(
                        {
                            "section_ref": ref.get("section_ref", ""),
                            "text": snippet,
                            "keywords": ref.get("keywords")
                            or self._extract_keywords(snippet),
                            "page_num": page_num,
                            "doc_id": doc_id,
                            "meta_doc_id": doc_id,
                        }
                    )
            else:
                ref_blocks.append(
                    {
                        "section_ref": f"Page {page_num}",
                        "text": markdown_content[:500],
                        "keywords": self._extract_keywords(markdown_content),
                        "page_num": page_num,
                        "doc_id": doc_id,
                        "meta_doc_id": doc_id,
                    }
                )

        logger.info(f"Reference Blocks ì¶”ì¶œ: {len(ref_blocks)}ê°œ")
        return ref_blocks

    def _extract_keywords(self, text: str, max_keywords: int = 5) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ í† í° ê¸°ë°˜)."""
        import re

        if not text:
            return []

        # ìˆ«ì í¬í•¨ ë‹¨ì–´ ìš°ì„  (ì˜ˆ: 20mg, Â§ 1141.1)
        numeric_words = re.findall(r"\b\w*\d+\w*\b", text)

        # ëŒ€ë¬¸ì ì‹œì‘ ë‹¨ì–´ (ê³ ìœ ëª…ì‚¬)
        capitalized = re.findall(r"\b[A-Z][a-z]+\b", text)

        # ê²°í•© ë° ì¤‘ë³µ ì œê±°
        keywords = list(dict.fromkeys(numeric_words[:3] + capitalized[:3]))

        return keywords[:max_keywords]

    async def _find_legacy_regulation_db(
        self, regul_data: Dict[str, Any], db_session, exclude_regulation_id: int = None
    ) -> Optional[int]:
        """DBì—ì„œ Legacy ê·œì œ ê²€ìƒ‰ (ê°•í™”ëœ ê²€ìƒ‰ ë¡œì§ + Citation Code ì •ê·œí™” + ê°™ì€ ë‚ ì§œ í•„í„°ë§)."""
        if not regul_data:
            logger.warning("regul_dataê°€ Noneì…ë‹ˆë‹¤")
            return None

        # vision_extraction_resultì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        vision_pages = regul_data.get("vision_extraction_result", [])
        if not vision_pages:
            logger.warning("vision_extraction_resultê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return None

        first_page = vision_pages[0]
        metadata = first_page.get("structure", {}).get("metadata", {})

        title = metadata.get("title", "")
        country = metadata.get("jurisdiction_code", "")
        citation_code = metadata.get("citation_code", "")
        version = metadata.get("version", "")
        effective_date = metadata.get("effective_date", "")

        # Citation Code ì •ê·œí™” (í•˜ì´í”ˆ ì œê±°, ëŒ€ë¬¸ì ë³€í™˜)
        def normalize_citation(code: str) -> str:
            if not code:
                return ""
            return code.upper().replace("-", "").replace(" ", "")

        normalized_citation = normalize_citation(citation_code)

        logger.info(f"DB Legacy ê²€ìƒ‰: title={title}, country={country}")
        # noisy print ì œê±°, loggerë¡œë§Œ ê¸°ë¡

        try:
            from app.core.repositories.regulation_repository import RegulationRepository

            repo = RegulationRepository()

            # ê°™ì€ ë‚ ì§œ í•„í„°ë§ì„ ìœ„í•œ ì‹ ê·œ ê·œì œ created_at ì¡°íšŒ
            exclude_date = None
            if exclude_regulation_id:
                result = await db_session.execute(
                    text(
                        "SELECT DATE(created_at) FROM regulations WHERE regulation_id = :rid"
                    ),
                    {"rid": exclude_regulation_id},
                )
                row = result.fetchone()
                if row:
                    exclude_date = row[0]

            # 1ìˆœìœ„: citation_code + country (ì •ê·œí™”ëœ ì½”ë“œë¡œ ê²€ìƒ‰)
            if normalized_citation and country:
                # ì›ë³¸ citation_codeë¡œ ê²€ìƒ‰ (ê°™ì€ ë‚ ì§œ ì œì™¸)
                if exclude_date:
                    result = await db_session.execute(
                        text(
                            """
                            SELECT regulation_id FROM regulations
                            WHERE citation_code = :citation
                            AND country_code = :country
                            AND DATE(created_at) < :exclude_date
                            AND (:exclude_id IS NULL OR regulation_id != :exclude_id)
                            ORDER BY created_at DESC LIMIT 1
                        """
                        ),
                        {
                            "citation": citation_code,
                            "country": country,
                            "exclude_date": exclude_date,
                            "exclude_id": exclude_regulation_id,
                        },
                    )
                    row = result.fetchone()
                    regulation = await repo.get(db_session, row[0]) if row else None
                else:
                    regulation = await repo.find_by_citation_and_country(
                        db_session, citation_code, country, exclude_regulation_id
                    )

                if regulation:
                    logger.info(
                        f"DB Legacy ë°œê²¬ (citation ì›ë³¸): regulation_id={regulation.regulation_id}"
                    )
                    return regulation.regulation_id

                # ì •ê·œí™”ëœ citation_codeë¡œ ì¬ê²€ìƒ‰ (fallback)
                regulation = await repo.find_by_citation_normalized(
                    db_session, normalized_citation, country, exclude_regulation_id
                )
                if regulation:
                    logger.info(
                        f"DB Legacy ë°œê²¬ (citation ì •ê·œí™”): regulation_id={regulation.regulation_id}"
                    )
                    return regulation.regulation_id

            # 2ìˆœìœ„: title + country + version
            if title and country and version:
                regulation = await repo.find_by_title_country_version(
                    db_session, title, country, version, exclude_regulation_id
                )
                if regulation:
                    logger.info(
                        f"DB Legacy ë°œê²¬ (title+version): regulation_id={regulation.regulation_id}"
                    )
                    return regulation.regulation_id

            # 3ìˆœìœ„: title + country (ê¸°ì¡´ ë¡œì§)
            if title and country:
                regulation = await repo.find_by_title_and_country(
                    db_session, title, country, exclude_regulation_id
                )
                if regulation:
                    logger.info(
                        f"DB Legacy ë°œê²¬ (title): regulation_id={regulation.regulation_id}"
                    )
                    return regulation.regulation_id
            regulation = await repo.find_by_title_and_country(
                db_session, title, country, exclude_regulation_id
            )

            if regulation:
                logger.info(f"DB Legacy ë°œê²¬: regulation_id={regulation.regulation_id}")
                return regulation.regulation_id

            logger.info("DB Legacy ë¯¸ë°œê²¬")
            return None

        except Exception as e:
            logger.error(f"DB Legacy ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def _normalize_section_ref(self, section_ref: str) -> str:
        """ì¡°í•­ ë²ˆí˜¸ ì •ê·œí™” (Â§1160.5, 1160.5, Â§ 1160.5 â†’ 1160.5)."""
        import re

        normalized = re.sub(r"[Â§\s]", "", section_ref)
        match = re.search(r"(\d+\.\d+)", normalized)
        return match.group(1) if match else normalized

    async def _match_reference_blocks(
        self, new_blocks: List[Dict[str, Any]], legacy_blocks: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        LLM ê¸°ë°˜ ëŠ¥ë™ì  ë§¤ì¹­: ì „ì²´ ì»¨í…ìŠ¤íŠ¸ë¥¼ LLMì— ì „ë‹¬í•˜ì—¬ ì˜ë¯¸ì  ë§¤ì¹­ ìˆ˜í–‰.
        """
        logger.info("ğŸ¤– LLM ê¸°ë°˜ ëŠ¥ë™ì  ë§¤ì¹­ ì‹œì‘")

        # ë¸”ë¡ ìš”ì•½ (LLM ì…ë ¥ í¬ê¸° ìµœëŒ€í™” - GPT-4o-mini 128K í† í°)
        def summarize_blocks(blocks: List[Dict[str, Any]], max_blocks: int = 100) -> List[Dict[str, Any]]:
            """ë¸”ë¡ ìš”ì•½ (ìµœëŒ€ 100ê°œ, ê° 2000ì - ë¯¸íƒ ë°©ì§€)"""
            summarized = []
            for idx, block in enumerate(blocks[:max_blocks]):
                summarized.append({
                    "id": f"block_{idx}",
                    "section_ref": block.get("section_ref", f"Page {block.get('page_num')}"),
                    "text_preview": block.get("text", "")[:2000],  # 300 â†’ 2000ì
                    "keywords": block.get("keywords", [])[:10],  # 5 â†’ 10ê°œ
                    "page_num": block.get("page_num")
                })
            return summarized

        new_summary = summarize_blocks(new_blocks)
        legacy_summary = summarize_blocks(legacy_blocks)

        # LLM ë§¤ì¹­ í”„ë¡¬í”„íŠ¸
        prompt = f"""You are a regulatory document comparison expert.

**Task**: Match corresponding blocks between NEW and LEGACY regulations based on semantic similarity.

**NEW Regulation Blocks** ({len(new_summary)} blocks):
{json.dumps(new_summary, indent=2, ensure_ascii=False)}

**LEGACY Regulation Blocks** ({len(legacy_summary)} blocks):
{json.dumps(legacy_summary, indent=2, ensure_ascii=False)}

**Instructions**:
1. Match blocks that discuss the SAME regulatory topic (even if section numbers differ)
2. Consider: keywords, content similarity, regulatory intent
3. Return ONLY matched pairs (skip unmatched blocks)
4. Assign confidence: 1.0 (exact), 0.8 (high), 0.6 (medium), 0.4 (low)

**Output JSON** (array of matches):
[
  {{
    "new_block_id": "block_0",
    "legacy_block_id": "block_3",
    "confidence": 0.9,
    "reason": "Both discuss nicotine concentration limits"
  }}
]

**CRITICAL**: Return ONLY valid JSON array. If no matches, return [].
"""

        try:
            response = await self.llm.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are a regulatory document matcher. Return JSON only."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"}
            )

            content = response.choices[0].message.content
            
            # JSON íŒŒì‹± (ë°°ì—´ ë˜ëŠ” ê°ì²´ ì²˜ë¦¬)
            try:
                parsed = json.loads(content)
                if isinstance(parsed, dict) and "matches" in parsed:
                    matches = parsed["matches"]
                elif isinstance(parsed, list):
                    matches = parsed
                else:
                    matches = []
            except json.JSONDecodeError:
                logger.error(f"LLM ë§¤ì¹­ JSON íŒŒì‹± ì‹¤íŒ¨: {content[:200]}")
                matches = []

            # ë§¤ì¹­ ê²°ê³¼ë¥¼ matched_pairs í˜•íƒœë¡œ ë³€í™˜
            matched_pairs = []
            for match in matches:
                new_id = match.get("new_block_id", "")
                legacy_id = match.get("legacy_block_id", "")
                
                try:
                    new_idx = int(new_id.split("_")[1])
                    legacy_idx = int(legacy_id.split("_")[1])
                    
                    if new_idx < len(new_blocks) and legacy_idx < len(legacy_blocks):
                        matched_pairs.append({
                            "new_block": new_blocks[new_idx],
                            "legacy_block": legacy_blocks[legacy_idx],
                            "match_confidence": match.get("confidence", 0.5),
                            "match_reason": match.get("reason", "LLM semantic match")
                        })
                except (IndexError, ValueError) as e:
                    logger.warning(f"ë§¤ì¹­ ì¸ë±ìŠ¤ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue

            logger.info(f"âœ… LLM ë§¤ì¹­ ì™„ë£Œ: {len(matched_pairs)}ê°œ ìŒ")
            return matched_pairs

        except Exception as e:
            logger.error(f"âŒ LLM ë§¤ì¹­ ì‹¤íŒ¨, Fallback ì‚¬ìš©: {e}")
            return self._fallback_keyword_matching(new_blocks, legacy_blocks)

    def _fallback_keyword_matching(
        self, new_blocks: List[Dict[str, Any]], legacy_blocks: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """LLM ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­."""
        logger.info("ğŸ”„ Fallback: í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­")
        matched_pairs = []
        matched_legacy = set()

        for new_block in new_blocks[:100]:  # 20 â†’ 100
            new_kw = set(new_block.get("keywords", []))
            if not new_kw:
                continue

            best_match = None
            best_score = 0.0

            for idx, legacy_block in enumerate(legacy_blocks[:100]):  # 20 â†’ 100
                if idx in matched_legacy:
                    continue

                legacy_kw = set(legacy_block.get("keywords", []))
                if not legacy_kw:
                    continue

                score = len(new_kw & legacy_kw) / len(new_kw | legacy_kw) if (new_kw | legacy_kw) else 0.0

                if score > best_score and score >= 0.3:
                    best_score = score
                    best_match = (idx, legacy_block)

            if best_match:
                matched_pairs.append({
                    "new_block": new_block,
                    "legacy_block": best_match[1],
                    "match_confidence": best_score,
                    "match_reason": f"Keyword fallback: {best_score:.2f}"
                })
                matched_legacy.add(best_match[0])

        logger.info(f"âœ… Fallback ë§¤ì¹­: {len(matched_pairs)}ê°œ ìŒ")
        return matched_pairs

    async def _detect_change_by_ref_id(
        self, pair: Dict[str, Any], new_regulation_id: str, legacy_regulation_id: str, state: Optional[AppState] = None
    ) -> Optional[Dict[str, Any]]:
        """CoT Step 2-4: Reference ID ê¸°ë°˜ ì •ë°€ ë³€ê²½ ê°ì§€ (Agentic + HITL)."""
        new_block = pair["new_block"]
        legacy_block = pair["legacy_block"]

        section_ref = new_block["section_ref"]
        new_text = new_block["text"]
        legacy_text = legacy_block["text"]
        new_doc_id = new_block.get("doc_id")
        legacy_doc_id = legacy_block.get("doc_id")

        # Reference ID ìƒì„±
        new_ref_id = (
            f"{new_regulation_id}-{section_ref}-P{new_block.get('page_num', 0)}"
        )
        legacy_ref_id = (
            f"{legacy_regulation_id}-{section_ref}-P{legacy_block.get('page_num', 0)}"
        )

        # ğŸ†• HITL: DBì—ì„œ ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ + ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸ ì¡°í•©
        hitl_context = ""
        if state and state.get("refined_change_detection_prompt"):
            # DBì—ì„œ ê¸°ì¡´ ë³€ê²½ ê°ì§€ ê²°ê³¼ ë¡œë“œ
            from app.core.repositories.intermediate_output_repository import IntermediateOutputRepository
            from app.core.database import AsyncSessionLocal
            
            regulation_id = state.get("regulation", {}).get("regulation_id")
            if regulation_id:
                try:
                    async with AsyncSessionLocal() as session:
                        intermediate_repo = IntermediateOutputRepository()
                        existing_data = await intermediate_repo.get_intermediate(
                            session, regulation_id, "change_detection"
                        )
                        
                        if existing_data and existing_data.get("change_detection_results"):
                            # í•´ë‹¹ ì„¹ì…˜ì˜ ê¸°ì¡´ ê²°ê³¼ ì°¾ê¸°
                            existing_results = existing_data["change_detection_results"]
                            section_result = next(
                                (r for r in existing_results if r.get("section_ref") == section_ref),
                                None
                            )
                            
                            if section_result:
                                hitl_context = f"""\n\n[EXISTING ANALYSIS - For Reference]
Previous Detection: {section_result.get('change_detected')}
Previous Confidence: {section_result.get('confidence_score')}
Previous Type: {section_result.get('change_type')}
Previous Reasoning: {section_result.get('reasoning', {})}

[EXPERT GUIDANCE]
{state['refined_change_detection_prompt']}

**CRITICAL**: Re-evaluate based on expert guidance above.
"""
                                logger.info(f"âœ… HITL: Section {section_ref} - ê¸°ì¡´ ê²°ê³¼ + ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸ ì ìš©")
                except Exception as e:
                    logger.warning(f"âš ï¸ HITL ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # LLM í˜¸ì¶œ (ref_id ê¸°ë°˜ ì •ë°€ ë¹„êµ + HITL)
        try:
            prompt = f"""Perform PRECISE comparison using Reference IDs for context-aware analysis.

**Reference IDs:**
- Legacy: {legacy_ref_id}
- New: {new_ref_id}

**Legacy Regulation (Section {section_ref}):**
{legacy_text[:3000]}

**New Regulation (Section {section_ref}):**
{new_text[:3000]}{hitl_context}

**Task**: 
1. Use Reference IDs to understand document context and hierarchy
2. Detect ALL substantive changes (numerical, wording, scope)
3. Follow Chain of Thought (4 steps)
4. Apply Adversarial Validation
5. Extract numerical changes with full context

**CRITICAL**: Return valid JSON only. If unsure, set change_detected=false.
"""

            # GPT-5: Chat Completions API (SDK ë²„ì „ í˜¸í™˜ì„±)
            response = await self.llm.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": CHANGE_DETECTION_SYSTEM_PROMPT},
                    {"role": "user", "content": prompt},
                ],
                response_format={"type": "json_object"},
            )

            # ìœ ì—°í•œ JSON íŒŒì‹± (íŒŒì‹± ì‹¤íŒ¨ ì‹œ fallback)
            content = response.choices[0].message.content
            try:
                result = json.loads(content)
            except json.JSONDecodeError as parse_err:
                logger.warning(
                    f"JSON íŒŒì‹± ì‹¤íŒ¨ (Section {section_ref}), fallback ì‚¬ìš©: {parse_err}"
                )
                logger.debug(f"ì›ë³¸ ì‘ë‹µ: {content[:200]}...")
                result = {
                    "change_detected": False,
                    "confidence_score": 0.0,
                    "change_type": "parse_error",
                    "reasoning": {
                        "error": "LLM JSON íŒŒì‹± ì‹¤íŒ¨",
                        "raw_response": content[:500],
                    },
                    "numerical_changes": [],
                    "keywords": [],
                }
            result["section_ref"] = section_ref
            result["new_ref_id"] = new_ref_id
            result["legacy_ref_id"] = legacy_ref_id
            result["doc_id"] = new_doc_id
            result["meta_doc_id"] = new_doc_id
            result.setdefault("new_snippet", new_text[:1000])
            result.setdefault("legacy_snippet", legacy_text[:1000])

            # í‚¤ì›Œë“œ/í•„ë“œ ë³´ê°• (ê²€ìƒ‰/ë§¤í•‘ íŒíŠ¸ìš©)
            kw: Set[str] = set(result.get("keywords") or [])
            kw |= set(self._extract_keywords(new_text, max_keywords=5))
            kw |= set(self._extract_keywords(legacy_text, max_keywords=5))
            for num_change in result.get("numerical_changes", []) or []:
                field = num_change.get("field")
                if field:
                    kw.add(str(field))
            if kw:
                result["keywords"] = list(kw)

            return result

        except Exception as e:
            logger.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨ (Section {section_ref}): {e}")
            return {
                "section_ref": section_ref,
                "new_ref_id": new_ref_id,
                "legacy_ref_id": legacy_ref_id,
                "change_detected": False,
                "confidence_score": 0.0,
                "confidence_level": "UNCERTAIN",
                "change_type": "llm_error",
                "reasoning": {"error": str(e)},
                "numerical_changes": [],
                "keywords": [],
                "new_snippet": new_text[:500],
                "legacy_snippet": legacy_text[:500],
            }

    def _mark_execution_state(self, state: AppState) -> None:
        """ì‹¤í–‰ ìƒíƒœ ë§ˆí‚¹ (ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€)."""
        state["change_detection_ran_inline"] = True

    async def _analyze_new_regulation(
        self, regul_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì‹ ê·œ ê·œì œ ë¶„ì„ (Legacy ì—†ì„ ë•Œ LLMìœ¼ë¡œ í•µì‹¬ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ)."""
        vision_pages = regul_data.get("vision_extraction_result", [])
        if not vision_pages:
            return {
                "regulation_summary": "",
                "key_requirements": [],
                "affected_areas": [],
            }

        # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìµœëŒ€ 5000ì)
        full_text = ""
        for page in vision_pages[:10]:  # ìµœëŒ€ 10í˜ì´ì§€
            markdown = page.get("structure", {}).get("markdown_content", "")
            full_text += markdown + "\n\n"

        full_text = full_text[:5000]

        user_prompt = f"""**Regulation Text:**
{full_text}
"""

        try:
            # GPT-5: Chat Completions API (SDK ë²„ì „ í˜¸í™˜ì„±)
            response = await self.llm.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": NEW_REGULATION_ANALYSIS_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={"type": "json_object"},
            )
            result = json.loads(response.choices[0].message.content)
            return result

        except Exception as e:
            logger.error(f"ì‹ ê·œ ê·œì œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "regulation_summary": "",
                "key_requirements": [],
                "affected_areas": [],
            }


# ==================== ë…¸ë“œ í•¨ìˆ˜ ====================
_default_node: Optional[ChangeDetectionNode] = None


async def change_detection_node(
    state: AppState, config: Dict[str, Any] = None
) -> AppState:
    """LangGraph ë…¸ë“œ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ (ë‚´ë¶€ì—ì„œ ì§§ì€ ì„¸ì…˜ ìƒì„±)."""
    global _default_node
    if _default_node is None:
        _default_node = ChangeDetectionNode()

    # ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: ì´ë¯¸ ê²°ê³¼ê°€ ìˆê³  ê°•ì œ ì¬ì‹¤í–‰ì´ ì•„ë‹Œ ê²½ìš° skip
    if (
        state.get("change_detection_ran_inline")
        or state.get("change_detection_results")
    ) and not state.get("force_rerun_change_detection"):
        logger.info("change_detection ì´ë¯¸ ì‹¤í–‰ë¨. ì¬ì‹¤í–‰ ê±´ë„ˆëœ€.")
        return state

    return await _default_node.run(state, db_session=None)


__all__ = ["ChangeDetectionNode", "change_detection_node", "ConfidenceScorer"]
