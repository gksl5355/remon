"""
module: map_products.py
description: ê²€ìƒ‰ TOOL + LLM ë§¤í•‘ Node
author: AI Agent
created: 2025-01-18
updated: 2025-12-09
dependencies:
    - openai
    - app.ai_pipeline.tools.retrieval_tool
    - app.core.repositories.product_repository
"""

import asyncio
import json
import logging
from decimal import Decimal
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple
from collections import defaultdict

try:  # pragma: no cover - import guard
    from openai import AsyncOpenAI
except Exception:  # pragma: no cover
    AsyncOpenAI = None  # type: ignore[assignment]

from app.ai_pipeline.state import (
    ProductInfo,
    RetrievedChunk,
    RetrievalResult,
    MappingItem,
    MappingParsed,
    MappingResults,
    AppState,
    MappingContext,
)

from app.ai_pipeline.prompts.mapping_prompt import MAPPING_PROMPT
from app.ai_pipeline.tools.retrieval_utils import build_product_filters
from app.ai_pipeline.tools.retrieval_tool import (
    RetrievalOutput,
    get_retrieval_tool,
)
from app.config.settings import settings
from app.core.database import AsyncSessionLocal

# ì¶”ê°€: Repository import
from app.core.repositories.product_repository import ProductRepository


logger = logging.getLogger(__name__)

HIGH_CONF_THRESHOLD = 0.7
LOW_CONF_THRESHOLD = 0.5


class MappingNode:
    """ê²€ìƒ‰ + ë§¤í•‘ í†µí•© Node."""

    def __init__(
        self,
        llm_client,
        search_tool,
        top_k: int = 10,
        alpha: float = 0.7,
        product_repository: Optional[ProductRepository] = None,
        max_candidates_per_doc: int = 2,
    ):
        self.llm = llm_client
        self.search_tool = search_tool or get_retrieval_tool()
        self.top_k = top_k
        self.alpha = alpha
        self.product_repository = product_repository or ProductRepository()
        self.debug_enabled = settings.MAPPING_DEBUG_ENABLED
        self.max_candidates_per_doc = max_candidates_per_doc
        self._llm_semaphore = None

    # ----------------------------------------------------------------------
    # change detection ì—°ê³„ ìœ í‹¸
    # ----------------------------------------------------------------------
    def _normalize_token(self, value: str) -> str:
        return value.lower().replace(" ", "_").replace("-", "_").replace(".", "_")

    def _extract_change_scope(
        self,
        change_results: List[Dict[str, Any]],
        present_features: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        ë³€ê²½ ê°ì§€ ê²°ê³¼ì—ì„œ ê²€ìƒ‰/ë§¤í•‘ì— ì“¸ íŒíŠ¸ë¥¼ ì¶”ì¶œí•œë‹¤.
        """
        if not change_results:
            return {
                "actionable_results": [],
                "pending_results": [],
                "doc_filters": set(),
                "chunk_filters": set(),
                "feature_hints": set(),
                "raw_results": [],
            }

        feature_key_map = {
            self._normalize_token(name): name for name in present_features.keys()
        }
        feature_keys = set(feature_key_map.keys())
        doc_filters: Set[str] = set()
        chunk_filters: Set[str] = set()
        feature_hints: Set[str] = set()
        actionable: List[Dict[str, Any]] = []
        pending: List[Dict[str, Any]] = []

        for result in change_results:
            status = result.get("status")
            change_detected = result.get("change_detected")
            positive_status = (status or "").lower() in (
                "changed",
                "updated",
                "new",
                "modified",
                "added",
            )
            confidence = (
                result.get("confidence_score")
                or result.get("score")
                or result.get("confidence")
                or 0.0
            )

            # ì‹ ë¢°ë„/ìƒíƒœì— ë”°ë¥¸ ë¶„ë¥˜
            is_inconclusive = (status or "").lower() == "inconclusive"
            if change_detected or positive_status:
                if confidence >= HIGH_CONF_THRESHOLD:
                    actionable.append(result)
                elif confidence >= LOW_CONF_THRESHOLD:
                    pending.append(result)
                else:
                    continue
            elif is_inconclusive:
                pending.append(result)
            else:
                # ë„ˆë¬´ ë‚®ì€ ì‹ ë¢°ë„ëŠ” ìŠ¤í‚µ
                continue

            # ë¬¸ì„œ/ì²­í¬ ì‹ë³„ì ìˆ˜ì§‘ (ê²€ìƒ‰ í•„í„°)
            for key in (
                "doc_id",
                "regulation_id",
                "new_regulation_id",
                "legacy_regulation_id",
                "meta_doc_id",
            ):
                val = result.get(key)
                if val:
                    doc_filters.add(str(val))

            meta = result.get("metadata") or {}
            for key in ("doc_id", "meta_doc_id"):
                val = meta.get(key)
                if val:
                    doc_filters.add(str(val))

            for key in (
                "chunk_id",
                "new_chunk_id",
                "legacy_chunk_id",
                "new_ref_id",
                "legacy_ref_id",
            ):
                val = result.get(key)
                if val:
                    chunk_filters.add(str(val))

            # feature íŒíŠ¸: ëª…ì‹œì  feature í•„ë“œ ë˜ëŠ” keywordsì™€ ì´ë¦„ ë§¤ì¹­
            for key in ("feature", "feature_name", "feature_names"):
                val = result.get(key)
                if isinstance(val, str):
                    normalized = self._normalize_token(val)
                    if normalized in feature_keys:
                        feature_hints.add(feature_key_map[normalized])
                elif isinstance(val, list):
                    for item in val:
                        if not isinstance(item, str):
                            continue
                        normalized = self._normalize_token(item)
                        if normalized in feature_keys:
                            feature_hints.add(feature_key_map[normalized])

            for kw in result.get("keywords", []) or []:
                if not isinstance(kw, str):
                    continue
                normalized_kw = self._normalize_token(kw)
                for norm_name, raw_name in feature_key_map.items():
                    # ì™„ì „ ì¼ì¹˜ ìš°ì„ , ë¶€ë¶„ ì¼ì¹˜ëŠ” ë³´ì¡°
                    if normalized_kw == norm_name:
                        feature_hints.add(raw_name)
                    elif normalized_kw in norm_name or norm_name in normalized_kw:
                        feature_hints.add(raw_name)

        return {
            "actionable_results": actionable,
            "pending_results": pending,
            "doc_filters": doc_filters,
            "chunk_filters": chunk_filters,
            "feature_hints": feature_hints,
            "raw_results": change_results,
        }

    def _build_change_filters(
        self, change_scope: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        filters: Dict[str, Any] = {}
        doc_filters = change_scope.get("doc_filters") or set()
        chunk_filters = change_scope.get("chunk_filters") or set()
        if doc_filters:
            filters["meta_doc_id"] = list(doc_filters)
        if chunk_filters:
            filters["chunk_id"] = list(chunk_filters)
        return filters or None

    def _select_features_for_mapping(
        self,
        present_features: Dict[str, Any],
        change_scope: Dict[str, Any],
        recovered_hints: Optional[Set[str]] = None,
    ) -> Tuple[List[Tuple[str, Any]], List[str]]:
        """
        ë³€ê²½ íŒíŠ¸/ë³µêµ¬ íŒíŠ¸ê°€ ìˆìœ¼ë©´ í•´ë‹¹ featureë§Œ ì„ íƒ.
        íŒíŠ¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ feature ë§¤í•‘ (Legacy ìˆì§€ë§Œ ë³€ê²½ ì—†ëŠ” ê²½ìš° ëŒ€ì‘).
        """
        unknown: List[str] = []
        if not present_features:
            return [], unknown

        hints: Set[str] = set(change_scope.get("feature_hints") or set())
        if recovered_hints:
            hints |= recovered_hints

        if hints:
            # íŒíŠ¸ê°€ ìˆìœ¼ë©´ í•´ë‹¹ featureë§Œ ì„ íƒ
            filtered = [
                (name, value)
                for name, value in present_features.items()
                if name in hints
            ]
            unknown = [hint for hint in hints if hint not in present_features]
            logger.info(f"ğŸ¯ íŒíŠ¸ ê¸°ë°˜ ë§¤í•‘: {len(filtered)}ê°œ feature ì„ íƒ")
            return filtered, unknown

        # íŒíŠ¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ feature ë§¤í•‘ (Legacy ìˆì§€ë§Œ ë³€ê²½ ì—†ëŠ” ê²½ìš°)
        all_features = [
            (name, value)
            for name, value in present_features.items()
            if name != "feature_units"  # feature_unitsëŠ” ì œì™¸
        ]
        logger.info(f"ğŸ” ì „ì²´ feature ë§¤í•‘: {len(all_features)}ê°œ feature")
        return all_features, unknown

    async def _classify_change_requirement(
        self,
        change_hint: Dict[str, Any],
        present_features: Dict[str, Any],
        sem,
    ) -> Dict[str, Any]:
        """
        change_detection ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
        - existing_feature: ìš°ë¦¬ ìŠ¤í™ì— ìˆìŒ â†’ matched_feature ë°˜í™˜
        - new_requirement: ì‹ ê·œ ìš”êµ¬ â†’ ì•Œë¦¼ìš© ê¸°ë¡
        - ambiguous: ë¶ˆí™•ì‹¤ â†’ ì•Œë¦¼ìš© ê¸°ë¡
        """
        features_list = [
            {
                "name": name,
                "unit": present_features.get("feature_units", {}).get(name),
                "value": val,
            }
            for name, val in present_features.items()
            if name != "feature_units"
        ]
        prompt = {
            "task": "classify_change_requirement",
            "change_hint": {
                "change_type": change_hint.get("change_type"),
                "keywords": change_hint.get("keywords", []),
                "numerical_changes": change_hint.get("numerical_changes", []),
                "new_snippet": change_hint.get("new_snippet")
                or change_hint.get("new_text"),
                "legacy_snippet": change_hint.get("legacy_snippet")
                or change_hint.get("legacy_text"),
                "section_ref": change_hint.get("section_ref"),
            },
            "product_features": features_list,
            "instructions": (
                "Given the change hint and product feature list, decide whether it matches an existing feature."
                " If not, mark as new_requirement. If unsure, mark ambiguous.\n"
                "Output JSON only: "
                '{"match_status": "existing_feature"|"new_requirement"|"ambiguous", '
                '"matched_feature": "name or null", '
                '"reason": "string", '
                '"suggested_hint": "string or null"}'
            ),
        }
        async with sem:
            try:
                res = await self.llm.chat.completions.create(
                    model="gpt-5-nano",
                    messages=[
                        {
                            "role": "user",
                            "content": json.dumps(prompt, ensure_ascii=False),
                        }
                    ],
                )
                return json.loads(res.choices[0].message.content)
            except Exception:
                return {
                    "match_status": "ambiguous",
                    "matched_feature": None,
                    "reason": "llm_error",
                    "suggested_hint": None,
                }

    def _candidate_matches_change(
        self,
        change_result: Dict[str, Any],
        doc_id: Optional[str],
        chunk_id: Optional[str],
    ) -> bool:
        doc_ids = {
            str(v)
            for v in (
                change_result.get("doc_id"),
                change_result.get("regulation_id"),
                change_result.get("new_regulation_id"),
                change_result.get("legacy_regulation_id"),
                change_result.get("meta_doc_id"),
            )
            if v is not None
        }
        meta = change_result.get("metadata") or {}
        for key in ("doc_id", "meta_doc_id"):
            val = meta.get(key)
            if val:
                doc_ids.add(str(val))

        chunk_ids = {
            str(v)
            for v in (
                change_result.get("chunk_id"),
                change_result.get("new_chunk_id"),
                change_result.get("legacy_chunk_id"),
                change_result.get("new_ref_id"),
                change_result.get("legacy_ref_id"),
            )
            if v is not None
        }

        if chunk_id and chunk_id in chunk_ids:
            return True
        if doc_id and doc_id in doc_ids:
            return True
        return False

    def _match_change_results_to_candidate(
        self,
        change_scope: Dict[str, Any],
        candidate: RetrievedChunk,
    ) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ëœ ì²­í¬ì™€ ì—°ê´€ëœ ë³€ê²½ ê°ì§€ ê²°ê³¼ë¥¼ ì°¾ì•„ regulation_metaì— ë‹´ëŠ”ë‹¤."""
        matches: List[Dict[str, Any]] = []
        meta = candidate.get("metadata") or {}
        doc_id = meta.get("meta_doc_id") or meta.get("doc_id")
        chunk_id = candidate.get("chunk_id")
        for result in (change_scope.get("actionable_results") or []) + (
            change_scope.get("pending_results") or []
        ):
            if self._candidate_matches_change(result, doc_id, chunk_id):
                matches.append(result)
        return matches

    def _build_regulation_filters(
        self, regulation: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """state.regulation ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ í•„í„°."""
        if not regulation:
            return {}
        filters: Dict[str, Any] = {}
        for key in (
            "country",
            "citation_code",
            "effective_date",
            "title",
            "regulation_id",
        ):
            val = regulation.get(key)
            if val:
                filters[key] = val
        return filters

    def _merge_filters(
        self, *filters: Optional[Dict[str, Any]]
    ) -> Optional[Dict[str, Any]]:
        merged: Dict[str, Any] = {}
        for src in filters:
            if src:
                merged.update(src)
        return merged or None

    def _build_change_query(
        self, change_hint: Optional[Dict[str, Any]]
    ) -> Optional[str]:
        """ë³€ê²½ ê°ì§€ íŒíŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ (ê°„ê²°í•œ ì¿¼ë¦¬)."""
        if not change_hint:
            return None
        parts: List[str] = []
        # í‚¤ì›Œë“œ ìš°ì„  (ê°€ì¥ í•µì‹¬ì )
        for kw in change_hint.get("keywords", []) or []:
            if isinstance(kw, str) and kw.strip():
                parts.append(kw.strip())
        # ìˆ˜ì¹˜ ë³€ê²½ ì •ë³´ ì¶”ê°€
        for num_change in change_hint.get("numerical_changes", []) or []:
            for key in ("new_value", "field"):
                val = num_change.get(key)
                if isinstance(val, str) and val.strip():
                    parts.append(val.strip())
        # ìµœëŒ€ 5ê°œ í† í°ìœ¼ë¡œ ì œí•œ (ê³¼ë„í•œ ì¿¼ë¦¬ ë°©ì§€)
        return " ".join(parts[:5]) if parts else None

    def _merge_candidate_lists(
        self,
        base: List[RetrievedChunk],
        extra: List[RetrievedChunk],
    ) -> List[RetrievedChunk]:
        """
        chunk_id ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©í•˜ë©° ë” ë†’ì€ semantic_scoreë¥¼ ìœ ì§€í•œë‹¤.
        """
        merged: Dict[str, RetrievedChunk] = {}
        for cand in base + extra:
            cid = cand.get("chunk_id")
            if cid in merged:
                if (cand.get("semantic_score") or 0) > (
                    merged[cid].get("semantic_score") or 0
                ):
                    merged[cid] = cand
            else:
                merged[cid] = cand
        return list(merged.values())

    def _choose_change_hint(
        self, change_scope: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        actionable = change_scope.get("actionable_results") or []
        pending = change_scope.get("pending_results") or []
        if actionable:
            return actionable[0]
        if pending:
            return pending[0]
        return None

    def _build_trace_entries(
        self,
        mapping_results: List[MappingItem],
        regulation_meta: Dict[str, Any],
        regulation_cache: Dict[str, Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        now_ts = datetime.utcnow().isoformat() + "Z"
        entries: List[Dict[str, Any]] = []
        for item in mapping_results:
            if not item.get("applies"):
                continue
            
            chunk_id = item.get("regulation_chunk_id")
            cache_data = regulation_cache.get(chunk_id, {})
            confidence = cache_data.get("confidence_score")
            change_status = "pending" if (confidence is not None and confidence < 0.7) else "applied"
            
            entries.append(
                {
                    "feature": item.get("feature_name"),
                    "applied_value": item.get("required_value"),
                    "regulation_record_id": chunk_id,
                    "mapping_score": cache_data.get("confidence_score"),
                    "change_status": change_status,
                    "evidence": {
                        "legacy_snippet": None,
                        "new_snippet": item.get("regulation_summary"),
                    },
                    "regulation_info": {
                        "country": regulation_meta.get("country"),
                        "citation_code": regulation_meta.get("citation_code"),
                        "title": regulation_meta.get("title"),
                        "effective_date": regulation_meta.get("effective_date"),
                        "regulation_id": regulation_meta.get("regulation_id"),
                    },
                    "updated_at": now_ts,
                }
            )
        return entries

    def _rule_rank_candidates(
        self,
        candidates: List[RetrievedChunk],
        change_hint: Optional[Dict[str, Any]],
        top_n: int = 3,
    ) -> List[RetrievedChunk]:
        """
        ê·œì¹™ ê¸°ë°˜ ìŠ¤ì½”ì–´ë¡œ ìƒìœ„ í›„ë³´ ì¶”ë¦¼.
        - semantic_score ìš°ì„ 
        - change keywords, numerical_change í…ìŠ¤íŠ¸ ë§¤ì¹­ì— ê°€ì 
        """
        if not candidates:
            return []

        keywords = set()
        numbers = set()
        if change_hint:
            for kw in change_hint.get("keywords", []) or []:
                if isinstance(kw, str):
                    keywords.add(self._normalize_token(kw))
            for num_entry in change_hint.get("numerical_changes", []) or []:
                for key in ("legacy_value", "new_value"):
                    val = num_entry.get(key)
                    if isinstance(val, str):
                        numbers.add(val.lower())

        def score(cand: RetrievedChunk) -> float:
            base = cand.get("semantic_score") or 0.0
            text = (cand.get("chunk_text") or "").lower()
            bonus = 0.0
            for kw in keywords:
                if kw in text:
                    bonus += 0.05
            for num in numbers:
                if num and num in text:
                    bonus += 0.05
            return base + bonus

        ranked = sorted(candidates, key=score, reverse=True)
        return ranked[:top_n]

    def _build_rerank_prompt(
        self,
        change_hint: Dict[str, Any],
        candidates: List[RetrievedChunk],
    ) -> str:
        """
        rerank + ë³€ê²½ ìš”ì•½ + ìš”êµ¬ì‚¬í•­ ì¶”ì¶œì„ í•œ ë²ˆì— ìˆ˜í–‰í•˜ë„ë¡ LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±.
        """
        evidence = {
            "change_type": change_hint.get("change_type"),
            "confidence_score": change_hint.get("confidence_score"),
            "new_snippet": change_hint.get("new_snippet")
            or change_hint.get("new_text")
            or change_hint.get("new_ref_text"),
            "legacy_snippet": change_hint.get("legacy_snippet")
            or change_hint.get("legacy_text")
            or change_hint.get("legacy_ref_text"),
            "keywords": change_hint.get("keywords", []),
            "numerical_changes": change_hint.get("numerical_changes", []),
        }
        cand_payload = []
        for idx, cand in enumerate(candidates):
            cand_payload.append(
                {
                    "id": cand.get("chunk_id"),
                    "text": cand.get("chunk_text"),
                    "metadata": cand.get("metadata", {}),
                    "semantic_score": cand.get("semantic_score"),
                }
            )

        prompt = {
            "task": "select_best_point_and_summarize_change",
            "change_evidence": evidence,
            "candidates": cand_payload,
            "instructions": (
                "1) í›„ë³´ ì¤‘ ë³€í™”ì™€ ê°€ì¥ ì˜ ë§ëŠ” point_idë¥¼ 1ê°œ ì„ íƒ.\n"
                "2) ë¬´ì—‡ì´ ì–´ë–»ê²Œ ë°”ë€Œì—ˆëŠ”ì§€ í•œ ì¤„ë¡œ ìš”ì•½.\n"
                "3) ì¡°í•­ ë‚´ ìš”êµ¬ì‚¬í•­ì„ bulletë¡œ ë‚˜ì—´.\n"
                "4) ìµœì¢… ì‹ ë¢°ë„ 0~1 ì‚°ì¶œ. 0.7 ë¯¸ë§Œì´ë©´ pending=true."
            ),
            "output_schema": {
                "selected_point_id": "string",
                "reason": "string",
                "change_summary": "string",
                "requirements": ["string"],
                "final_confidence": "float",
                "pending": "boolean",
            },
        }
        return json.dumps(prompt, ensure_ascii=False)

    async def _rerank_candidates(
        self,
        change_hint: Dict[str, Any],
        candidates: List[RetrievedChunk],
    ) -> Optional[Dict[str, Any]]:
        if not candidates:
            return None
        prompt = self._build_rerank_prompt(change_hint, candidates)
        try:
            res = await self.llm.chat.completions.create(
                model="gpt-5-nano",
                messages=[{"role": "user", "content": prompt}],
            )
            return json.loads(res.choices[0].message.content)
        except Exception:
            return None

    async def _run_search(
        self,
        product: ProductInfo,
        feature_name: str,
        feature_value: Any,
        feature_unit: str | None,
        extra_filters: Optional[Dict[str, Any]] = None,
        change_query: Optional[str] = None,
    ) -> RetrievalResult:
        import asyncio

        product_id = product["product_id"]
        base_query = self._build_search_query(feature_name, feature_value, feature_unit)

        # ê°œì„ : Change queryë¥¼ ë³„ë„ ê²€ìƒ‰í•˜ì§€ ì•Šê³  ê²°í•© (1íšŒ ê²€ìƒ‰)
        if change_query:
            combined_query = f"{base_query} {change_query}"
        else:
            combined_query = base_query

        filters = build_product_filters(product)
        if extra_filters:
            filters.update(extra_filters)

        async def _search_once(q: str) -> RetrievalOutput:
            return await self.search_tool.search(
                query=q,
                strategy="hybrid",
                top_k=self.top_k,
                alpha=self.alpha,
                filters=filters or None,
            )

        async def _search_with_retry(q: str) -> Optional[RetrievalOutput]:
            for attempt in range(3):
                try:
                    return await _search_once(q)
                except Exception as exc:
                    if attempt < 2:
                        backoff = 0.5 * (attempt + 1)
                        logger.warning(
                            "retrieval tool ì‹¤íŒ¨ retry=%d query=%s err=%s",
                            attempt + 1,
                            q,
                            exc,
                        )
                        await asyncio.sleep(backoff)
                    else:
                        logger.warning(
                            "retrieval tool ìµœì¢… ì‹¤íŒ¨ query=%s err=%s", q, exc
                        )
                        return None
            return None

        # run the combined query (base + change hint) once; retry on transient failures
        tool_result = await _search_with_retry(combined_query)

        if tool_result is None:
            return RetrievalResult(
                product_id=product_id,
                feature_name=feature_name,
                feature_value=feature_value,
                feature_unit=feature_unit,
                candidates=[],
            )

        def _convert(out: RetrievalOutput) -> List[RetrievedChunk]:
            converted: List[RetrievedChunk] = []
            for item in out["results"]:
                converted.append(
                    RetrievedChunk(
                        chunk_id=item.get("id", ""),
                        chunk_text=item.get("text", ""),
                        semantic_score=item.get("scores", {}).get("final_score", 0.0),
                        metadata=item.get("metadata", {}),
                    )
                )
            return converted

        candidates = _convert(tool_result)

        return RetrievalResult(
            product_id=product_id,
            feature_name=feature_name,
            feature_value=feature_value,
            feature_unit=feature_unit,
            candidates=candidates,
        )

    def _extract_section_number(
        self,
        chunk_metadata: Dict[str, Any],
        change_evidence: Optional[Dict[str, Any]],
        chunk_text: str
    ) -> str:
        """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì¡°í•­ ë²ˆí˜¸ ì¶”ì¶œ."""
        # 1ìˆœìœ„: Change Detectionì˜ section_ref
        if change_evidence and change_evidence.get("section_ref"):
            return change_evidence["section_ref"]
        
        # 2ìˆœìœ„: Qdrant metadata
        section = (
            chunk_metadata.get("section_label") or
            chunk_metadata.get("section_ref") or
            (chunk_metadata.get("hierarchy", [])[-1] if chunk_metadata.get("hierarchy") else None)
        )
        if section:
            return section
        
        # 3ìˆœìœ„: chunk_textì—ì„œ regex ì¶”ì¶œ
        import re
        patterns = [
            r'Â§\s*(\d+(?:\.\d+)?(?:\([a-z]\))?)',
            r'Section\s+(\d+(?:\.\d+)?)',
            r'Article\s+(\d+)',
        ]
        for pattern in patterns:
            match = re.search(pattern, chunk_text, re.IGNORECASE)
            if match:
                return f"Â§{match.group(1)}"
        
        # 4ìˆœìœ„: citation_code í™œìš©
        citation = chunk_metadata.get("citation_code")
        if citation:
            return f"{citation}"
        
        return "Â§Unknown"

    def _extract_section_from_chunk(
        self, chunk_metadata: Dict[str, Any], chunk_text: str
    ) -> Optional[str]:
        """ì²­í¬ì—ì„œ ì¡°í•­ ë²ˆí˜¸ ì¶”ì¶œ ë° ì •ê·œí™”."""
        import re
        
        # 1ìˆœìœ„: metadataì—ì„œ ì¶”ì¶œ
        section = (
            chunk_metadata.get("section_label") or
            chunk_metadata.get("section_ref") or
            (chunk_metadata.get("hierarchy", [])[-1] if chunk_metadata.get("hierarchy") else None)
        )
        
        if section:
            # ì •ê·œí™” (Â§1160.5 â†’ 1160.5)
            normalized = re.sub(r'[Â§\s]', '', section)
            match = re.search(r'(\d+\.\d+)', normalized)
            return match.group(1) if match else None
        
        # 2ìˆœìœ„: chunk_textì—ì„œ regex ì¶”ì¶œ
        patterns = [
            r'Â§\s*(\d+\.\d+)',
            r'Section\s+(\d+\.\d+)',
        ]
        for pattern in patterns:
            match = re.search(pattern, chunk_text, re.IGNORECASE)
            if match:
                return match.group(1)
        
        return None

    def _build_prompt(
        self,
        feature_name,
        present_value,
        target_value,
        feature_unit,
        chunk_text,
        chunk_metadata: Optional[Dict[str, Any]] = None,
        change_evidence: Optional[Dict[str, Any]] = None,
        change_context: Optional[Dict[str, Any]] = None,
    ):
        feature = {
            "name": feature_name,
            "present_value": present_value,
            "target_value": target_value,
            "unit": feature_unit,
        }
        feature_json = json.dumps(feature, ensure_ascii=False)

        # ì¡°í•­ ë²ˆí˜¸ ì¶”ì¶œ
        section_info = ""
        if chunk_metadata:
            section = self._extract_section_number(
                chunk_metadata, change_evidence, chunk_text
            )
            citation = chunk_metadata.get("citation_code")
            
            if section or citation:
                section_info = "\n[REGULATION METADATA]\n"
                if citation:
                    section_info += f"Citation: {citation}\n"
                if section:
                    section_info += f"Section: {section}\n"

        # ë³€ê²½ ê°ì§€ ì¦ê±° í¬ë§·íŒ…
        if change_evidence:
            evidence_text = f"""\n[CHANGE EVIDENCE]
Change Type: {change_evidence.get('change_type', 'N/A')}
Confidence: {change_evidence.get('confidence_score', 0)}
Keywords: {', '.join(change_evidence.get('keywords', []))}
Reasoning: {change_evidence.get('reasoning', {}).get('step4_final_judgment', 'N/A')}
"""
        else:
            evidence_text = ""
        
        # ğŸ¯ Change Context ì£¼ì… (ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ë³´ì •)
        if change_context:
            evidence_text += f"""\n[KNOWN CHANGE - Direct from Change Detection]
Section: {change_context.get('section_ref', 'N/A')}
Legacy Text: {change_context.get('legacy_snippet', '')[:200]}
New Text: {change_context.get('new_snippet', '')[:200]}
Numerical Changes: {change_context.get('numerical_changes', [])}
"""

        return (
            MAPPING_PROMPT.replace("{feature}", feature_json)
            .replace("{chunk}", chunk_text)
            .replace("{metadata}", section_info)
            .replace("{change_evidence}", evidence_text)
        )

    def _build_search_query(self, feature_name, feature_value, feature_unit):
        parts: List[str] = [str(feature_name)]
        if feature_value is not None:
            parts.append(str(feature_value))
        if feature_unit:
            parts.append(feature_unit)

        return " ".join(parts)

    def _prune_candidates(
        self, candidates: List[RetrievedChunk]
    ) -> List[RetrievedChunk]:
        seen_chunks = set()
        doc_counts = defaultdict(int)
        pruned: List[RetrievedChunk] = []

        for cand in candidates:
            chunk_id = cand.get("chunk_id")
            if chunk_id in seen_chunks:
                continue
            meta = cand.get("metadata", {}) or {}
            doc_id = meta.get("meta_doc_id") or meta.get("doc_id")
            if doc_id:
                if doc_counts[doc_id] >= self.max_candidates_per_doc:
                    continue
                doc_counts[doc_id] += 1

            seen_chunks.add(chunk_id)
            pruned.append(cand)

        return pruned

    async def _call_llm(self, prompt: str) -> Dict:
        try:
            res = await self.llm.chat.completions.create(
                model="gpt-5-nano",
                messages=[
                    {
                        "role": "system",
                        "content": "You are a compliance mapping agent. Provide concise, citation-based reasoning (MAX 250 chars) starting with section/article number (e.g., 'Â§1234.56'). Use Section from REGULATION METADATA if provided. If required_value is null, explain why: 'N/A (not regulated)' or 'N/A (already compliant)' or 'N/A (unrelated)'. Format: '[Â§XXX] [Core regulation] [Application status]'. Return JSON only.",
                    },
                    {"role": "user", "content": prompt},
                ],
            )
            return json.loads(res.choices[0].message.content)

        except Exception:
            return {
                "applies": False,
                "required_value": None,
                "current_value": None,
                "gap": None,
                "reasoning": "LLM call failed",
                "parsed": {
                    "category": None,
                    "requirement_type": "other",
                    "condition": None,
                },
            }

    async def run(self, state: Dict) -> Dict:
        product: Optional[ProductInfo] = state.get("product_info")
        mapping_filters: Dict[str, Any] = state.get("mapping_filters") or {}
        regulation_meta: Dict[str, Any] = state.get("regulation") or {}
        change_results: List[Dict[str, Any]] = (
            state.get("change_detection_results") or []
        )

        # â­ ë™ì  í•„í„°ë§: product_info ì—†ìœ¼ë©´ êµ­ê°€ë³„ ì œí’ˆ ì¡°íšŒ
        if not product:
            product_id = mapping_filters.get("product_id")

            # ì˜µì…˜ 1: product_id ì§€ì •ë¨ (ê¸°ì¡´ ë¡œì§)
            if product_id:
                async with AsyncSessionLocal() as session:
                    product = await self.product_repository.fetch_product_for_mapping(
                        session, int(product_id)
                    )
                state["product_info"] = product

            # ì˜µì…˜ 2: êµ­ê°€ ê¸°ë°˜ ë™ì  í•„í„°ë§ (ìë™ ì‹¤í–‰)
            else:
                country = self._extract_country_from_state(state)

                if not country:
                    logger.error("âŒ êµ­ê°€ ì •ë³´ ì—†ìŒ, ë§¤í•‘ ë¶ˆê°€")
                    state["mapping"] = MappingResults(
                        product_id="unknown",
                        items=[],
                        targets={},
                        actionable_changes=[],
                        pending_changes=[],
                        unknown_requirements=[],
                    )
                    return state

                logger.info(f"ğŸŒ ë™ì  í•„í„°ë§: {country} êµ­ê°€ ì œí’ˆ ì¡°íšŒ")

                # DB ì¡°íšŒ í›„ ì¦‰ì‹œ ì„¸ì…˜ ë‹«ê¸°
                products = []
                async with AsyncSessionLocal() as session:
                    products = await self.product_repository.find_by_country(
                        session, country
                    )
                # â† ì„¸ì…˜ ë‹«í˜ (ì—¬ê¸°ì„œ ë)

                if not products:
                    logger.warning(f"âš ï¸ {country} êµ­ê°€ ì œí’ˆ ì—†ìŒ")
                    state["mapping"] = MappingResults(
                        product_id="unknown",
                        items=[],
                        targets={},
                        actionable_changes=[],
                        pending_changes=[],
                        unknown_requirements=[],
                    )
                    return state

                logger.info(
                    f"âœ… {len(products)}ê°œ ì œí’ˆ ë°œê²¬: {[p['product_name'] for p in products[:3]]}..."
                )

                # ê° ì œí’ˆë³„ë¡œ ë§¤í•‘ ì‹¤í–‰ (ì„¸ì…˜ ì—†ì´)
                all_mapping_results = []
                for product in products:
                    state["product_info"] = product
                    result = await self._run_mapping_for_single_product(state)
                    all_mapping_results.append(result)

                # ê²°ê³¼ ë³‘í•©
                state["mapping"] = self._merge_multi_product_results(
                    all_mapping_results
                )
                return state

        product_id = product["product_id"]
        product_name = product.get("product_name", product.get("name", "unknown"))
        mapping_spec = product.get("mapping") or {}
        target_state = mapping_spec.get("target") or {}
        present_state = mapping_spec.get("present_state") or {}
        present_features = (
            present_state or target_state or product.get("features", {}) or {}
        )
        units = product.get("feature_units", {})

        change_scope = self._extract_change_scope(change_results, present_features)
        change_hint = self._choose_change_hint(change_scope)
        change_query = self._build_change_query(change_hint)
        recovered_hints: Set[str] = set()

        mapping_results: List[MappingItem] = []
        mapping_targets: Dict[str, Dict[str, Any]] = {}
        unknown_requirements: List[Dict[str, Any]] = []

        extra_search_filters = {
            key: value
            for key, value in mapping_filters.items()
            if key not in {"product_id"}
        }
        change_search_filters = self._build_change_filters(change_scope)
        regulation_filters = self._build_regulation_filters(regulation_meta)
        merged_search_filters = self._merge_filters(
            extra_search_filters, change_search_filters, regulation_filters
        )

        # ------------------------------------------------------
        # change_detection ë…¸ë“œì—ì„œ ë°›ì€ íŒíŠ¸ í™œìš© (ì‹ ê·œ ê·œì œ ë¶„ì„ ê²°ê³¼)
        # ------------------------------------------------------
        regulation_hints = state.get("regulation_analysis_hints") or {}

        if self.debug_enabled:
            logger.info(
                "ğŸ§­ Mapping start: product=%s name=%s features=%d top_k=%d alpha=%.2f",
                product_id,
                product_name,
                len(present_features),
                self.top_k,
                self.alpha,
            )
            logger.info(f"ğŸ“Š change_results: {len(change_results)}ê°œ")
            logger.info(
                f"ğŸ“Š change_scope: actionable={len(change_scope.get('actionable_results', []))}, pending={len(change_scope.get('pending_results', []))}, feature_hints={len(change_scope.get('feature_hints', set()))}"
            )
            logger.info(f"ğŸ“Š regulation_hints: {bool(regulation_hints)}")
            if not present_features:
                logger.info(
                    "ğŸ’¤ ë§¤í•‘ ëŒ€ìƒ íŠ¹ì„±ì´ ì—†ìŠµë‹ˆë‹¤. mapping.present_stateë‚˜ targetì„ í™•ì¸í•˜ì„¸ìš”."
                )
        if regulation_hints and not change_scope.get("feature_hints"):
            # ì‹ ê·œ ê·œì œ ë¶„ì„ ê²°ê³¼ì—ì„œ affected_areasë¥¼ feature_hintsë¡œ ë³€í™˜
            affected_areas = regulation_hints.get("affected_areas", [])
            for area in affected_areas:
                normalized = self._normalize_token(area)
                for norm_name, raw_name in {
                    self._normalize_token(name): name
                    for name in present_features.keys()
                }.items():
                    if normalized == norm_name or normalized in norm_name:
                        recovered_hints.add(raw_name)

            if self.debug_enabled:
                logger.info(
                    f"ğŸ†• ì‹ ê·œ ê·œì œ íŒíŠ¸ í™œìš©: {len(recovered_hints)}ê°œ feature ë³µêµ¬"
                )

        feature_iterable, unknown_hints = self._select_features_for_mapping(
            present_features, change_scope, recovered_hints
        )
        if self.debug_enabled:
            logger.info(
                "ğŸ” feature selection â€” hints=%s recovered=%s selected=%d",
                list(change_scope.get("feature_hints") or []),
                list(recovered_hints),
                len(feature_iterable),
            )
        if unknown_hints:
            unknown_requirements.extend(
                [
                    {
                        "hint": hint,
                        "reason": "change_detection_hint_not_in_product_features",
                    }
                    for hint in unknown_hints
                ]
            )

        # ğŸ”¥ featureë³„ë¡œ ê²€ìƒ‰ TOOL â†’ ë§¤í•‘
        llm_semaphore = asyncio.Semaphore(10)

        async def process_feature(feature_name: str, present_value: Any):
            unit = units.get(feature_name)
            target_value = target_state.get(feature_name)

            # a) ê²€ìƒ‰ TOOL í˜¸ì¶œ
            if self.debug_enabled:
                logger.info(
                    "ğŸ” Searching feature=%s value=%s unit=%s",
                    feature_name,
                    present_value,
                    unit or "-",
                )
            retrieval: RetrievalResult = await self._run_search(
                product,
                feature_name,
                present_value,
                unit,
                merged_search_filters,
                change_query=change_query,
            )
            original_count = len(retrieval["candidates"])
            retrieval["candidates"] = self._prune_candidates(retrieval["candidates"])
            pruned_count = len(retrieval["candidates"])
            if self.debug_enabled:
                logger.info(
                    "   â†³ candidates=%d (pruned to %d)",
                    original_count,
                    pruned_count,
                )

            ranked_candidates = retrieval["candidates"]
            rerank_result: Optional[Dict[str, Any]] = None
            if change_hint and ranked_candidates:
                # ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ 3ê°œ ì„ íƒ
                ranked_candidates = self._rule_rank_candidates(
                    ranked_candidates, change_hint, top_n=3
                )
                # LLM rerankë¡œ ìµœì¢… 1ê°œ ì„ íƒ
                rerank_result = await self._rerank_candidates(
                    change_hint, ranked_candidates
                )
                if rerank_result and rerank_result.get("selected_point_id"):
                    selected_id = rerank_result["selected_point_id"]
                    ranked_candidates = [
                        cand
                        for cand in ranked_candidates
                        if cand.get("chunk_id") == selected_id
                    ] or ranked_candidates

            # rerankê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•´ë„ ì¤‘ë³µ ë§¤í•‘ì„ í”¼í•˜ê¸° ìœ„í•´ ìƒìœ„ 1ê°œë§Œ ì‚¬ìš©
            if ranked_candidates:
                ranked_candidates = ranked_candidates[:1]

            # b) LLM ë§¤í•‘ ìˆ˜í–‰ (í›„ë³´ë³„ ë³‘ë ¬ + Semaphore ì œí•œ)
            async def process_candidate(cand: RetrievedChunk):
                # ë³€ê²½ ê°ì§€ ì¦ê±° ì¶”ì¶œ
                change_matches = self._match_change_results_to_candidate(
                    change_scope, cand
                )
                change_evidence = change_matches[0] if change_matches else None
                
                # ğŸ”‘ Change Detection Indexì—ì„œ ì§ì ‘ ì¡°íšŒ
                change_context = None
                change_index = state.get("change_detection_index", {})
                if change_index:
                    section = self._extract_section_from_chunk(
                        cand.get("metadata", {}), cand["chunk_text"]
                    )
                    if section and section in change_index:
                        change_context = change_index[section]
                        logger.debug(f"ğŸ¯ Change Context ë°œê²¬: {section}")

                prompt = self._build_prompt(
                    feature_name,
                    present_value,
                    target_value,
                    unit,
                    cand["chunk_text"],
                    chunk_metadata=cand.get("metadata", {}),
                    change_evidence=change_evidence,
                    change_context=change_context,
                )

                # Semaphoreë¡œ LLM í˜¸ì¶œ ì œí•œ
                async with llm_semaphore:
                    llm_out = await self._call_llm(prompt)

                parsed: MappingParsed = llm_out.get("parsed", {})
                required_value = llm_out.get("required_value")
                current_value = llm_out.get("current_value")
                if (
                    llm_out.get("applies")
                    and required_value is None
                    and target_value is not None
                ):
                    required_value = target_value
                if current_value is None and present_value is not None:
                    current_value = present_value

                return MappingItem(
                    feature_name=feature_name,
                    applies=llm_out["applies"],
                    required_value=required_value,
                    current_value=current_value,
                    gap=llm_out["gap"],
                    reasoning=llm_out.get("reasoning", "")[:250],  # ìµœëŒ€ 250ì
                    regulation_chunk_id=cand["chunk_id"],
                    regulation_summary=cand["chunk_text"][:120],
                    parsed=parsed,
                )

            # í›„ë³´ë³„ ë³‘ë ¬ ì²˜ë¦¬
            candidate_results = await asyncio.gather(
                *[process_candidate(cand) for cand in ranked_candidates],
                return_exceptions=True,
            )
            items: List[MappingItem] = []
            for r in candidate_results:
                if isinstance(r, Exception):
                    continue
                items.append(r)
                if self.debug_enabled:
                    logger.info(
                        "ğŸ§© applies=%s required=%s current=%s chunk=%s (%s)",
                        r["applies"],
                        r["required_value"],
                        r["current_value"],
                        r["regulation_chunk_id"],
                        r["feature_name"],
                    )
            return items

        # featureë³„ ë³‘ë ¬ ì²˜ë¦¬
        feature_results = await asyncio.gather(
            *[process_feature(fname, fval) for fname, fval in feature_iterable],
            return_exceptions=True,
        )

        for result in feature_results:
            if isinstance(result, Exception):
                logger.error(f"âŒ Feature ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
                continue
            if isinstance(result, list):
                mapping_results.extend(result)
                for item in result:
                    if item["applies"]:
                        feature_name = item["feature_name"]
                        existing = mapping_targets.get(feature_name)
                        has_req = item.get("required_value") is not None
                        replace = False
                        if existing is None:
                            replace = True
                        elif existing.get("required_value") is None and has_req:
                            replace = True
                        if replace:
                            mapping_targets[feature_name] = {
                                "required_value": item.get("required_value"),
                                "chunk_id": item.get("regulation_chunk_id"),
                                "doc_id": item.get("regulation_meta", {}).get(
                                    "meta_doc_id"
                                ),
                            }

        # -----------------------------------------
        # c) ì „ì—­ State ì—…ë°ì´íŠ¸
        # -----------------------------------------
        # ë§¤í•‘ ê²°ê³¼(required_value)ë¥¼ product_info.mapping.targetì— ë°˜ì˜í•´ ì´í›„ ë…¸ë“œê°€ ë°”ë¡œ ë¹„êµí•  ìˆ˜ ìˆê²Œ í•œë‹¤.
        product_mapping = product.get("mapping") or {}
        updated_target = dict(product_mapping.get("target") or {})
        for fname, target_info in mapping_targets.items():
            required_value = target_info.get("required_value")
            if required_value is not None:
                updated_target[fname] = required_value
        product_mapping["target"] = updated_target
        product["mapping"] = product_mapping
        state["product_info"] = product

        # regulation_cache ìƒì„± (ì¤‘ë³µ ì œê±°)
        regulation_cache = {}
        for item in mapping_results:
            chunk_id = item["regulation_chunk_id"]
            if chunk_id not in regulation_cache:
                # í•„ìš”í•œ ë©”íƒ€ë°ì´í„°ë§Œ ìºì‹œ
                meta = change_scope.get("raw_results", [])
                matched_change = next(
                    (r for r in meta if r.get("chunk_id") == chunk_id or 
                     r.get("new_ref_id") == chunk_id),
                    None
                )
                regulation_cache[chunk_id] = {
                    "change_detected": bool(matched_change),
                    "confidence_score": matched_change.get("confidence_score") if matched_change else None,
                    "change_type": matched_change.get("change_type") if matched_change else None,
                }
        
        mapping_payload = MappingResults(
            product_id=product_id,
            product_name=product_name,
            items=mapping_results,
            targets=mapping_targets,
            unknown_requirements=unknown_requirements,
            regulation_cache=regulation_cache,
        )
        # NOTE: mappingê³¼ mapping_resultsëŠ” ë™ì¼í•œ ë°ì´í„° (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
        state["mapping"] = mapping_payload
        # regulation_trace ì—…ë°ì´íŠ¸ (in-memory)
        trace_entries = self._build_trace_entries(mapping_results, regulation_meta, regulation_cache)
        if trace_entries:
            existing_trace = product.get("regulation_trace") or {}
            existing_list = existing_trace.get("trace") or []
            last_updated = trace_entries[0]["updated_at"]
            product["regulation_trace"] = {
                "trace": existing_list + trace_entries,
                "last_updated": last_updated,
            }
            state["product_info"] = product
        if self.debug_enabled:
            _log_mapping_preview(product_id, mapping_results)
            snapshot_path = _persist_mapping_snapshot(
                product,
                mapping_results,
                state,
                self.top_k,
                self.alpha,
            )
            if snapshot_path:
                state["mapping_debug"] = {
                    "snapshot_path": snapshot_path,
                    "total_items": len(mapping_results),
                }

        return state

    def _extract_country_from_state(self, state: Dict) -> Optional[str]:
        """
        Stateì—ì„œ êµ­ê°€ ì •ë³´ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„ ì ìš©).

                ìš°ì„ ìˆœìœ„:
                1. regulation ë©”íƒ€ë°ì´í„°
                2. preprocess_results (Vision ì¶”ì¶œ)
                3. change_detection_results
        """
        # ìš°ì„ ìˆœìœ„ 1: regulation ë©”íƒ€ë°ì´í„°
        regulation = state.get("regulation") or {}
        country = regulation.get("country") or regulation.get("jurisdiction_code")
        if country:
            logger.debug(f"êµ­ê°€ ì¶”ì¶œ (regulation): {country}")
            return country

        # ìš°ì„ ìˆœìœ„ 2: preprocess_results (Vision ì¶”ì¶œ)
        preprocess_results = state.get("preprocess_results") or []
        if preprocess_results:
            first_result = preprocess_results[0]
            vision_results = first_result.get("vision_extraction_result") or []
            if vision_results:
                first_page = vision_results[0]
                metadata = first_page.get("structure", {}).get("metadata", {})
                country = metadata.get("jurisdiction_code") or metadata.get("country")
                if country:
                    logger.debug(f"êµ­ê°€ ì¶”ì¶œ (vision): {country}")
                    return country

        # ìš°ì„ ìˆœìœ„ 3: change_detection_results
        change_results = state.get("change_detection_results") or []
        if change_results:
            for result in change_results:
                metadata = result.get("metadata") or {}
                country = metadata.get("country") or metadata.get("jurisdiction_code")
                if country:
                    logger.debug(f"êµ­ê°€ ì¶”ì¶œ (change_detection): {country}")
                    return country

        logger.warning("êµ­ê°€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
        return None

    async def _run_mapping_for_single_product(self, state: Dict) -> MappingResults:
        """ë‹¨ì¼ ì œí’ˆì— ëŒ€í•œ ë§¤í•‘ ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)."""
        product: ProductInfo = state["product_info"]
        product_id = product["product_id"]
        product_name = product.get("product_name", product.get("name", "unknown"))
        mapping_spec = product.get("mapping") or {}
        target_state = mapping_spec.get("target") or {}
        present_state = mapping_spec.get("present_state") or {}
        present_features = (
            present_state or target_state or product.get("features", {}) or {}
        )
        units = product.get("feature_units", {})

        change_results: List[Dict[str, Any]] = (
            state.get("change_detection_results") or []
        )
        regulation_meta: Dict[str, Any] = state.get("regulation") or {}
        mapping_filters: Dict[str, Any] = state.get("mapping_filters") or {}

        change_scope = self._extract_change_scope(change_results, present_features)
        change_hint = self._choose_change_hint(change_scope)
        change_query = self._build_change_query(change_hint)
        recovered_hints: Set[str] = set()

        regulation_hints = state.get("regulation_analysis_hints") or {}
        if regulation_hints and not change_scope.get("feature_hints"):
            affected_areas = regulation_hints.get("affected_areas", [])
            for area in affected_areas:
                normalized = self._normalize_token(area)
                for norm_name, raw_name in {
                    self._normalize_token(name): name
                    for name in present_features.keys()
                }.items():
                    if normalized == norm_name or normalized in norm_name:
                        recovered_hints.add(raw_name)

        feature_iterable, unknown_hints = self._select_features_for_mapping(
            present_features, change_scope, recovered_hints
        )

        mapping_results: List[MappingItem] = []
        mapping_targets: Dict[str, Dict[str, Any]] = {}
        unknown_requirements: List[Dict[str, Any]] = []

        if unknown_hints:
            unknown_requirements.extend(
                [
                    {
                        "hint": hint,
                        "reason": "change_detection_hint_not_in_product_features",
                    }
                    for hint in unknown_hints
                ]
            )

        extra_search_filters = {
            key: value
            for key, value in mapping_filters.items()
            if key not in {"product_id"}
        }
        change_search_filters = self._build_change_filters(change_scope)
        regulation_filters = self._build_regulation_filters(regulation_meta)
        merged_search_filters = self._merge_filters(
            extra_search_filters, change_search_filters, regulation_filters
        )

        async def process_feature(feature_name: str, present_value: Any):
            unit = units.get(feature_name)
            target_value = target_state.get(feature_name)

            retrieval: RetrievalResult = await self._run_search(
                product,
                feature_name,
                present_value,
                unit,
                merged_search_filters,
                change_query=change_query,
            )
            original_count = len(retrieval["candidates"])
            retrieval["candidates"] = self._prune_candidates(retrieval["candidates"])
            pruned_count = len(retrieval["candidates"])

            ranked_candidates = retrieval["candidates"]
            rerank_result: Optional[Dict[str, Any]] = None
            if change_hint and ranked_candidates:
                ranked_candidates = self._rule_rank_candidates(
                    ranked_candidates, change_hint, top_n=3
                )
                rerank_result = await self._rerank_candidates(
                    change_hint, ranked_candidates
                )
                if rerank_result and rerank_result.get("selected_point_id"):
                    selected_id = rerank_result["selected_point_id"]
                    ranked_candidates = [
                        cand
                        for cand in ranked_candidates
                        if cand.get("chunk_id") == selected_id
                    ] or ranked_candidates

            if ranked_candidates:
                ranked_candidates = ranked_candidates[:1]

            async def process_candidate(cand: RetrievedChunk):
                prompt = self._build_prompt(
                    feature_name,
                    present_value,
                    target_value,
                    unit,
                    cand["chunk_text"],
                )
                llm_out = await self._call_llm(prompt)

                parsed: MappingParsed = llm_out.get("parsed", {})
                required_value = llm_out.get("required_value")
                current_value = llm_out.get("current_value")
                if (
                    llm_out.get("applies")
                    and required_value is None
                    and target_value is not None
                ):
                    required_value = target_value
                if current_value is None and present_value is not None:
                    current_value = present_value

                return MappingItem(
                    feature_name=feature_name,
                    applies=llm_out["applies"],
                    required_value=required_value,
                    current_value=current_value,
                    gap=llm_out["gap"],
                    reasoning=llm_out.get("reasoning", "")[:250],  # ìµœëŒ€ 250ì
                    regulation_chunk_id=cand["chunk_id"],
                    regulation_summary=cand["chunk_text"][:120],
                    parsed=parsed,
                )

            candidate_results = await asyncio.gather(
                *[process_candidate(cand) for cand in ranked_candidates],
                return_exceptions=True,
            )
            items: List[MappingItem] = []
            for r in candidate_results:
                if isinstance(r, Exception):
                    continue
                items.append(r)
            return items

        feature_results = await asyncio.gather(
            *[process_feature(fname, fval) for fname, fval in feature_iterable],
            return_exceptions=True,
        )

        for result in feature_results:
            if isinstance(result, Exception):
                logger.error(f"âŒ Feature ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
                continue
            if isinstance(result, list):
                mapping_results.extend(result)
                for item in result:
                    if item["applies"]:
                        feature_name = item["feature_name"]
                        existing = mapping_targets.get(feature_name)
                        has_req = item.get("required_value") is not None
                        replace = False
                        if existing is None:
                            replace = True
                        elif existing.get("required_value") is None and has_req:
                            replace = True
                        if replace:
                            mapping_targets[feature_name] = {
                                "required_value": item.get("required_value"),
                                "chunk_id": item.get("regulation_chunk_id"),
                                "doc_id": item.get("regulation_meta", {}).get(
                                    "meta_doc_id"
                                ),
                            }

        # regulation_cache ìƒì„±
        regulation_cache = {}
        for item in mapping_results:
            chunk_id = item["regulation_chunk_id"]
            if chunk_id not in regulation_cache:
                meta = change_scope.get("raw_results", [])
                matched_change = next(
                    (r for r in meta if r.get("chunk_id") == chunk_id or 
                     r.get("new_ref_id") == chunk_id),
                    None
                )
                regulation_cache[chunk_id] = {
                    "change_detected": bool(matched_change),
                    "confidence_score": matched_change.get("confidence_score") if matched_change else None,
                    "change_type": matched_change.get("change_type") if matched_change else None,
                }
        
        return MappingResults(
            product_id=product_id,
            product_name=product_name,
            items=mapping_results,
            targets=mapping_targets,
            unknown_requirements=unknown_requirements,
            regulation_cache=regulation_cache,
        )

    def _merge_multi_product_results(
        self, results: List[MappingResults]
    ) -> MappingResults:
        """ì—¬ëŸ¬ ì œí’ˆì˜ ë§¤í•‘ ê²°ê³¼ ë³‘í•©."""
        all_items = []
        all_targets = {}
        all_unknown = []

        for result in results:
            all_items.extend(result["items"])
            all_targets.update(result["targets"])
            all_unknown.extend(result["unknown_requirements"])

        # regulation_cache ë³‘í•©
        merged_cache = {}
        for result in results:
            merged_cache.update(result.get("regulation_cache", {}))
        
        return MappingResults(
            product_id="multi",
            product_name="Multiple Products",
            items=all_items,
            targets=all_targets,
            unknown_requirements=all_unknown,
            regulation_cache=merged_cache,
        )


_DEFAULT_LLM_CLIENT = None
_DEFAULT_PRODUCT_REPOSITORY: Optional[ProductRepository] = None
_DEFAULT_MAPPING_NODE: Optional[MappingNode] = None


def _get_default_llm_client():
    """AsyncOpenAI ì‹±ê¸€í†¤ì„ êµ¬ì„±í•œë‹¤."""
    global _DEFAULT_LLM_CLIENT
    if _DEFAULT_LLM_CLIENT is not None:
        return _DEFAULT_LLM_CLIENT

    if AsyncOpenAI is None:
        raise RuntimeError(
            "openai íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `pip install openai` í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
        )

    _DEFAULT_LLM_CLIENT = AsyncOpenAI()
    return _DEFAULT_LLM_CLIENT


def _get_default_product_repository() -> ProductRepository:
    """ìˆ˜ì •: Repository ìƒì„± ë°©ì‹ ê°„ì†Œí™”"""
    global _DEFAULT_PRODUCT_REPOSITORY
    if _DEFAULT_PRODUCT_REPOSITORY is None:
        _DEFAULT_PRODUCT_REPOSITORY = ProductRepository()
    return _DEFAULT_PRODUCT_REPOSITORY


def _build_mapping_node(
    *,
    llm_client=None,
    search_tool=None,
    top_k: Optional[int] = None,
    alpha: Optional[float] = None,
    product_repository: Optional[ProductRepository] = None,
    max_candidates_per_doc: int = 2,
) -> MappingNode:
    """MappingNode ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•œë‹¤."""
    resolved_llm = llm_client or _get_default_llm_client()
    resolved_top_k = top_k if top_k is not None else settings.MAPPING_TOP_K
    resolved_alpha = alpha if alpha is not None else settings.MAPPING_ALPHA
    resolved_repo = product_repository or _get_default_product_repository()
    return MappingNode(
        llm_client=resolved_llm,
        search_tool=search_tool,
        top_k=resolved_top_k,
        alpha=resolved_alpha,
        product_repository=resolved_repo,
        max_candidates_per_doc=max_candidates_per_doc,
    )


def _get_default_mapping_node() -> MappingNode:
    """íŒŒì´í”„ë¼ì¸ ì „ìš© ê¸°ë³¸ MappingNode."""
    global _DEFAULT_MAPPING_NODE
    if _DEFAULT_MAPPING_NODE is None:
        _DEFAULT_MAPPING_NODE = _build_mapping_node()
    return _DEFAULT_MAPPING_NODE


async def map_products_node(state: AppState) -> AppState:
    """
    LangGraph entrypoint wrapping MappingNode.

    state["mapping_context"]ë¥¼ í†µí•´ í…ŒìŠ¤íŠ¸/íŠ¹ìˆ˜ ì‹¤í–‰ ì‹œ LLM ë˜ëŠ” Toolì„ ì£¼ì…í•  ìˆ˜ ìˆë‹¤.
    """

    context: MappingContext = state.get("mapping_context", {}) or {}
    has_override = any(
        key in context
        for key in (
            "llm_client",
            "search_tool",
            "top_k",
            "alpha",
            "max_candidates_per_doc",
        )
    )
    if has_override:
        node = _build_mapping_node(
            llm_client=context.get("llm_client"),
            search_tool=context.get("search_tool"),
            top_k=context.get("top_k"),
            alpha=context.get("alpha"),
            max_candidates_per_doc=context.get("max_candidates_per_doc", 2),
        )
    else:
        node = _get_default_mapping_node()

    return await node.run(state)


__all__ = ["MappingNode", "map_products_node"]


def _log_mapping_preview(product_id: str, items: List[MappingItem]) -> None:
    max_items = max(1, settings.MAPPING_DEBUG_MAX_ITEMS)
    preview = items[:max_items]
    if not preview:
        logger.info("ğŸ“­ Mapping produced no items for product=%s", product_id)
        return

    logger.info("ğŸ“’ Mapping preview (showing %d/%d items):", len(preview), len(items))
    for idx, item in enumerate(preview, 1):
        logger.info(
            "  %d) feature=%s applies=%s required=%s current=%s chunk=%s",
            idx,
            item["feature_name"],
            item["applies"],
            item["required_value"],
            item["current_value"],
            item["regulation_chunk_id"],
        )


def _persist_mapping_snapshot(
    product: ProductInfo,
    items: List[MappingItem],
    state: AppState,
    top_k: int,
    alpha: float,
) -> Optional[str]:
    if not settings.MAPPING_DEBUG_DIR:
        return None

    target_dir = Path(settings.MAPPING_DEBUG_DIR)
    try:
        target_dir.mkdir(parents=True, exist_ok=True)
    except Exception as exc:  # pragma: no cover - disk trouble
        logger.warning("Failed to create mapping debug dir: %s", exc)
        return None

    product_id = product["product_id"]
    timestamp = datetime.utcnow().strftime("%Y%m%d-%H%M%S")
    doc_id = None
    preprocess_results = state.get("preprocess_results") or []
    if preprocess_results:
        doc_id = preprocess_results[0].get("doc_id")
    doc_suffix = doc_id or "unknown-doc"
    filename = f"{timestamp}_{product_id}_{doc_suffix}.json"
    snapshot_path = target_dir / filename

    payload = {
        "generated_at": datetime.utcnow().isoformat(),
        "product": product,
        "preprocess_summary": state.get("preprocess_summary"),
        "mapping_config": {
            "top_k": top_k,
            "alpha": alpha,
        },
        "total_items": len(items),
        "items": items,
    }

    snapshot_path.write_text(
        json.dumps(
            payload,
            ensure_ascii=False,
            indent=2,
            default=_json_safe_encoder,
        ),
        encoding="utf-8",
    )
    logger.info("ğŸ“ Mapping snapshot saved: %s", snapshot_path)
    return str(snapshot_path)


def _json_safe_encoder(value: Any):
    if isinstance(value, Decimal):
        return float(value)
    raise TypeError(f"Object of type {type(value).__name__} is not JSON serializable")
