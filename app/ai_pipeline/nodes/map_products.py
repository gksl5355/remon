"""
module: map_products.py
description: Í≤ÄÏÉâ TOOL + LLM Îß§Ìïë Node
author: AI Agent
created: 2025-01-18
updated: 2025-12-09
dependencies:
    - openai
    - app.ai_pipeline.tools.retrieval_tool
    - app.core.repositories.product_repository
"""

import asyncio
import json
import logging
from decimal import Decimal
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple
from collections import defaultdict

try:  # pragma: no cover - import guard
    from openai import AsyncOpenAI
except Exception:  # pragma: no cover
    AsyncOpenAI = None  # type: ignore[assignment]

from app.ai_pipeline.state import (
    ProductInfo,
    RetrievedChunk,
    RetrievalResult,
    MappingItem,
    MappingParsed,
    MappingResults,
    AppState,
    MappingContext,
)

from app.ai_pipeline.prompts.mapping_prompt import MAPPING_PROMPT
from app.ai_pipeline.tools.retrieval_utils import build_product_filters
from app.ai_pipeline.tools.retrieval_tool import (
    RetrievalOutput,
    get_retrieval_tool,
)
from app.config.settings import settings
from app.core.database import AsyncSessionLocal

# Ï∂îÍ∞Ä: Repository import
from app.core.repositories.product_repository import ProductRepository


logger = logging.getLogger(__name__)

HIGH_CONF_THRESHOLD = 0.7
LOW_CONF_THRESHOLD = 0.5


class MappingNode:
    """Í≤ÄÏÉâ + Îß§Ìïë ÌÜµÌï© Node."""

    def __init__(
        self,
        llm_client,
        search_tool,
        top_k: int = 10,
        alpha: float = 0.7,
        product_repository: Optional[ProductRepository] = None,
        max_candidates_per_doc: int = 2,
    ):
        self.llm = llm_client
        self.search_tool = search_tool or get_retrieval_tool()
        self.top_k = top_k
        self.alpha = alpha
        self.product_repository = product_repository or ProductRepository()
        self.debug_enabled = settings.MAPPING_DEBUG_ENABLED
        self.max_candidates_per_doc = max_candidates_per_doc
        self._llm_semaphore = None

    # ----------------------------------------------------------------------
    # change detection Ïó∞Í≥Ñ Ïú†Ìã∏
    # ----------------------------------------------------------------------
    def _normalize_token(self, value: str) -> str:
        return (
            value.lower()
            .replace(" ", "_")
            .replace("-", "_")
            .replace(".", "_")
        )

    def _extract_change_scope(
        self,
        change_results: List[Dict[str, Any]],
        present_features: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Î≥ÄÍ≤Ω Í∞êÏßÄ Í≤∞Í≥ºÏóêÏÑú Í≤ÄÏÉâ/Îß§ÌïëÏóê Ïì∏ ÌûåÌä∏Î•º Ï∂îÏ∂úÌïúÎã§.
        """
        if not change_results:
            return {
                "actionable_results": [],
                "pending_results": [],
                "doc_filters": set(),
                "chunk_filters": set(),
                "feature_hints": set(),
                "raw_results": [],
            }

        feature_key_map = {
            self._normalize_token(name): name for name in present_features.keys()
        }
        feature_keys = set(feature_key_map.keys())
        doc_filters: Set[str] = set()
        chunk_filters: Set[str] = set()
        feature_hints: Set[str] = set()
        actionable: List[Dict[str, Any]] = []
        pending: List[Dict[str, Any]] = []

        for result in change_results:
            status = result.get("status")
            change_detected = result.get("change_detected")
            positive_status = (status or "").lower() in (
                "changed",
                "updated",
                "new",
                "modified",
                "added",
            )
            confidence = (
                result.get("confidence_score")
                or result.get("score")
                or result.get("confidence")
                or 0.0
            )

            # Ïã†Î¢∞ÎèÑ/ÏÉÅÌÉúÏóê Îî∞Î•∏ Î∂ÑÎ•ò
            is_inconclusive = (status or "").lower() == "inconclusive"
            if change_detected or positive_status:
                if confidence >= HIGH_CONF_THRESHOLD:
                    actionable.append(result)
                elif confidence >= LOW_CONF_THRESHOLD:
                    pending.append(result)
                else:
                    continue
            elif is_inconclusive:
                pending.append(result)
            else:
                # ÎÑàÎ¨¥ ÎÇÆÏùÄ Ïã†Î¢∞ÎèÑÎäî Ïä§ÌÇµ
                continue

            # Î¨∏ÏÑú/Ï≤≠ÌÅ¨ ÏãùÎ≥ÑÏûê ÏàòÏßë (Í≤ÄÏÉâ ÌïÑÌÑ∞)
            for key in (
                "doc_id",
                "regulation_id",
                "new_regulation_id",
                "legacy_regulation_id",
                "meta_doc_id",
            ):
                val = result.get(key)
                if val:
                    doc_filters.add(str(val))

            meta = result.get("metadata") or {}
            for key in ("doc_id", "meta_doc_id"):
                val = meta.get(key)
                if val:
                    doc_filters.add(str(val))

            for key in (
                "chunk_id",
                "new_chunk_id",
                "legacy_chunk_id",
                "new_ref_id",
                "legacy_ref_id",
            ):
                val = result.get(key)
                if val:
                    chunk_filters.add(str(val))

            # feature ÌûåÌä∏: Î™ÖÏãúÏ†Å feature ÌïÑÎìú ÎòêÎäî keywordsÏôÄ Ïù¥Î¶Ñ Îß§Ïπ≠
            for key in ("feature", "feature_name", "feature_names"):
                val = result.get(key)
                if isinstance(val, str):
                    normalized = self._normalize_token(val)
                    if normalized in feature_keys:
                        feature_hints.add(feature_key_map[normalized])
                elif isinstance(val, list):
                    for item in val:
                        if not isinstance(item, str):
                            continue
                        normalized = self._normalize_token(item)
                        if normalized in feature_keys:
                            feature_hints.add(feature_key_map[normalized])

            for kw in result.get("keywords", []) or []:
                if not isinstance(kw, str):
                    continue
                normalized_kw = self._normalize_token(kw)
                for norm_name, raw_name in feature_key_map.items():
                    # ÏôÑÏ†Ñ ÏùºÏπò Ïö∞ÏÑ†, Î∂ÄÎ∂Ñ ÏùºÏπòÎäî Î≥¥Ï°∞
                    if normalized_kw == norm_name:
                        feature_hints.add(raw_name)
                    elif normalized_kw in norm_name or norm_name in normalized_kw:
                        feature_hints.add(raw_name)

        return {
            "actionable_results": actionable,
            "pending_results": pending,
            "doc_filters": doc_filters,
            "chunk_filters": chunk_filters,
            "feature_hints": feature_hints,
            "raw_results": change_results,
        }

    def _build_change_filters(self, change_scope: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        filters: Dict[str, Any] = {}
        doc_filters = change_scope.get("doc_filters") or set()
        chunk_filters = change_scope.get("chunk_filters") or set()
        if doc_filters:
            filters["meta_doc_id"] = list(doc_filters)
        if chunk_filters:
            filters["chunk_id"] = list(chunk_filters)
        return filters or None

    def _select_features_for_mapping(
        self,
        present_features: Dict[str, Any],
        change_scope: Dict[str, Any],
        recovered_hints: Optional[Set[str]] = None,
    ) -> Tuple[List[Tuple[str, Any]], List[str]]:
        """
        Î≥ÄÍ≤Ω ÌûåÌä∏/Î≥µÍµ¨ ÌûåÌä∏Í∞Ä ÏûàÏúºÎ©¥ Ìï¥Îãπ featureÎßå ÏÑ†ÌÉù.
        ÌûåÌä∏Í∞Ä ÏóÜÏúºÎ©¥ Ï†ÑÏ≤¥ feature Îß§Ìïë (Legacy ÏûàÏßÄÎßå Î≥ÄÍ≤Ω ÏóÜÎäî Í≤ΩÏö∞ ÎåÄÏùë).
        """
        unknown: List[str] = []
        if not present_features:
            return [], unknown

        hints: Set[str] = set(change_scope.get("feature_hints") or set())
        if recovered_hints:
            hints |= recovered_hints

        if hints:
            # ÌûåÌä∏Í∞Ä ÏûàÏúºÎ©¥ Ìï¥Îãπ featureÎßå ÏÑ†ÌÉù
            filtered = [
                (name, value)
                for name, value in present_features.items()
                if name in hints
            ]
            unknown = [hint for hint in hints if hint not in present_features]
            logger.info(f"üéØ ÌûåÌä∏ Í∏∞Î∞ò Îß§Ìïë: {len(filtered)}Í∞ú feature ÏÑ†ÌÉù")
            return filtered, unknown

        # ÌûåÌä∏Í∞Ä ÏóÜÏúºÎ©¥ Ï†ÑÏ≤¥ feature Îß§Ìïë (Legacy ÏûàÏßÄÎßå Î≥ÄÍ≤Ω ÏóÜÎäî Í≤ΩÏö∞)
        all_features = [
            (name, value)
            for name, value in present_features.items()
            if name != "feature_units"  # feature_unitsÎäî Ï†úÏô∏
        ]
        logger.info(f"üîç Ï†ÑÏ≤¥ feature Îß§Ìïë: {len(all_features)}Í∞ú feature")
        return all_features, unknown

    async def _classify_change_requirement(
        self,
        change_hint: Dict[str, Any],
        present_features: Dict[str, Any],
        sem,
    ) -> Dict[str, Any]:
        """
        change_detection Í≤∞Í≥ºÎ•º Í∏∞Î∞òÏúºÎ°ú
        - existing_feature: Ïö∞Î¶¨ Ïä§ÌéôÏóê ÏûàÏùå ‚Üí matched_feature Î∞òÌôò
        - new_requirement: Ïã†Í∑ú ÏöîÍµ¨ ‚Üí ÏïåÎ¶ºÏö© Í∏∞Î°ù
        - ambiguous: Î∂àÌôïÏã§ ‚Üí ÏïåÎ¶ºÏö© Í∏∞Î°ù
        """
        features_list = [
            {"name": name, "unit": present_features.get("feature_units", {}).get(name), "value": val}
            for name, val in present_features.items()
            if name != "feature_units"
        ]
        prompt = {
            "task": "classify_change_requirement",
            "change_hint": {
                "change_type": change_hint.get("change_type"),
                "keywords": change_hint.get("keywords", []),
                "numerical_changes": change_hint.get("numerical_changes", []),
                "new_snippet": change_hint.get("new_snippet") or change_hint.get("new_text"),
                "legacy_snippet": change_hint.get("legacy_snippet") or change_hint.get("legacy_text"),
                "section_ref": change_hint.get("section_ref"),
            },
            "product_features": features_list,
            "instructions": (
                "Given the change hint and product feature list, decide whether it matches an existing feature."
                " If not, mark as new_requirement. If unsure, mark ambiguous.\n"
                "Output JSON only: "
                "{\"match_status\": \"existing_feature\"|\"new_requirement\"|\"ambiguous\", "
                "\"matched_feature\": \"name or null\", "
                "\"reason\": \"string\", "
                "\"suggested_hint\": \"string or null\"}"
            ),
        }
        async with sem:
            try:
                res = await self.llm.chat.completions.create(
                    model="gpt-5-nano",
                    messages=[{"role": "user", "content": json.dumps(prompt, ensure_ascii=False)}],
                )
                return json.loads(res.choices[0].message.content)
            except Exception:
                return {
                    "match_status": "ambiguous",
                    "matched_feature": None,
                    "reason": "llm_error",
                    "suggested_hint": None,
                }

    def _candidate_matches_change(
        self,
        change_result: Dict[str, Any],
        doc_id: Optional[str],
        chunk_id: Optional[str],
    ) -> bool:
        doc_ids = {
            str(v)
            for v in (
                change_result.get("doc_id"),
                change_result.get("regulation_id"),
                change_result.get("new_regulation_id"),
                change_result.get("legacy_regulation_id"),
                change_result.get("meta_doc_id"),
            )
            if v is not None
        }
        meta = change_result.get("metadata") or {}
        for key in ("doc_id", "meta_doc_id"):
            val = meta.get(key)
            if val:
                doc_ids.add(str(val))

        chunk_ids = {
            str(v)
            for v in (
                change_result.get("chunk_id"),
                change_result.get("new_chunk_id"),
                change_result.get("legacy_chunk_id"),
                change_result.get("new_ref_id"),
                change_result.get("legacy_ref_id"),
            )
            if v is not None
        }

        if chunk_id and chunk_id in chunk_ids:
            return True
        if doc_id and doc_id in doc_ids:
            return True
        return False

    def _match_change_results_to_candidate(
        self,
        change_scope: Dict[str, Any],
        candidate: RetrievedChunk,
    ) -> List[Dict[str, Any]]:
        """Í≤ÄÏÉâÎêú Ï≤≠ÌÅ¨ÏôÄ Ïó∞Í¥ÄÎêú Î≥ÄÍ≤Ω Í∞êÏßÄ Í≤∞Í≥ºÎ•º Ï∞æÏïÑ regulation_metaÏóê Îã¥ÎäîÎã§."""
        matches: List[Dict[str, Any]] = []
        meta = candidate.get("metadata") or {}
        doc_id = meta.get("meta_doc_id") or meta.get("doc_id")
        chunk_id = candidate.get("chunk_id")
        for result in (change_scope.get("actionable_results") or []) + (
            change_scope.get("pending_results") or []
        ):
            if self._candidate_matches_change(result, doc_id, chunk_id):
                matches.append(result)
        return matches

    def _build_regulation_filters(self, regulation: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """state.regulation Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò Í≤ÄÏÉâ ÌïÑÌÑ∞."""
        if not regulation:
            return {}
        filters: Dict[str, Any] = {}
        for key in ("country", "citation_code", "effective_date", "title", "regulation_id"):
            val = regulation.get(key)
            if val:
                filters[key] = val
        return filters

    def _merge_filters(self, *filters: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        merged: Dict[str, Any] = {}
        for src in filters:
            if src:
                merged.update(src)
        return merged or None

    def _build_change_query(self, change_hint: Optional[Dict[str, Any]]) -> Optional[str]:
        """Î≥ÄÍ≤Ω Í∞êÏßÄ ÌûåÌä∏ÏóêÏÑú ÌïµÏã¨ ÌÇ§ÏõåÎìúÎßå Ï∂îÏ∂ú (Í∞ÑÍ≤∞Ìïú ÏøºÎ¶¨)."""
        if not change_hint:
            return None
        parts: List[str] = []
        # ÌÇ§ÏõåÎìú Ïö∞ÏÑ† (Í∞ÄÏû• ÌïµÏã¨Ï†Å)
        for kw in change_hint.get("keywords", []) or []:
            if isinstance(kw, str) and kw.strip():
                parts.append(kw.strip())
        # ÏàòÏπò Î≥ÄÍ≤Ω Ï†ïÎ≥¥ Ï∂îÍ∞Ä
        for num_change in change_hint.get("numerical_changes", []) or []:
            for key in ("new_value", "field"):
                val = num_change.get(key)
                if isinstance(val, str) and val.strip():
                    parts.append(val.strip())
        # ÏµúÎåÄ 5Í∞ú ÌÜ†ÌÅ∞ÏúºÎ°ú Ï†úÌïú (Í≥ºÎèÑÌïú ÏøºÎ¶¨ Î∞©ÏßÄ)
        return " ".join(parts[:5]) if parts else None

    def _merge_candidate_lists(
        self,
        base: List[RetrievedChunk],
        extra: List[RetrievedChunk],
    ) -> List[RetrievedChunk]:
        """
        chunk_id Í∏∞Ï§ÄÏúºÎ°ú Î≥ëÌï©ÌïòÎ©∞ Îçî ÎÜíÏùÄ semantic_scoreÎ•º Ïú†ÏßÄÌïúÎã§.
        """
        merged: Dict[str, RetrievedChunk] = {}
        for cand in base + extra:
            cid = cand.get("chunk_id")
            if cid in merged:
                if (cand.get("semantic_score") or 0) > (
                    merged[cid].get("semantic_score") or 0
                ):
                    merged[cid] = cand
            else:
                merged[cid] = cand
        return list(merged.values())

    def _choose_change_hint(self, change_scope: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        actionable = change_scope.get("actionable_results") or []
        pending = change_scope.get("pending_results") or []
        if actionable:
            return actionable[0]
        if pending:
            return pending[0]
        return None

    def _build_trace_entries(
        self,
        mapping_results: List[MappingItem],
        regulation_meta: Dict[str, Any],
    ) -> List[Dict[str, Any]]:
        now_ts = datetime.utcnow().isoformat() + "Z"
        entries: List[Dict[str, Any]] = []
        for item in mapping_results:
            if not item.get("applies"):
                continue
            rerank_meta = item.get("regulation_meta", {}).get("rerank", {}) or {}
            change_status = (
                "pending" if rerank_meta.get("pending") else "applied"
            )
            entries.append(
                {
                    "feature": item.get("feature_name"),
                    "applied_value": item.get("required_value"),
                    "regulation_record_id": item.get("regulation_chunk_id"),
                    "mapping_score": rerank_meta.get("final_confidence")
                    or item.get("regulation_meta", {}).get("semantic_score"),
                    "change_status": change_status,
                    "evidence": {
                        "legacy_snippet": None,
                        "new_snippet": item.get("regulation_summary"),
                    },
                    "regulation_info": {
                        "country": regulation_meta.get("country"),
                        "citation_code": regulation_meta.get("citation_code"),
                        "title": regulation_meta.get("title"),
                        "effective_date": regulation_meta.get("effective_date"),
                        "regulation_id": regulation_meta.get("regulation_id"),
                    },
                    "updated_at": now_ts,
                }
            )
        return entries

    def _rule_rank_candidates(
        self,
        candidates: List[RetrievedChunk],
        change_hint: Optional[Dict[str, Any]],
        top_n: int = 3,
    ) -> List[RetrievedChunk]:
        """
        Í∑úÏπô Í∏∞Î∞ò Ïä§ÏΩîÏñ¥Î°ú ÏÉÅÏúÑ ÌõÑÎ≥¥ Ï∂îÎ¶º.
        - semantic_score Ïö∞ÏÑ†
        - change keywords, numerical_change ÌÖçÏä§Ìä∏ Îß§Ïπ≠Ïóê Í∞ÄÏ†ê
        """
        if not candidates:
            return []

        keywords = set()
        numbers = set()
        if change_hint:
            for kw in change_hint.get("keywords", []) or []:
                if isinstance(kw, str):
                    keywords.add(self._normalize_token(kw))
            for num_entry in change_hint.get("numerical_changes", []) or []:
                for key in ("legacy_value", "new_value"):
                    val = num_entry.get(key)
                    if isinstance(val, str):
                        numbers.add(val.lower())

        def score(cand: RetrievedChunk) -> float:
            base = cand.get("semantic_score") or 0.0
            text = (cand.get("chunk_text") or "").lower()
            bonus = 0.0
            for kw in keywords:
                if kw in text:
                    bonus += 0.05
            for num in numbers:
                if num and num in text:
                    bonus += 0.05
            return base + bonus

        ranked = sorted(candidates, key=score, reverse=True)
        return ranked[:top_n]

    def _build_rerank_prompt(
        self,
        change_hint: Dict[str, Any],
        candidates: List[RetrievedChunk],
    ) -> str:
        """
        rerank + Î≥ÄÍ≤Ω ÏöîÏïΩ + ÏöîÍµ¨ÏÇ¨Ìï≠ Ï∂îÏ∂úÏùÑ Ìïú Î≤àÏóê ÏàòÌñâÌïòÎèÑÎ°ù LLM ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±.
        """
        evidence = {
            "change_type": change_hint.get("change_type"),
            "confidence_score": change_hint.get("confidence_score"),
            "new_snippet": change_hint.get("new_snippet")
            or change_hint.get("new_text")
            or change_hint.get("new_ref_text"),
            "legacy_snippet": change_hint.get("legacy_snippet")
            or change_hint.get("legacy_text")
            or change_hint.get("legacy_ref_text"),
            "keywords": change_hint.get("keywords", []),
            "numerical_changes": change_hint.get("numerical_changes", []),
        }
        cand_payload = []
        for idx, cand in enumerate(candidates):
            cand_payload.append(
                {
                    "id": cand.get("chunk_id"),
                    "text": cand.get("chunk_text"),
                    "metadata": cand.get("metadata", {}),
                    "semantic_score": cand.get("semantic_score"),
                }
            )

        prompt = {
            "task": "select_best_point_and_summarize_change",
            "change_evidence": evidence,
            "candidates": cand_payload,
            "instructions": (
                "1) ÌõÑÎ≥¥ Ï§ë Î≥ÄÌôîÏôÄ Í∞ÄÏû• Ïûò ÎßûÎäî point_idÎ•º 1Í∞ú ÏÑ†ÌÉù.\n"
                "2) Î¨¥ÏóáÏù¥ Ïñ¥ÎñªÍ≤å Î∞îÎÄåÏóàÎäîÏßÄ Ìïú Ï§ÑÎ°ú ÏöîÏïΩ.\n"
                "3) Ï°∞Ìï≠ ÎÇ¥ ÏöîÍµ¨ÏÇ¨Ìï≠ÏùÑ bulletÎ°ú ÎÇòÏó¥.\n"
                "4) ÏµúÏ¢Ö Ïã†Î¢∞ÎèÑ 0~1 ÏÇ∞Ï∂ú. 0.7 ÎØ∏ÎßåÏù¥Î©¥ pending=true."
            ),
            "output_schema": {
                "selected_point_id": "string",
                "reason": "string",
                "change_summary": "string",
                "requirements": ["string"],
                "final_confidence": "float",
                "pending": "boolean",
            },
        }
        return json.dumps(prompt, ensure_ascii=False)

    async def _rerank_candidates(
        self,
        change_hint: Dict[str, Any],
        candidates: List[RetrievedChunk],
    ) -> Optional[Dict[str, Any]]:
        if not candidates:
            return None
        prompt = self._build_rerank_prompt(change_hint, candidates)
        try:
            res = await self.llm.chat.completions.create(
                model="gpt-5-nano",
                messages=[{"role": "user", "content": prompt}],
            )
            return json.loads(res.choices[0].message.content)
        except Exception:
            return None

    async def _run_search(
        self,
        product: ProductInfo,
        feature_name: str,
        feature_value: Any,
        feature_unit: str | None,
        extra_filters: Optional[Dict[str, Any]] = None,
        change_query: Optional[str] = None,
    ) -> RetrievalResult:
        import asyncio

        product_id = product["product_id"]
        base_query = self._build_search_query(feature_name, feature_value, feature_unit)
        
        # Í∞úÏÑ†: Change queryÎ•º Î≥ÑÎèÑ Í≤ÄÏÉâÌïòÏßÄ ÏïäÍ≥† Í≤∞Ìï© (1Ìöå Í≤ÄÏÉâ)
        if change_query:
            combined_query = f"{base_query} {change_query}"
        else:
            combined_query = base_query
        
        filters = build_product_filters(product)
        if extra_filters:
            filters.update(extra_filters)

        async def _search_once(q: str) -> RetrievalOutput:
            return await self.search_tool.search(
                query=q,
                strategy="hybrid",
                top_k=self.top_k,
                alpha=self.alpha,
                filters=filters or None,
            )

        async def _search_with_retry(q: str) -> Optional[RetrievalOutput]:
            for attempt in range(3):
                try:
                    return await _search_once(q)
                except Exception as exc:
                    if attempt < 2:
                        backoff = 0.5 * (attempt + 1)
                        logger.warning(
                            "retrieval tool Ïã§Ìå® retry=%d query=%s err=%s",
                            attempt + 1,
                            q,
                            exc,
                        )
                        await asyncio.sleep(backoff)
                    else:
                        logger.warning("retrieval tool ÏµúÏ¢Ö Ïã§Ìå® query=%s err=%s", q, exc)
                        return None
            return None

        # run the combined query (base + change hint) once; retry on transient failures
        tool_result = await _search_with_retry(combined_query)

        if tool_result is None:
            return RetrievalResult(
                product_id=product_id,
                feature_name=feature_name,
                feature_value=feature_value,
                feature_unit=feature_unit,
                candidates=[],
            )

        def _convert(out: RetrievalOutput) -> List[RetrievedChunk]:
            converted: List[RetrievedChunk] = []
            for item in out["results"]:
                converted.append(
                    RetrievedChunk(
                        chunk_id=item.get("id", ""),
                        chunk_text=item.get("text", ""),
                        semantic_score=item.get("scores", {}).get("final_score", 0.0),
                        metadata=item.get("metadata", {}),
                    )
                )
            return converted

        candidates = _convert(tool_result)

        return RetrievalResult(
            product_id=product_id,
            feature_name=feature_name,
            feature_value=feature_value,
            feature_unit=feature_unit,
            candidates=candidates,
        )

    def _build_prompt(
        self,
        feature_name,
        present_value,
        target_value,
        feature_unit,
        chunk_text,
    ):
        feature = {
            "name": feature_name,
            "present_value": present_value,
            "target_value": target_value,
            "unit": feature_unit,
        }
        feature_json = json.dumps(feature, ensure_ascii=False)
        return MAPPING_PROMPT.replace("{feature}", feature_json).replace(
            "{chunk}", chunk_text
        )

    def _build_search_query(self, feature_name, feature_value, feature_unit):
        parts: List[str] = [str(feature_name)]
        if feature_value is not None:
            parts.append(str(feature_value))
        if feature_unit:
            parts.append(feature_unit)

        return " ".join(parts)

    def _prune_candidates(
        self, candidates: List[RetrievedChunk]
    ) -> List[RetrievedChunk]:
        seen_chunks = set()
        doc_counts = defaultdict(int)
        pruned: List[RetrievedChunk] = []

        for cand in candidates:
            chunk_id = cand.get("chunk_id")
            if chunk_id in seen_chunks:
                continue
            meta = cand.get("metadata", {}) or {}
            doc_id = meta.get("meta_doc_id") or meta.get("doc_id")
            if doc_id:
                if doc_counts[doc_id] >= self.max_candidates_per_doc:
                    continue
                doc_counts[doc_id] += 1

            seen_chunks.add(chunk_id)
            pruned.append(cand)

        return pruned

    async def _call_llm(self, prompt: str) -> Dict:
        try:
            res = await self.llm.chat.completions.create(
                model="gpt-5-nano",
                messages=[{"role": "user", "content": prompt}],
            )
            return json.loads(res.choices[0].message.content)

        except Exception:
            return {
                "applies": False,
                "required_value": None,
                "current_value": None,
                "gap": None,
                "parsed": {
                    "category": None,
                    "requirement_type": "other",
                    "condition": None,
                },
            }

    async def run(self, state: Dict) -> Dict:
        product: Optional[ProductInfo] = state.get("product_info")
        mapping_filters: Dict[str, Any] = state.get("mapping_filters") or {}
        regulation_meta: Dict[str, Any] = state.get("regulation") or {}
        change_results: List[Dict[str, Any]] = (
            state.get("change_detection_results") or []
        )
        if not product:
            product_id = mapping_filters.get("product_id")
            async with AsyncSessionLocal() as session:
                product = await self.product_repository.fetch_product_for_mapping(
                    session, int(product_id) if product_id is not None else None
                )
            state["product_info"] = product

        product_id = product["product_id"]
        product_name = product.get("product_name", product.get("name", "unknown"))
        mapping_spec = product.get("mapping") or {}
        target_state = mapping_spec.get("target") or {}
        present_state = mapping_spec.get("present_state") or {}
        present_features = (
            present_state or target_state or product.get("features", {}) or {}
        )
        units = product.get("feature_units", {})

        change_scope = self._extract_change_scope(change_results, present_features)
        change_hint = self._choose_change_hint(change_scope)
        change_query = self._build_change_query(change_hint)
        recovered_hints: Set[str] = set()

        mapping_results: List[MappingItem] = []
        mapping_targets: Dict[str, Dict[str, Any]] = {}
        unknown_requirements: List[Dict[str, Any]] = []

        extra_search_filters = {
            key: value
            for key, value in mapping_filters.items()
            if key not in {"product_id"}
        }
        change_search_filters = self._build_change_filters(change_scope)
        regulation_filters = self._build_regulation_filters(regulation_meta)
        merged_search_filters = self._merge_filters(
            extra_search_filters, change_search_filters, regulation_filters
        )

        # ------------------------------------------------------
        # change_detection ÎÖ∏ÎìúÏóêÏÑú Î∞õÏùÄ ÌûåÌä∏ ÌôúÏö© (Ïã†Í∑ú Í∑úÏ†ú Î∂ÑÏÑù Í≤∞Í≥º)
        # ------------------------------------------------------
        regulation_hints = state.get("regulation_analysis_hints") or {}

        if self.debug_enabled:
            logger.info(
                "üß≠ Mapping start: product=%s name=%s features=%d top_k=%d alpha=%.2f",
                product_id,
                product_name,
                len(present_features),
                self.top_k,
                self.alpha,
            )
            logger.info(f"üìä change_results: {len(change_results)}Í∞ú")
            logger.info(f"üìä change_scope: actionable={len(change_scope.get('actionable_results', []))}, pending={len(change_scope.get('pending_results', []))}, feature_hints={len(change_scope.get('feature_hints', set()))}")
            logger.info(f"üìä regulation_hints: {bool(regulation_hints)}")
            if not present_features:
                logger.info(
                    "üí§ Îß§Ìïë ÎåÄÏÉÅ ÌäπÏÑ±Ïù¥ ÏóÜÏäµÎãàÎã§. mapping.present_stateÎÇò targetÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî."
                )
        if regulation_hints and not change_scope.get("feature_hints"):
            # Ïã†Í∑ú Í∑úÏ†ú Î∂ÑÏÑù Í≤∞Í≥ºÏóêÏÑú affected_areasÎ•º feature_hintsÎ°ú Î≥ÄÌôò
            affected_areas = regulation_hints.get("affected_areas", [])
            for area in affected_areas:
                normalized = self._normalize_token(area)
                for norm_name, raw_name in {
                    self._normalize_token(name): name for name in present_features.keys()
                }.items():
                    if normalized == norm_name or normalized in norm_name:
                        recovered_hints.add(raw_name)
            
            if self.debug_enabled:
                logger.info(f"üÜï Ïã†Í∑ú Í∑úÏ†ú ÌûåÌä∏ ÌôúÏö©: {len(recovered_hints)}Í∞ú feature Î≥µÍµ¨")

        feature_iterable, unknown_hints = self._select_features_for_mapping(
            present_features, change_scope, recovered_hints
        )
        if self.debug_enabled:
            logger.info(
                "üîé feature selection ‚Äî hints=%s recovered=%s selected=%d",
                list(change_scope.get("feature_hints") or []),
                list(recovered_hints),
                len(feature_iterable),
            )
        if unknown_hints:
            unknown_requirements.extend(
                [
                    {
                        "hint": hint,
                        "reason": "change_detection_hint_not_in_product_features",
                    }
                    for hint in unknown_hints
                ]
            )

        # üî• featureÎ≥ÑÎ°ú Í≤ÄÏÉâ TOOL ‚Üí Îß§Ìïë (Î≥ëÎ†¨ Ï≤òÎ¶¨)
        async def process_feature(feature_name: str, present_value: Any):
            unit = units.get(feature_name)
            target_value = target_state.get(feature_name)

            # a) Í≤ÄÏÉâ TOOL Ìò∏Ï∂ú
            if self.debug_enabled:
                logger.info(
                    "üîç Searching feature=%s value=%s unit=%s",
                    feature_name,
                    present_value,
                    unit or "-",
                )
            retrieval: RetrievalResult = await self._run_search(
                product,
                feature_name,
                present_value,
                unit,
                merged_search_filters,
                change_query=change_query,
            )
            original_count = len(retrieval["candidates"])
            retrieval["candidates"] = self._prune_candidates(retrieval["candidates"])
            pruned_count = len(retrieval["candidates"])
            if self.debug_enabled:
                logger.info(
                    "   ‚Ü≥ candidates=%d (pruned to %d)",
                    original_count,
                    pruned_count,
                )

            ranked_candidates = retrieval["candidates"]
            rerank_result: Optional[Dict[str, Any]] = None
            if change_hint and ranked_candidates:
                # Í∑úÏπô Í∏∞Î∞òÏúºÎ°ú ÏÉÅÏúÑ 3Í∞ú ÏÑ†ÌÉù
                ranked_candidates = self._rule_rank_candidates(
                    ranked_candidates, change_hint, top_n=3
                )
                # LLM rerankÎ°ú ÏµúÏ¢Ö 1Í∞ú ÏÑ†ÌÉù
                rerank_result = await self._rerank_candidates(
                    change_hint, ranked_candidates
                )
                if rerank_result and rerank_result.get("selected_point_id"):
                    selected_id = rerank_result["selected_point_id"]
                    ranked_candidates = [
                        cand for cand in ranked_candidates
                        if cand.get("chunk_id") == selected_id
                    ] or ranked_candidates

            # rerankÍ∞Ä ÏóÜÍ±∞ÎÇò Ïã§Ìå®Ìï¥ÎèÑ Ï§ëÎ≥µ Îß§ÌïëÏùÑ ÌîºÌïòÍ∏∞ ÏúÑÌï¥ ÏÉÅÏúÑ 1Í∞úÎßå ÏÇ¨Ïö©
            if ranked_candidates:
                ranked_candidates = ranked_candidates[:1]

            # b) LLM Îß§Ìïë ÏàòÌñâ (ÌõÑÎ≥¥Î≥Ñ Î≥ëÎ†¨)
            async def process_candidate(cand: RetrievedChunk):
                prompt = self._build_prompt(
                    feature_name,
                    present_value,
                    target_value,
                    unit,
                    cand["chunk_text"],
                )
                llm_out = await self._call_llm(prompt)

                parsed: MappingParsed = llm_out.get("parsed", {})
                required_value = llm_out.get("required_value")
                current_value = llm_out.get("current_value")
                if (
                    llm_out.get("applies")
                    and required_value is None
                    and target_value is not None
                ):
                    required_value = target_value
                if current_value is None and present_value is not None:
                    current_value = present_value

                regulation_meta = dict(cand.get("metadata") or {})
                regulation_meta["semantic_score"] = cand.get("semantic_score")
                change_matches = self._match_change_results_to_candidate(
                    change_scope, cand
                )
                if change_matches:
                    regulation_meta["change_detection_matches"] = change_matches
                if rerank_result:
                    regulation_meta["rerank"] = rerank_result

                return MappingItem(
                    product_id=product_id,
                    product_name=product_name,
                    feature_name=feature_name,
                    applies=llm_out["applies"],
                    required_value=required_value,
                    current_value=current_value,
                    gap=llm_out["gap"],
                    regulation_chunk_id=cand["chunk_id"],
                    regulation_summary=cand["chunk_text"][:120],
                    regulation_meta=regulation_meta,
                    parsed=parsed,
                )
                
            
            # ÌõÑÎ≥¥Î≥Ñ Î≥ëÎ†¨ Ï≤òÎ¶¨
            import asyncio
            candidate_results = await asyncio.gather(
                *[process_candidate(cand) for cand in ranked_candidates],
                return_exceptions=True
            )
            items: List[MappingItem] = []
            for r in candidate_results:
                if isinstance(r, Exception):
                    continue
                items.append(r)
                if self.debug_enabled:
                    logger.info(
                        "üß© applies=%s required=%s current=%s chunk=%s (%s)",
                        r["applies"],
                        r["required_value"],
                        r["current_value"],
                        r["regulation_chunk_id"],
                        r["feature_name"],
                    )
            return items
        
        # featureÎ≥Ñ Î≥ëÎ†¨ Ï≤òÎ¶¨
        import asyncio
        feature_results = await asyncio.gather(
            *[process_feature(fname, fval) for fname, fval in feature_iterable],
            return_exceptions=True
        )
        
        # Í≤∞Í≥º Î≥ëÌï©
        for result in feature_results:
            if isinstance(result, Exception):
                logger.error(f"‚ùå Feature Ï≤òÎ¶¨ Ïã§Ìå®: {result}")
                continue
            if isinstance(result, list):
                mapping_results.extend(result)
                for item in result:
                    if item["applies"]:
                        feature_name = item["feature_name"]
                        existing = mapping_targets.get(feature_name)
                        has_req = item.get("required_value") is not None
                        replace = False
                        if existing is None:
                            replace = True
                        elif existing.get("required_value") is None and has_req:
                            replace = True
                        if replace:
                            mapping_targets[feature_name] = {
                                "required_value": item.get("required_value"),
                                "chunk_id": item.get("regulation_chunk_id"),
                                "doc_id": item.get("regulation_meta", {}).get(
                                    "meta_doc_id"
                                ),
                            }

        # -----------------------------------------
        # c) Ï†ÑÏó≠ State ÏóÖÎç∞Ïù¥Ìä∏
        # -----------------------------------------
        # Îß§Ìïë Í≤∞Í≥º(required_value)Î•º product_info.mapping.targetÏóê Î∞òÏòÅÌï¥ Ïù¥ÌõÑ ÎÖ∏ÎìúÍ∞Ä Î∞îÎ°ú ÎπÑÍµêÌï† Ïàò ÏûàÍ≤å ÌïúÎã§.
        product_mapping = product.get("mapping") or {}
        updated_target = dict(product_mapping.get("target") or {})
        for fname, target_info in mapping_targets.items():
            required_value = target_info.get("required_value")
            if required_value is not None:
                updated_target[fname] = required_value
        product_mapping["target"] = updated_target
        product["mapping"] = product_mapping
        state["product_info"] = product

        mapping_payload = MappingResults(
            product_id=product_id,
            items=mapping_results,
            targets=mapping_targets,
            actionable_changes=change_scope.get("actionable_results", []),
            pending_changes=change_scope.get("pending_results", []),
            unknown_requirements=unknown_requirements,
        )
        state["mapping"] = mapping_payload
        state["mapping_results"] = mapping_payload
        # regulation_trace ÏóÖÎç∞Ïù¥Ìä∏ (in-memory)
        trace_entries = self._build_trace_entries(mapping_results, regulation_meta)
        if trace_entries:
            existing_trace = product.get("regulation_trace") or {}
            existing_list = existing_trace.get("trace") or []
            last_updated = trace_entries[0]["updated_at"]
            product["regulation_trace"] = {
                "trace": existing_list + trace_entries,
                "last_updated": last_updated,
            }
            state["product_info"] = product
        if self.debug_enabled:
            _log_mapping_preview(product_id, mapping_results)
            snapshot_path = _persist_mapping_snapshot(
                product,
                mapping_results,
                state,
                self.top_k,
                self.alpha,
            )
            if snapshot_path:
                state["mapping_debug"] = {
                    "snapshot_path": snapshot_path,
                    "total_items": len(mapping_results),
                }

        return state


_DEFAULT_LLM_CLIENT = None
_DEFAULT_PRODUCT_REPOSITORY: Optional[ProductRepository] = None
_DEFAULT_MAPPING_NODE: Optional[MappingNode] = None


def _get_default_llm_client():
    """AsyncOpenAI Ïã±Í∏ÄÌÜ§ÏùÑ Íµ¨ÏÑ±ÌïúÎã§."""
    global _DEFAULT_LLM_CLIENT
    if _DEFAULT_LLM_CLIENT is not None:
        return _DEFAULT_LLM_CLIENT

    if AsyncOpenAI is None:
        raise RuntimeError(
            "openai Ìå®ÌÇ§ÏßÄÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. `pip install openai` ÌõÑ Îã§Ïãú ÏãúÎèÑÌïòÏÑ∏Ïöî."
        )

    _DEFAULT_LLM_CLIENT = AsyncOpenAI()
    return _DEFAULT_LLM_CLIENT


def _get_default_product_repository() -> ProductRepository:
    """ÏàòÏ†ï: Repository ÏÉùÏÑ± Î∞©Ïãù Í∞ÑÏÜåÌôî"""
    global _DEFAULT_PRODUCT_REPOSITORY
    if _DEFAULT_PRODUCT_REPOSITORY is None:
        _DEFAULT_PRODUCT_REPOSITORY = ProductRepository()
    return _DEFAULT_PRODUCT_REPOSITORY


def _build_mapping_node(
    *,
    llm_client=None,
    search_tool=None,
    top_k: Optional[int] = None,
    alpha: Optional[float] = None,
    product_repository: Optional[ProductRepository] = None,
    max_candidates_per_doc: int = 2,
) -> MappingNode:
    """MappingNode Ïù∏Ïä§ÌÑ¥Ïä§Î•º ÏÉùÏÑ±ÌïúÎã§."""
    resolved_llm = llm_client or _get_default_llm_client()
    resolved_top_k = top_k if top_k is not None else settings.MAPPING_TOP_K
    resolved_alpha = alpha if alpha is not None else settings.MAPPING_ALPHA
    resolved_repo = product_repository or _get_default_product_repository()
    return MappingNode(
        llm_client=resolved_llm,
        search_tool=search_tool,
        top_k=resolved_top_k,
        alpha=resolved_alpha,
        product_repository=resolved_repo,
        max_candidates_per_doc=max_candidates_per_doc,
    )


def _get_default_mapping_node() -> MappingNode:
    """ÌååÏù¥ÌîÑÎùºÏù∏ Ï†ÑÏö© Í∏∞Î≥∏ MappingNode."""
    global _DEFAULT_MAPPING_NODE
    if _DEFAULT_MAPPING_NODE is None:
        _DEFAULT_MAPPING_NODE = _build_mapping_node()
    return _DEFAULT_MAPPING_NODE


async def map_products_node(state: AppState) -> AppState:
    """
    LangGraph entrypoint wrapping MappingNode.

    state["mapping_context"]Î•º ÌÜµÌï¥ ÌÖåÏä§Ìä∏/ÌäπÏàò Ïã§Ìñâ Ïãú LLM ÎòêÎäî ToolÏùÑ Ï£ºÏûÖÌï† Ïàò ÏûàÎã§.
    """

    context: MappingContext = state.get("mapping_context", {}) or {}
    has_override = any(
        key in context
        for key in (
            "llm_client",
            "search_tool",
            "top_k",
            "alpha",
            "max_candidates_per_doc",
        )
    )
    if has_override:
        node = _build_mapping_node(
            llm_client=context.get("llm_client"),
            search_tool=context.get("search_tool"),
            top_k=context.get("top_k"),
            alpha=context.get("alpha"),
            max_candidates_per_doc=context.get("max_candidates_per_doc", 2),
        )
    else:
        node = _get_default_mapping_node()

    return await node.run(state)


__all__ = ["MappingNode", "map_products_node"]


def _log_mapping_preview(product_id: str, items: List[MappingItem]) -> None:
    max_items = max(1, settings.MAPPING_DEBUG_MAX_ITEMS)
    preview = items[:max_items]
    if not preview:
        logger.info("üì≠ Mapping produced no items for product=%s", product_id)
        return

    logger.info("üìí Mapping preview (showing %d/%d items):", len(preview), len(items))
    for idx, item in enumerate(preview, 1):
        logger.info(
            "  %d) feature=%s applies=%s required=%s current=%s chunk=%s",
            idx,
            item["feature_name"],
            item["applies"],
            item["required_value"],
            item["current_value"],
            item["regulation_chunk_id"],
        )


def _persist_mapping_snapshot(
    product: ProductInfo,
    items: List[MappingItem],
    state: AppState,
    top_k: int,
    alpha: float,
) -> Optional[str]:
    if not settings.MAPPING_DEBUG_DIR:
        return None

    target_dir = Path(settings.MAPPING_DEBUG_DIR)
    try:
        target_dir.mkdir(parents=True, exist_ok=True)
    except Exception as exc:  # pragma: no cover - disk trouble
        logger.warning("Failed to create mapping debug dir: %s", exc)
        return None

    product_id = product["product_id"]
    timestamp = datetime.utcnow().strftime("%Y%m%d-%H%M%S")
    doc_id = None
    preprocess_results = state.get("preprocess_results") or []
    if preprocess_results:
        doc_id = preprocess_results[0].get("doc_id")
    doc_suffix = doc_id or "unknown-doc"
    filename = f"{timestamp}_{product_id}_{doc_suffix}.json"
    snapshot_path = target_dir / filename

    payload = {
        "generated_at": datetime.utcnow().isoformat(),
        "product": product,
        "preprocess_summary": state.get("preprocess_summary"),
        "mapping_config": {
            "top_k": top_k,
            "alpha": alpha,
        },
        "total_items": len(items),
        "items": items,
    }

    snapshot_path.write_text(
        json.dumps(
            payload,
            ensure_ascii=False,
            indent=2,
            default=_json_safe_encoder,
        ),
        encoding="utf-8",
    )
    logger.info("üìù Mapping snapshot saved: %s", snapshot_path)
    return str(snapshot_path)


def _json_safe_encoder(value: Any):
    if isinstance(value, Decimal):
        return float(value)
    raise TypeError(f"Object of type {type(value).__name__} is not JSON serializable")
