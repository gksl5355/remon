# app/ai_pipeline/nodes/hitl.py
"""
HITL(Human-In-The-Loop) í†µí•© ë…¸ë“œ

ê¸°ëŠ¥:
1) intent ë¶„ë¥˜ (question/modification)
2) question: ë‹µë³€ë§Œ ì œê³µ (íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰ ì—†ìŒ)
3) modification: target_node ì‹ë³„ + í”¼ë“œë°± ì •ì œ + íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰
4) DB ì¤‘ê°„ ê²°ê³¼ë¬¼ í™œìš©
5) LangGraph ë‚´ report ì´í›„ì— ìœ„ì¹˜í•˜ëŠ” hitl ë…¸ë“œ
"""

import os
import json
import logging
import re
from typing import Dict, Any
from datetime import datetime

from openai import OpenAI
from app.ai_pipeline.state import AppState

# Import prompts for refined prompt generation
from app.ai_pipeline.prompts.mapping_prompt import MAPPING_PROMPT, MAPPING_SCHEMA
from app.ai_pipeline.prompts.strategy_prompt import STRATEGY_PROMPT, STRATEGY_SCHEMA
from app.ai_pipeline.prompts.impact_prompt import IMPACT_PROMPT, IMPACT_SCHEMA
from app.ai_pipeline.prompts.refined_prompt import REFINED_PROMPT

logger = logging.getLogger(__name__)
client = OpenAI()

OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# ============================================================
# 1) Intent Classification (Question vs Modification)
# ============================================================

INTENT_CLASSIFICATION_PROMPT = """
ë‹¹ì‹ ì€ REMONì˜ Intent ë¶„ë¥˜ê¸°ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì…ë ¥ì„ 2ê°€ì§€ë¡œ ë¶„ë¥˜í•˜ì‹­ì‹œì˜¤:

1. **question**: ê²°ê³¼ì— ëŒ€í•œ ì§ˆë¬¸/ì„¤ëª… ìš”ì²­
   - ì˜ˆì‹œ: "ì´ ê²°ê³¼ê°€ ë­ì•¼?", "ì™œ ì´ë ‡ê²Œ ë‚˜ì™”ì–´?", "ì˜í–¥ë„ê°€ ë­”ë°?"
   - ì˜ˆì‹œ: "ì´ ë§¤í•‘ì€ ì–´ë–»ê²Œ ëœ ê±°ì•¼?", "ì „ëµì´ ì´í•´ê°€ ì•ˆ ë¼"
   - ì˜ˆì‹œ: "ë³€ê²½ ê°ì§€ê°€ ë­ì•¼?", "ì´ ì ìˆ˜ëŠ” ì–´ë–»ê²Œ ê³„ì‚°ëœ ê±°ì•¼?"

2. **modification**: íŒŒì´í”„ë¼ì¸ ìˆ˜ì • ìš”ì²­
   - ì˜ˆì‹œ: "ë§¤í•‘ì„ ê³ ì³ì¤˜", "ì˜í–¥ë„ë¥¼ ë‚®ì¶°ì¤˜", "ì „ëµì„ ë‹¤ì‹œ ë§Œë“¤ì–´ì¤˜"
   - ì˜ˆì‹œ: "ë³€ê²½ ê°ì§€ë¥¼ ë‹¤ì‹œ í•´ì¤˜", "ì œí’ˆì„ ë°”ê¿”ì¤˜", "ë‹¤ì‹œ ë¶„ì„í•´ì¤˜"

ì¶œë ¥(JSON):
{
  "intent": "question" | "modification",
  "confidence": 0.0~1.0,
  "reasoning": "ë¶„ë¥˜ ì´ìœ  (í•œê¸€)"
}
"""

def classify_intent(message: str) -> Dict[str, Any]:
    """ì‚¬ìš©ì ë©”ì‹œì§€ â†’ intent ë¶„ë¥˜ (question/modification)"""
    try:
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": INTENT_CLASSIFICATION_PROMPT},
                {"role": "user", "content": message},
            ],
            temperature=0,
        )
        raw = resp.choices[0].message.content.strip()
        
        result = json.loads(raw)
        return {
            "intent": result.get("intent", "question"),
            "confidence": result.get("confidence", 0.5),
            "reasoning": result.get("reasoning", "")
        }
    except Exception as e:
        logger.warning(f"Intent ë¶„ë¥˜ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ question ì‚¬ìš©")
        return {"intent": "question", "confidence": 0.0, "reasoning": "parsing_error"}


# ============================================================
# 2) Question Answering (íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰ ì—†ìŒ)
# ============================================================

async def answer_question(state: AppState, question: str) -> str:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰ ì—†ìŒ)
    
    DBì™€ stateì—ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•˜ì—¬ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
    """
    context_parts = []
    
    # 1) ê·œì œ ì •ë³´
    regulation = state.get("regulation", {})
    if regulation:
        context_parts.append(f"ê·œì œëª…: {regulation.get('title', 'N/A')}")
        context_parts.append(f"êµ­ê°€: {regulation.get('country', 'N/A')}")
        context_parts.append(f"ì¸ìš© ì½”ë“œ: {regulation.get('citation_code', 'N/A')}")
    
    # 2) ë³€ê²½ ê°ì§€ ê²°ê³¼
    change_summary = state.get("change_summary", {})
    if change_summary:
        total = change_summary.get("total_changes", 0)
        high_conf = change_summary.get("high_confidence_changes", 0)
        context_parts.append(f"ë³€ê²½ ê°ì§€: ì´ {total}ê°œ ë³€ê²½ (ê³ ì‹ ë¢°ë„ {high_conf}ê°œ)")
    
    change_results = state.get("change_detection_results", [])
    if change_results:
        context_parts.append("\nì£¼ìš” ë³€ê²½ ì‚¬í•­:")
        for idx, result in enumerate(change_results[:3], 1):
            if result.get("change_detected"):
                section = result.get("section_ref", 'Unknown')
                change_type = result.get("change_type", 'N/A')
                context_parts.append(f"  {idx}. {section}: {change_type}")
    
    # 3) ë§¤í•‘ ê²°ê³¼
    mapping = state.get("mapping", {})
    if mapping:
        items = mapping.get("items", [])
        product_name = mapping.get("product_name", "Unknown")
        context_parts.append(f"\në§¤í•‘ ê²°ê³¼ ({product_name}): {len(items)}ê°œ í•­ëª©")
        
        # ì£¼ìš” ë§¤í•‘ í•­ëª© (applies=Trueë§Œ, ìµœëŒ€ 5ê°œ)
        applies_items = [item for item in items if item.get("applies")]
        for idx, item in enumerate(applies_items[:5], 1):
            feature = item.get("feature_name", 'N/A')
            current = item.get("current_value", '-')
            required = item.get("required_value", '-')
            context_parts.append(f"  {idx}. {feature}: {current} â†’ {required}")
    
    # 4) ì˜í–¥ë„
    impact_scores = state.get("impact_scores", [])
    if impact_scores:
        impact = impact_scores[0]
        level = impact.get("impact_level", 'N/A')
        score = impact.get("weighted_score", 0.0)
        reasoning = impact.get("reasoning", '')[:200]
        context_parts.append(f"\nì˜í–¥ë„: {level} (ì ìˆ˜: {score:.2f})")
        if reasoning:
            context_parts.append(f"ê·¼ê±°: {reasoning}...")
    
    # 5) ì „ëµ
    strategies = state.get("strategies", [])
    if strategies:
        context_parts.append(f"\nëŒ€ì‘ ì „ëµ:")
        for idx, strategy in enumerate(strategies[:3], 1):
            context_parts.append(f"  {idx}. {strategy[:150]}...")
    
    context = "\n".join(context_parts)
    
    # LLM ë‹µë³€ ìƒì„±
    prompt = f"""ë‹¹ì‹ ì€ REMON ê·œì œ ë¶„ì„ ì‹œìŠ¤í…œì˜ ì„¤ëª… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

**ì»¨í…ìŠ¤íŠ¸**:
{context}

**ì‚¬ìš©ì ì§ˆë¬¸**:
{question}

**ë‹µë³€ ê·œì¹™**:
1. í•œê¸€ë¡œ ë‹µë³€ (ê³ ìœ ëª…ì‚¬, ìˆ˜ì¹˜, ë²•ë ¹ ì¡°í•­, êµ­ê°€ ì½”ë“œëŠ” ì›ë¬¸ ìœ ì§€)
2. 3-5ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ
3. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ "í•´ë‹¹ ì •ë³´ê°€ ë¶„ì„ ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œ
4. ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…
5. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ì˜ˆì‹œë¥¼ í¬í•¨
"""
    
    try:
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ê·œì œ ë¶„ì„ ê²°ê³¼ë¥¼ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
        )
        answer = resp.choices[0].message.content.strip()
        logger.info(f"âœ… ì§ˆë¬¸ ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(answer)} chars")
        return answer
    except Exception as e:
        logger.error(f"âŒ ì§ˆë¬¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."


# ============================================================
# 3) Target Node Detection (Modificationìš©)
# ============================================================

TARGET_NODE_PROMPT = """
ë‹¹ì‹ ì€ REMONì˜ HITL target_node ë¶„ë¥˜ê¸°ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ìˆ˜ì •í•˜ë ¤ëŠ” íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë¥¼ ì‹ë³„í•˜ì‹­ì‹œì˜¤:

- change_detection: ë³€ê²½ ê°ì§€ ê´€ë ¨
- map_products: ì œí’ˆ ë§¤í•‘ ê´€ë ¨  
- generate_strategy: ì „ëµ ìƒì„± ê´€ë ¨
- score_impact: ì˜í–¥ë„ ì ìˆ˜ ê´€ë ¨

ì¶œë ¥(JSON):
{
  "target_node": "change_detection" | "map_products" | "generate_strategy" | "score_impact"
}
"""

def detect_target_node(message: str) -> str:
    """ì‚¬ìš©ì ë©”ì‹œì§€ â†’ target_node"""
    resp = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": TARGET_NODE_PROMPT},
            {"role": "user", "content": message},
        ],
        temperature=0,
    )
    raw = resp.choices[0].message.content.strip()

    try:
        result = json.loads(raw)
        return result.get("target_node", "map_products")
    except Exception:
        return "map_products"


# ============================================================
# 2) Feedback Cleaning
# ============================================================

CHANGE_FEEDBACK_PROMPT = """
ì‚¬ìš©ìì˜ ë©”ì‹œì§€ê°€ ì˜ë¯¸í•˜ëŠ” ë³€ê²½ ê°ì§€ ê²°ê³¼ë¥¼ íŒë‹¨í•˜ì‹­ì‹œì˜¤.

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì‹­ì‹œì˜¤:

{ "manual_change": true }   â† ë³€ê²½ ìˆìŒìœ¼ë¡œ ì²˜ë¦¬
ë˜ëŠ”
{ "manual_change": false }  â† ë³€ê²½ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
"""

IMPACT_LEVEL_FEEDBACK_PROMPT = """
ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ë¶„ì„í•´ì„œ ì›í•˜ëŠ” ì˜í–¥ë„ ë ˆë²¨ì„ íŒë‹¨í•˜ì‹­ì‹œì˜¤.

ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²ƒ:
- ë‚®ì¶”ê³  ì‹¶ë‹¤ë©´: Low
- ë†’ì´ê³  ì‹¶ë‹¤ë©´: High  
- ë³´í†µ/ì ë‹¹íˆ/ì¡°ê¸ˆë§Œ ìˆ˜ì •í•˜ê³  ì‹¶ë‹¤ë©´: Medium

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì‹­ì‹œì˜¤:

{ "desired_level": "Low" | "Medium" | "High" }
"""

STRATEGY_STYLE_FEEDBACK_PROMPT = """
ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ë¶„ì„í•´ì„œ ì›í•˜ëŠ” ì „ëµ ìŠ¤íƒ€ì¼ì„ íŒë‹¨í•˜ì‹­ì‹œì˜¤.

ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²ƒ:
- ë³´ìˆ˜ì /ì•ˆì „í•˜ê²Œ/ì‹ ì¤‘í•˜ê²Œ: conservative
- ì ê·¹ì /ê³µê²©ì /ë¹ ë¥´ê²Œ: aggressive
- ë‹¨ê³„ì /ì ì§„ì /ì°¨ê·¼ì°¨ê·¼: gradual
- ê°„ë‹¨í•˜ê²Œ/í•µì‹¬ë§Œ/ìµœì†Œí•œ: minimal
- ìì„¸í•˜ê²Œ/ë§ì´/êµ¬ì²´ì ìœ¼ë¡œ: detailed

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì‹­ì‹œì˜¤:

{ "strategy_style": "conservative" | "aggressive" | "gradual" | "minimal" | "detailed" | "default" }
"""

def refine_hitl_feedback(message: str, target_node: str) -> str:
    """
    ë…¸ë“œ íƒ€ì…ì— ë”°ë¼ í”¼ë“œë°± ì •ì œ

    - change_detection: "true" / "false" ë¬¸ìì—´ë¡œ ì •ì œ
    - score_impact: "Low" / "Medium" / "High" ë ˆë²¨ë¡œ ì •ì œ
    - ë‚˜ë¨¸ì§€ ë…¸ë“œ: ìì—°ì–´ í”¼ë“œë°± í•œ ë¬¸ì¥ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    """

    if target_node == "change_detection":
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": CHANGE_FEEDBACK_PROMPT},
                {"role": "user", "content": message},
            ],
            temperature=0,
        )
        raw = resp.choices[0].message.content.strip()
        try:
            data = json.loads(raw)
            flag = bool(data.get("manual_change", False))
            return "true" if flag else "false"
        except Exception:
            return "false"
    
    elif target_node == "score_impact":
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": IMPACT_LEVEL_FEEDBACK_PROMPT},
                {"role": "user", "content": message},
            ],
            temperature=0,
        )
        raw = resp.choices[0].message.content.strip()
        try:
            data = json.loads(raw)
            return data.get("desired_level", "Medium")
        except Exception:
            return "Medium"
    
    elif target_node == "generate_strategy":
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": STRATEGY_STYLE_FEEDBACK_PROMPT},
                {"role": "user", "content": message},
            ],
            temperature=0,
        )
        raw = resp.choices[0].message.content.strip()
        try:
            data = json.loads(raw)
            return data.get("strategy_style", "default")
        except Exception:
            return "default"

    # map_products â†’ ê·¸ëƒ¥ ìì—°ì–´ ì‚¬ìš©
    return message.strip()


# ============================================================
# 3) Apply HITL â†’ Patch State + call validator
# ============================================================

def generate_refined_prompt(node_name: str, pipeline_state: dict, error_summary: str):
    """Generate a refined version of the original prompt for a specific node."""
    
    if node_name == "map_products":
        original_prompt = MAPPING_PROMPT
        schema = MAPPING_SCHEMA
    elif node_name == "generate_strategy":
        original_prompt = STRATEGY_PROMPT
        schema = STRATEGY_SCHEMA
    elif node_name == "score_impact":
        original_prompt = IMPACT_PROMPT
        schema = IMPACT_SCHEMA
        # score_impact ì „ìš©: ìˆ«ì ì¶œë ¥ ê°•ì œ
        error_summary += "\n\nCRITICAL REQUIREMENT: All score values MUST be plain NUMBERS (1-5), NOT objects or nested structures.\n" + \
                        "CORRECT: 'directness': 3, 'legal_severity': 4\n" + \
                        "WRONG: 'directness': {'score': 3}, 'directness': {'value': 3, 'reason': '...'}\n" + \
                        "OUTPUT ONLY FLAT JSON with number values. NO nested objects allowed."
    else:
        logger.error(f"[HITL] Unknown node for refinement: {node_name}")
        return None

    # ğŸ†• ì¤‘ê°„ ê²°ê³¼ë¬¼ ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
    intermediate_context = ""
    intermediate_data = pipeline_state.get("intermediate_data")
    if intermediate_data:
        if node_name == "change_detection" and "change_detection" in intermediate_data:
            prev_data = intermediate_data["change_detection"]
            intermediate_context = f"\n\n[PREVIOUS CHANGE DETECTION RESULTS]\n"
            intermediate_context += f"Total changes detected: {len(prev_data.get('change_detection_results', []))}\n"
            intermediate_context += f"Summary: {prev_data.get('change_summary', {})}\n"
        elif node_name == "map_products" and "map_products" in intermediate_data:
            prev_data = intermediate_data["map_products"]
            intermediate_context = f"\n\n[PREVIOUS MAPPING RESULTS]\n"
            intermediate_context += f"Total items: {len(prev_data.get('mapping', {}).get('items', []))}\n"
            intermediate_context += f"Product: {prev_data.get('product_info', {}).get('product_name', 'Unknown')}\n"
    
    error_summary += intermediate_context

    # score_impactëŠ” ê°„ë‹¨í•œ override í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    if node_name == "score_impact" and "Force impact_level to" in error_summary:
        # ì§ì ‘ override í”„ë¡¬í”„íŠ¸ ìƒì„± (REFINED_PROMPT ìš°íšŒ)
        refine_request = f"""{error_summary}

Original Prompt:
{original_prompt.strip()}

You MUST include the exact phrase "Force impact_level to 'High'" (or Low/Medium) in your rewritten prompt.
This phrase is used for automated detection.

Rewrite the prompt to enforce the human override while maintaining the original structure.
"""
    else:
        refine_request = REFINED_PROMPT.format(
            original_prompt=original_prompt.strip(),
            error_summary=error_summary,
            pipeline_state=json.dumps(pipeline_state, ensure_ascii=False, indent=2),
            schema=json.dumps(schema, ensure_ascii=False, indent=2),
        )

    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "You rewrite prompts to be strict and error-proof.",
                },
                {"role": "user", "content": refine_request},
            ],
            temperature=0,
        )
        refined_prompt_text = resp.choices[0].message.content.strip()
        return refined_prompt_text

    except Exception as e:
        logger.error(f"[HITL] Failed to generate refined prompt: {e}")
        return None


async def apply_hitl_patch(state: AppState, target_node: str, cleaned_feedback: str) -> AppState:
    """
    HITL í”¼ë“œë°±ì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ (validator ì˜ì¡´ì„± ì œê±°)
    + DBì—ì„œ ì¤‘ê°„ ê²°ê³¼ë¬¼ ë¡œë“œ ë° ì¬í™œìš©
    """
    
    logger.info(f"[HITL] Processing feedback for {target_node}: {cleaned_feedback}")
    
    # ğŸ†• DBì—ì„œ ì¤‘ê°„ ê²°ê³¼ë¬¼ ë¡œë“œ
    regulation_id = state.get("regulation", {}).get("regulation_id")
    intermediate_data = None
    
    if regulation_id:
        from app.core.repositories.intermediate_output_repository import IntermediateOutputRepository
        from app.core.database import AsyncSessionLocal
        
        async with AsyncSessionLocal() as session:
            intermediate_repo = IntermediateOutputRepository()
            try:
                intermediate_data = await intermediate_repo.get_intermediate(
                    session,
                    regulation_id=regulation_id
                )
                if intermediate_data:
                    logger.info(f"âœ… ì¤‘ê°„ ê²°ê³¼ë¬¼ ë¡œë“œ ì„±ê³µ: regulation_id={regulation_id}")
                    logger.info(f"   ë…¸ë“œ: {list(intermediate_data.keys())}")
            except Exception as db_err:
                logger.error(f"âŒ ì¤‘ê°„ ê²°ê³¼ë¬¼ ë¡œë“œ ì‹¤íŒ¨: {db_err}")
    
    # compiled_inputì— DB ë°ì´í„° ë³‘í•©
    compiled_input = {
        "mapping": state.get("mapping"),
        "strategies": state.get("strategies"),
        "impact": state.get("impact_scores"),
        "regulation": state.get("regulation"),
        "intermediate_data": intermediate_data,  # ğŸ†• DB ë°ì´í„° ì¶”ê°€
    }
    
    # ===============================
    # change_detection ì „ìš© HITL
    # ===============================
    if target_node == "change_detection":
        # ë¬¸ìì—´("true"/"false") ì²˜ë¦¬
        if isinstance(cleaned_feedback, str):
            cleaned = cleaned_feedback.strip().lower()
            manual_flag = cleaned == "true"
        else:
            manual_flag = bool(cleaned_feedback)

        state["manual_change_flag"] = manual_flag
        state["needs_embedding"] = manual_flag

        logger.info(
            f"[HITL][change_detection] "
            f"manual_change_flag set to {manual_flag}, needs_embedding={manual_flag}"
        )

        if not manual_flag:  # ë³€ê²½ ì—†ìŒì¼ ë•Œ - change_detection.pyì™€ ë™ì¼í•œ ë¡œì§
            # change_detection.pyì™€ ë™ì¼í•œ "ë³€ê²½ ì—†ìŒ" ìƒíƒœ ì„¤ì •
            state["change_detection_results"] = []
            state["change_summary"] = {
                "status": "manual_no_change",
                "total_changes": 0,
                "high_confidence_changes": 0,
                "total_reference_blocks": 0,
            }
            state["change_detection_index"] = {}
            state["regulation_analysis_hints"] = {}
            
            logger.info("[HITL][change_detection] ë³€ê²½ ì—†ìŒ ìƒíƒœ ì§ì ‘ ì„¤ì • ì™„ë£Œ (ì¬ì‹¤í–‰ ë¶ˆí•„ìš”)")
            # ì¬ì‹¤í–‰ ë¶ˆí•„ìš” - ì´ë¯¸ ì™„ë£Œëœ ìƒíƒœë¡œ ì„¤ì •
        else:
            # ë³€ê²½ ìˆìŒì¼ ë•Œë§Œ ì´ˆê¸°í™” í›„ ì¬ì‹¤í–‰
            for key in [
                "change_detection_results",
                "change_summary",
                "regulation_analysis_hints",
                "change_detection_index",
            ]:
                if key in state:
                    state[key] = None

            state["restarted_node"] = "change_detection"
            logger.info("[HITL][change_detection] ë³€ê²½ ìˆìŒ - ì¬ì‹¤í–‰ ì„¤ì •")
        
    # ===============================
    # ë‚˜ë¨¸ì§€ ë…¸ë“œë“¤ HITL
    # ===============================
    else:
        # ëª¨ë“  ë…¸ë“œì— ëŒ€í•´ error_summary ìƒì„±
        if target_node == "score_impact":
            desired_level = cleaned_feedback
            error_summary = (
                f"**CRITICAL OVERRIDE INSTRUCTION**\n\n"
                f"MANDATORY: Force impact_level to '{desired_level}' and reasoning to 'Human in the loop'.\n\n"
                f"This is a HUMAN-IN-THE-LOOP correction. You MUST:\n"
                f"1. Set impact_level = '{desired_level}' (ignore calculated scores)\n"
                f"2. Set reasoning = 'Human in the loop'\n"
                f"3. All raw_scores must be plain numbers (1-5), NOT objects\n\n"
                f"Example CORRECT output:\n"
                f"{{\n"
                f"  \"directness\": 3,\n"
                f"  \"legal_severity\": 4,\n"
                f"  \"reasoning\": \"Human in the loop\"\n"
                f"}}\n"
            )
            logger.info(f"[HITL] Processing score_impact feedback: {desired_level}")
        else:
            error_summary = f"HUMAN FEEDBACK: {cleaned_feedback}. INSTRUCTION: Adjust the analysis according to this feedback."

        # ë…¸ë“œë³„ ê´€ë ¨ state ì´ˆê¸°í™” (ëˆ„ì  ë°©ì§€)
        if target_node == "generate_strategy":
            state["strategies"] = None
            logger.info(f"[HITL] Cleared existing strategies for regeneration")
        elif target_node == "map_products":
            state["mapping"] = None
            state["product_info"] = None
            logger.info(f"[HITL] Cleared existing mapping and product_info for regeneration")
        elif target_node == "score_impact":
            state["impact_scores"] = None
            logger.info(f"[HITL] Cleared existing impact scores for regeneration")
        
        # refined prompt ìƒì„±
        refined_key = f"refined_{target_node}_prompt"
        try:
            refined_prompt = generate_refined_prompt(
                node_name=target_node,
                pipeline_state=compiled_input,
                error_summary=error_summary,
            )

            if refined_prompt:
                state[refined_key] = refined_prompt
                logger.info(f"[HITL] âœ… Refined prompt ìƒì„± ì„±ê³µ: {refined_key}")
                logger.info(f"[HITL] í”„ë¡¬í”„íŠ¸ ë‚´ìš©: {refined_prompt[:300]}...")
            else:
                logger.error(f"[HITL] âŒ Refined prompt ìƒì„± ì‹¤íŒ¨: {target_node}")
        except Exception as e:
            logger.error(f"[HITL] âŒ Refined prompt ìƒì„± ì˜ˆì™¸: {target_node}: {e}")

        # ì¬ì‹œì‘ ë…¸ë“œ ì„¤ì •
        state["restarted_node"] = target_node
        logger.info(f"[HITL] Set restart node to: {target_node}")
    
    # ğŸ†• ì¤‘ê°„ ê²°ê³¼ë¬¼ì„ stateì— ë³µì› (ì¬ì‹¤í–‰ ì‹œ í™œìš©)
    if intermediate_data and target_node in ["change_detection", "map_products"]:
        node_data = intermediate_data.get(target_node)
        if node_data:
            if target_node == "change_detection":
                # ë³€ê²½ ê°ì§€ ê²°ê³¼ ë³µì› (ì°¸ê³ ìš©)
                state["_hitl_previous_change_detection"] = node_data
                logger.info("âœ… ì´ì „ ë³€ê²½ ê°ì§€ ê²°ê³¼ ë³µì› (ì°¸ê³ ìš©)")
            elif target_node == "map_products":
                # ë§¤í•‘ ê²°ê³¼ ë³µì› (ì°¸ê³ ìš©)
                state["_hitl_previous_mapping"] = node_data
                logger.info("âœ… ì´ì „ ë§¤í•‘ ê²°ê³¼ ë³µì› (ì°¸ê³ ìš©)")
    
    # HITL ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
    state["hitl_target_node"] = None
    state["hitl_feedback_text"] = None
    state.pop("hitl_feedback", None)
    
    return state


# ============================================================
# 4) LangGraph HITL ë…¸ë“œ (report ì´í›„)
# ============================================================

async def hitl_node(state: AppState) -> AppState:
    """
    LangGraphì—ì„œ report ì´í›„ í˜¸ì¶œë˜ëŠ” HITL ë…¸ë“œ.
    
    Intent ë¶„ë¥˜:
    - question: ë‹µë³€ë§Œ ì œê³µ (íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰ ì—†ìŒ)
    - modification: íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰
    
    DBì—ì„œ ì¤‘ê°„ ê²°ê³¼ë¬¼ ë¡œë“œ ë° ì¬í™œìš©
    """
    
    user_msg = state.get("external_hitl_feedback")
    
    if not user_msg:
        logger.info("[HITL Node] external_hitl_feedback ì—†ìŒ â†’ ì¢…ë£Œ")
        return state
    
    logger.info(f"[HITL Node] ì‚¬ìš©ì ì…ë ¥ ìˆ˜ì‹ : {user_msg}")
    
    # í”¼ë“œë°± ì œê±° (ë¬´í•œ ë£¨í”„ ë°©ì§€)
    state["external_hitl_feedback"] = None
    
    # ğŸ†• Intent ë¶„ë¥˜
    intent_result = classify_intent(user_msg)
    intent = intent_result["intent"]
    confidence = intent_result["confidence"]
    reasoning = intent_result["reasoning"]
    
    logger.info(
        f"[HITL Intent] {intent} (confidence: {confidence:.2f}) - {reasoning}"
    )
    
    # ğŸ”¹ Intentë³„ ì²˜ë¦¬
    if intent == "question":
        # ì§ˆë¬¸ ì²˜ë¦¬: ë‹µë³€ë§Œ ìƒì„± (íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰ ì—†ìŒ)
        logger.info("[HITL Question] ë‹µë³€ ìƒì„± ì‹œì‘...")
        answer = await answer_question(state, user_msg)
        
        # ë‹µë³€ì„ stateì— ì €ì¥ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í‘œì‹œ)
        state["hitl_answer"] = {
            "question": user_msg,
            "answer": answer,
            "intent": "question",
            "confidence": confidence,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        logger.info(f"[HITL Question] ë‹µë³€ ìƒì„± ì™„ë£Œ: {answer[:100]}...")
        
        # ì¬ì‹¤í–‰ ì—†ìŒ
        state["restarted_node"] = None
        
    elif intent == "modification":
        # ìˆ˜ì • ì²˜ë¦¬: ê¸°ì¡´ ë¡œì§ (íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰)
        logger.info("[HITL Modification] íŒŒì´í”„ë¼ì¸ ìˆ˜ì • ì‹œì‘...")
        
        # (1) target_node ì‹ë³„
        target = detect_target_node(user_msg)
        logger.info(f"[HITL Target] target_node = {target}")
        
        # (2) í”¼ë“œë°± ì •ì œ
        cleaned = refine_hitl_feedback(user_msg, target)
        
        # ğŸ” ì›ë³¸ ë©”ì‹œì§€ ì €ì¥ (ë””ë²„ê¹…ìš©)
        state["_hitl_original_message"] = user_msg
        
        # (3) state íŒ¨ì¹˜ (ë…ë¦½ì  ì²˜ë¦¬ + DB ë¡œë“œ)
        new_state = await apply_hitl_patch(state, target, cleaned)
        
        logger.info(
            f"[HITL Modification] ì²˜ë¦¬ ì™„ë£Œ â†’ "
            f"restarted_node={new_state.get('restarted_node')}"
        )
        
        return new_state
    
    return state
