# app/ai_pipeline/nodes/hitl.py
"""
HITL(Human-In-The-Loop) í†µí•© ë…¸ë“œ

ê¸°ëŠ¥:
1) intent(hitl/general) ë¶„ë¥˜
2) target_node ì‹ë³„
3) í”¼ë“œë°± ì •ì œ
4) state íŒ¨ì¹˜(hitl_target_node, hitl_feedback_text)
5) validator_node í˜¸ì¶œ â†’ ì¬ì‹œì‘ ë…¸ë“œ ê²°ì •
6) LangGraph ë‚´ report ì´í›„ì— ìœ„ì¹˜í•˜ëŠ” hitl ë…¸ë“œ
"""

import os
import json
import logging
import re
from typing import Dict, Any

from openai import OpenAI
from app.ai_pipeline.state import AppState

# Import prompts for refined prompt generation
from app.ai_pipeline.prompts.mapping_prompt import MAPPING_PROMPT, MAPPING_SCHEMA
from app.ai_pipeline.prompts.strategy_prompt import STRATEGY_PROMPT, STRATEGY_SCHEMA
from app.ai_pipeline.prompts.impact_prompt import IMPACT_PROMPT, IMPACT_SCHEMA
from app.ai_pipeline.prompts.refined_prompt import REFINED_PROMPT

logger = logging.getLogger(__name__)
client = OpenAI()

OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# ============================================================
# 1) Intent Detection
# ============================================================

TARGET_NODE_PROMPT = """
ë‹¹ì‹ ì€ REMONì˜ HITL target_node ë¶„ë¥˜ê¸°ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ìˆ˜ì •í•˜ë ¤ëŠ” íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë¥¼ ì‹ë³„í•˜ì‹­ì‹œì˜¤:

- change_detection: ë³€ê²½ ê°ì§€ ê´€ë ¨
- map_products: ì œí’ˆ ë§¤í•‘ ê´€ë ¨  
- generate_strategy: ì „ëµ ìƒì„± ê´€ë ¨
- score_impact: ì˜í–¥ë„ ì ìˆ˜ ê´€ë ¨

ì¶œë ¥(JSON):
{
  "target_node": "change_detection" | "map_products" | "generate_strategy" | "score_impact"
}
"""

def detect_target_node(message: str) -> str:
    """ì‚¬ìš©ì ë©”ì‹œì§€ â†’ target_node"""
    resp = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": TARGET_NODE_PROMPT},
            {"role": "user", "content": message},
        ],
        temperature=0,
    )
    raw = resp.choices[0].message.content.strip()

    try:
        result = json.loads(raw)
        return result.get("target_node", "map_products")
    except Exception:
        return "map_products"


# ============================================================
# 2) Feedback Cleaning
# ============================================================

CHANGE_FEEDBACK_PROMPT = """
ì‚¬ìš©ìì˜ ë©”ì‹œì§€ê°€ ì˜ë¯¸í•˜ëŠ” ë³€ê²½ ê°ì§€ ê²°ê³¼ë¥¼ íŒë‹¨í•˜ì‹­ì‹œì˜¤.

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì‹­ì‹œì˜¤:

{ "manual_change": true }   â† ë³€ê²½ ìˆìŒìœ¼ë¡œ ì²˜ë¦¬
ë˜ëŠ”
{ "manual_change": false }  â† ë³€ê²½ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
"""

IMPACT_LEVEL_FEEDBACK_PROMPT = """
ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ë¶„ì„í•´ì„œ ì›í•˜ëŠ” ì˜í–¥ë„ ë ˆë²¨ì„ íŒë‹¨í•˜ì‹­ì‹œì˜¤.

ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²ƒ:
- ë‚®ì¶”ê³  ì‹¶ë‹¤ë©´: Low
- ë†’ì´ê³  ì‹¶ë‹¤ë©´: High  
- ë³´í†µ/ì ë‹¹íˆ/ì¡°ê¸ˆë§Œ ìˆ˜ì •í•˜ê³  ì‹¶ë‹¤ë©´: Medium

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì‹­ì‹œì˜¤:

{ "desired_level": "Low" | "Medium" | "High" }
"""

STRATEGY_STYLE_FEEDBACK_PROMPT = """
ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ë¶„ì„í•´ì„œ ì›í•˜ëŠ” ì „ëµ ìŠ¤íƒ€ì¼ì„ íŒë‹¨í•˜ì‹­ì‹œì˜¤.

ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²ƒ:
- ë³´ìˆ˜ì /ì•ˆì „í•˜ê²Œ/ì‹ ì¤‘í•˜ê²Œ: conservative
- ì ê·¹ì /ê³µê²©ì /ë¹ ë¥´ê²Œ: aggressive
- ë‹¨ê³„ì /ì ì§„ì /ì°¨ê·¼ì°¨ê·¼: gradual
- ê°„ë‹¨í•˜ê²Œ/í•µì‹¬ë§Œ/ìµœì†Œí•œ: minimal
- ìì„¸í•˜ê²Œ/ë§ì´/êµ¬ì²´ì ìœ¼ë¡œ: detailed

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì‹­ì‹œì˜¤:

{ "strategy_style": "conservative" | "aggressive" | "gradual" | "minimal" | "detailed" | "default" }
"""

def refine_hitl_feedback(message: str, target_node: str) -> str:
    """
    ë…¸ë“œ íƒ€ì…ì— ë”°ë¼ í”¼ë“œë°± ì •ì œ

    - change_detection: "true" / "false" ë¬¸ìì—´ë¡œ ì •ì œ
    - score_impact: "Low" / "Medium" / "High" ë ˆë²¨ë¡œ ì •ì œ
    - ë‚˜ë¨¸ì§€ ë…¸ë“œ: ìì—°ì–´ í”¼ë“œë°± í•œ ë¬¸ì¥ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    """

    if target_node == "change_detection":
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": CHANGE_FEEDBACK_PROMPT},
                {"role": "user", "content": message},
            ],
            temperature=0,
        )
        raw = resp.choices[0].message.content.strip()
        try:
            data = json.loads(raw)
            flag = bool(data.get("manual_change", False))
            return "true" if flag else "false"
        except Exception:
            return "false"
    
    elif target_node == "score_impact":
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": IMPACT_LEVEL_FEEDBACK_PROMPT},
                {"role": "user", "content": message},
            ],
            temperature=0,
        )
        raw = resp.choices[0].message.content.strip()
        try:
            data = json.loads(raw)
            return data.get("desired_level", "Medium")
        except Exception:
            return "Medium"
    
    elif target_node == "generate_strategy":
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": STRATEGY_STYLE_FEEDBACK_PROMPT},
                {"role": "user", "content": message},
            ],
            temperature=0,
        )
        raw = resp.choices[0].message.content.strip()
        try:
            data = json.loads(raw)
            return data.get("strategy_style", "default")
        except Exception:
            return "default"

    # map_products â†’ ê·¸ëƒ¥ ìì—°ì–´ ì‚¬ìš©
    return message.strip()


# ============================================================
# 3) Apply HITL â†’ Patch State + call validator
# ============================================================

def generate_refined_prompt(node_name: str, pipeline_state: dict, error_summary: str):
    """Generate a refined version of the original prompt for a specific node."""
    
    if node_name == "map_products":
        original_prompt = MAPPING_PROMPT
        schema = MAPPING_SCHEMA
    elif node_name == "generate_strategy":
        original_prompt = STRATEGY_PROMPT
        schema = STRATEGY_SCHEMA
    elif node_name == "score_impact":
        original_prompt = IMPACT_PROMPT
        schema = IMPACT_SCHEMA
        # score_impact ì „ìš©: ìˆ«ì ì¶œë ¥ ê°•ì œ
        error_summary += "\n\nCRITICAL REQUIREMENT: All score values MUST be plain NUMBERS (1-5), NOT objects or nested structures.\n" + \
                        "CORRECT: 'directness': 3, 'legal_severity': 4\n" + \
                        "WRONG: 'directness': {'score': 3}, 'directness': {'value': 3, 'reason': '...'}\n" + \
                        "OUTPUT ONLY FLAT JSON with number values. NO nested objects allowed."
    else:
        logger.error(f"[HITL] Unknown node for refinement: {node_name}")
        return None

    refine_request = REFINED_PROMPT.format(
        original_prompt=original_prompt.strip(),
        error_summary=error_summary,
        pipeline_state=json.dumps(pipeline_state, ensure_ascii=False, indent=2),
        schema=json.dumps(schema, ensure_ascii=False, indent=2),
    )

    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "You rewrite prompts to be strict and error-proof.",
                },
                {"role": "user", "content": refine_request},
            ],
            temperature=0,
        )
        refined_prompt_text = resp.choices[0].message.content.strip()
        return refined_prompt_text

    except Exception as e:
        logger.error(f"[HITL] Failed to generate refined prompt: {e}")
        return None


def apply_hitl_patch(state: AppState, target_node: str, cleaned_feedback: str) -> AppState:
    """
    HITL í”¼ë“œë°±ì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ (validator ì˜ì¡´ì„± ì œê±°)
    """
    
    logger.info(f"[HITL] Processing feedback for {target_node}: {cleaned_feedback}")
    
    compiled_input = {
        "mapping": state.get("mapping"),
        "strategies": state.get("strategies"),  # List[str] í˜•íƒœ ìœ ì§€
        "impact": state.get("impact_scores"),
        "regulation": state.get("regulation"),
    }
    
    # ===============================
    # change_detection ì „ìš© HITL
    # ===============================
    if target_node == "change_detection":
        # ë¬¸ìì—´("true"/"false") ì²˜ë¦¬
        if isinstance(cleaned_feedback, str):
            cleaned = cleaned_feedback.strip().lower()
            manual_flag = cleaned == "true"
        else:
            manual_flag = bool(cleaned_feedback)

        state["manual_change_flag"] = manual_flag
        state["needs_embedding"] = manual_flag

        logger.info(
            f"[HITL][change_detection] "
            f"manual_change_flag set to {manual_flag}, needs_embedding={manual_flag}"
        )

        if not manual_flag:  # ë³€ê²½ ì—†ìŒì¼ ë•Œ - change_detection.pyì™€ ë™ì¼í•œ ë¡œì§
            # change_detection.pyì™€ ë™ì¼í•œ "ë³€ê²½ ì—†ìŒ" ìƒíƒœ ì„¤ì •
            state["change_detection_results"] = []
            state["change_summary"] = {
                "status": "manual_no_change",
                "total_changes": 0,
                "high_confidence_changes": 0,
                "total_reference_blocks": 0,
            }
            state["change_detection_index"] = {}
            state["regulation_analysis_hints"] = {}
            
            logger.info("[HITL][change_detection] ë³€ê²½ ì—†ìŒ ìƒíƒœ ì§ì ‘ ì„¤ì • ì™„ë£Œ (ì¬ì‹¤í–‰ ë¶ˆí•„ìš”)")
            # ì¬ì‹¤í–‰ ë¶ˆí•„ìš” - ì´ë¯¸ ì™„ë£Œëœ ìƒíƒœë¡œ ì„¤ì •
        else:
            # ë³€ê²½ ìˆìŒì¼ ë•Œë§Œ ì´ˆê¸°í™” í›„ ì¬ì‹¤í–‰
            for key in [
                "change_detection_results",
                "change_summary",
                "regulation_analysis_hints",
                "change_detection_index",
            ]:
                if key in state:
                    state[key] = None

            state["restarted_node"] = "change_detection"
            logger.info("[HITL][change_detection] ë³€ê²½ ìˆìŒ - ì¬ì‹¤í–‰ ì„¤ì •")
        
    # ===============================
    # ë‚˜ë¨¸ì§€ ë…¸ë“œë“¤ HITL
    # ===============================
    else:
        # ëª¨ë“  ë…¸ë“œì— ëŒ€í•´ refined prompt ìƒì„±
        if target_node == "score_impact":
            desired_level = cleaned_feedback
            error_summary = f"CRITICAL INSTRUCTION: Force impact_level to '{desired_level}' and reasoning to 'Human in the loop'.\n" + \
                           "CRITICAL: All raw_scores values must be plain numbers (1-5), not objects. Example: 'directness': 3"
            logger.info(f"[HITL] Processing score_impact feedback: {desired_level}")
        else:
            # map_products, generate_strategyëŠ” ìì—°ì–´ ê·¸ëŒ€ë¡œ
            error_summary = f"HUMAN FEEDBACK: {cleaned_feedback}. INSTRUCTION: Adjust the analysis according to this feedback."

        # ì´ì „ refined prompt ì™„ì „ ì œê±° (ìƒˆ HITL í”¼ë“œë°± ë°˜ì˜ì„ ìœ„í•´)
        refined_key = f"refined_{target_node}_prompt"
        if refined_key in state:
            del state[refined_key]
            logger.info(f"[HITL] Removed previous refined prompt for {target_node}")
        
        # ë…¸ë“œë³„ ê´€ë ¨ state ì´ˆê¸°í™” (ëˆ„ì  ë°©ì§€)
        if target_node == "generate_strategy":
            state["strategies"] = None  # ê¸°ì¡´ ì „ëµ ì´ˆê¸°í™”
            logger.info(f"[HITL] Cleared existing strategies for regeneration")
        elif target_node == "map_products":
            state["mapping"] = None  # ê¸°ì¡´ ë§¤í•‘ ì´ˆê¸°í™”
            state["product_info"] = None  # â­ ì¬ì‹œë„ ì‹œ ì œí’ˆ ì¬ì„ íƒ í—ˆìš©
            logger.info(f"[HITL] Cleared existing mapping and product_info for regeneration")
        elif target_node == "score_impact":
            state["impact_scores"] = None  # ê¸°ì¡´ ì˜í–¥ë„ ì´ˆê¸°í™”
            logger.info(f"[HITL] Cleared existing impact scores for regeneration")
        
        # refined prompt ìƒì„± (fallback ì²˜ë¦¬ ì¶”ê°€)
        try:
            refined_prompt = generate_refined_prompt(
                node_name=target_node,
                pipeline_state=compiled_input,
                error_summary=error_summary,
            )

            if refined_prompt:
                state[refined_key] = refined_prompt
                logger.info(f"[HITL] NEW refined prompt saved to state['{refined_key}']")
                logger.debug(f"[HITL] New refined prompt content: {refined_prompt[:200]}...")
            else:
                logger.error(f"[HITL] Failed to generate refined prompt for {target_node} â†’ fallback accept")
        except Exception as e:
            logger.error(f"[HITL] Refined prompt generation error for {target_node}: {e} â†’ fallback accept")

        # ì¬ì‹œì‘ ë…¸ë“œ ì„¤ì •
        state["restarted_node"] = target_node
        logger.info(f"[HITL] Set restart node to: {target_node}")
    
    # HITL ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
    state["hitl_target_node"] = None
    state["hitl_feedback_text"] = None
    state.pop("hitl_feedback", None)
    
    return state


# ============================================================
# 4) LangGraph HITL ë…¸ë“œ (report ì´í›„)
# ============================================================

def hitl_node(state: AppState) -> AppState:
    """
    LangGraphì—ì„œ report ì´í›„ í˜¸ì¶œë˜ëŠ” HITL ë…¸ë“œ.

    - ì™¸ë¶€ì—ì„œ ì‚¬ìš©ì í”¼ë“œë°±ì„ state["external_hitl_feedback"]ì— ë„£ì–´ ì¤€ë‹¤ê³  ê°€ì •
    - ëª¨ë“  ì…ë ¥ì„ HITL í”¼ë“œë°±ìœ¼ë¡œ ì²˜ë¦¬ (general ë¶„ë¥˜ ì œê±°)
    - target_node ì‹ë³„ + í”¼ë“œë°± ì •ì œ + state íŒ¨ì¹˜ê¹Œì§€ ìˆ˜í–‰
    - ì´í›„ validator_nodeê°€ HITL ëª¨ë“œë¡œ ì‹¤í–‰ë˜ë©° restarted_nodeë¥¼ ê²°ì •
    """

    user_msg = state.get("external_hitl_feedback")

    if not user_msg:
        logger.info("[HITL Node] external_hitl_feedback ì—†ìŒ â†’ ì•„ë¬´ ê²ƒë„ í•˜ì§€ ì•Šê³  ì¢…ë£Œ")
        return state

    logger.info(f"[HITL Node] ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì‹ : {user_msg}")

    # ğŸ”¹ í”¼ë“œë°± ì²˜ë¦¬ í›„ ì¦‰ì‹œ ì œê±° (ë¬´í•œ ë£¨í”„ ë°©ì§€)
    state["external_hitl_feedback"] = None
    logger.debug(f"[HITL] Cleared external_hitl_feedback after processing: {user_msg}")

    # (1) target_node ì‹ë³„
    target = detect_target_node(user_msg)
    logger.info(f"[HITL Target] target_node = {target}")

    # (2) í”¼ë“œë°± ì •ì œ
    cleaned = refine_hitl_feedback(user_msg, target)

    # (3) state íŒ¨ì¹˜ (ë…ë¦½ì  ì²˜ë¦¬)
    new_state = apply_hitl_patch(state, target, cleaned)

    logger.info(f"[HITL Node] ì²˜ë¦¬ ì™„ë£Œ â†’ restarted_node={new_state.get('restarted_node')}")
    return new_state
