"""
module: metadata_extractor_v2.py
description: ê·œì œ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ë„ë©”ì¸ íŠ¹í™” - ë‹´ë°° ê·œì œ + í•œêµ­ ë²•ë ¹)
             ë¯¸êµ­ ë‹´ë°° ê·œì œ(ì—°ë°©/ì£¼/ì§€ë°©ë²•) + í•œêµ­ ë²•ë ¹ ìë™ ê°ì§€
author: AI Agent
created: 2025-11-12
updated: 2025-11-12
dependencies:
    - re, json, logging, datetime, pathlib
"""

from typing import Optional, Dict, Any, List, Tuple, Union
import re
import json
import logging
from datetime import datetime
from pathlib import Path

# LangChain imports (optional)
try:
    from langchain_community.document_loaders import (
        PyPDFLoader, 
        TextLoader, 
        UnstructuredPDFLoader,
        UnstructuredWordDocumentLoader
    )
    from langchain.schema import Document
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    # í´ë°±ìš© ë”ë¯¸ í´ë˜ìŠ¤
    class Document:
        def __init__(self, page_content="", metadata=None):
            self.page_content = page_content
            self.metadata = metadata or {}

logger = logging.getLogger(__name__)

if not LANGCHAIN_AVAILABLE:
    logger.warning("LangChain not available. Using pattern-based extraction only.")


class RegulationPatterns:
    """ê·œì œ ë¬¸ì„œ íŒ¨í„´ ì§‘í•© (ë„ë©”ì¸: ë‹´ë°° ê·œì œ + í•œêµ­ ë²•ë ¹)"""
    
    # ==================== ë‹´ë°° ê´€ë ¨ í‚¤ì›Œë“œ ====================
    TOBACCO_KEYWORDS = {
        "tobacco", "cigarette", "smoking", "nicotine", "vape", "e-cigarette",
        "cigar", "pipe", "smokeless", "snuff", "chewing", "tobacco product",
        "ë‹´ë°°", "í¡ì—°", "ë‹´ë°°ì œí’ˆ", "ë‹ˆì½”í‹´", "ê¶ë ¨", "ìœ¡ì—°", "ì”¹ëŠ”ë‹´ë°°",
    }
    
    # ==================== ë¯¸êµ­ ê´€í• ê¶Œ íŒ¨í„´ ====================
    FEDERAL_INDICATORS = [
        r"(?:public\s+)?law\s+\d+[-â€“]\d+",  # Public Law 111-31
        r"congress(?:ional)?\b",
        r"(?:united\s+)?states\s+code\s*\(?\d+\s*(?:u\.?s\.?c\.?|usc)\)?",  # 15 USC
        r"\bstat\.\s+\d+",  # 123 STAT. 1776
        r"(?:senate|house)\s+bill",
        r"h\.?r\.?\s+\d+|s\.?\s+\d+",  # HR 1256, S 100
        r"federal\s+(?:statute|law|regulation|register)",
        r"(?:title\s+)?21\s+(?:cfr|code\s+of\s+federal\s+regulations)",
    ]
    
    STATE_INDICATORS = [
        r"(?:division|part|chapter|section|article)\s+\d+[.,]?\s*\d*",  # Division 8.6
        r"(?:california|florida|texas|new\s+york|pennsylvania|ohio|illinois)\b",
        r"(?:california\s+)?(?:business\s+[&and]\s+)?profession",
        r"state\s+(?:board|law|code|statute)",
        r"(?:bpc|california\s+business\s+and\s+professions\s+code)",
        r"\bca\s+(?:code|statute)",
    ]
    
    LOCAL_INDICATORS = [
        r"(?:san\s+francisco|los\s+angeles|new\s+york\s+city|chicago|seattle)",
        r"(?:city|county|municipal|township|ordinance)",
        r"(?:ordinance|municipal\s+code)\s+(?:no\.?|#|\d+)",
        r"health\s+(?:code|department|ordinance)",
    ]
    
    # ==================== ê·œì œê¸°ê´€ íŒ¨í„´ ====================
    REGULATORY_BODY_MAP = {
        "FDA": [
            r"(?:food\s+and\s+drug\s+administration|fda)",
            r"center\s+for\s+tobacco\s+products",
            r"ctp\b",
            r"(?:title\s+)?21\s+cfr",
        ],
        "State Board": [
            r"state\s+board(?:\s+of)?",
            r"board\s+of\s+(?:equalization|revenue|supervisors)",
            r"state\s+(?:health|revenue|regulatory|licensing)\s+(?:department|board)",
        ],
        "Local Health Dept": [
            r"(?:city|county|municipal|local)\s+(?:health|department)",
            r"health\s+(?:and\s+)?(?:safety|services|code)",
            r"department\s+of\s+(?:public\s+)?health",
        ],
    }
    
    # ==================== ë²•ì˜ ìœ í˜• íŒ¨í„´ ====================
    LAW_TYPE_MAP = {
        "statute": [
            r"(?:public\s+)?law\s+\d+[â€“-]\d+",
            r"statute\s+\d+",
            r"act(?:\s+of)?\s+\d{4}",
            r"ë²•\s+(?:ì œ\s*)?\d+í˜¸",  # í•œêµ­: ë²• XXí˜¸
        ],
        "code": [
            r"(?:\d+\s+)?(?:u\.?s\.?c\.?|usc)",  # 15 USC
            r"(?:california\s+)?(?:penal|health|business|professional|revenue|government)\s+code",
            r"california\s+business\s+and\s+professions\s+code",
            r"(?:bpc|code)\s+(?:section|Â§)",
        ],
        "regulation": [
            r"(?:federal\s+)?regulation",
            r"(?:title\s+)?21\s+cfr",
            r"code\s+of\s+federal\s+regulations",
            r"(?:state\s+)?regulation",
            r"ì‹œí–‰ë ¹|ì‹œí–‰ê·œì¹™",  # í•œêµ­
        ],
        "rule": [
            r"(?:proposed\s+)?rule(?:\s+\(cfr\))?",
            r"final\s+rule",
            r"ê·œì •|ê·œì¹™",  # í•œêµ­
        ],
        "notice": [
            r"(?:federal\s+)?register",
            r"notice(?:\s+of)?",
            r"proposed\s+(?:amendment|regulation|rule)",
            r"ê³µê³ |ê³ ì‹œ",  # í•œêµ­
        ],
    }
    
    # ==================== ë‚ ì§œ íŒ¨í„´ ====================
    DATE_FORMATS = [
        (r"(\d{4})[ë…„-](\d{1,2})[ì›”-](\d{1,2})[ì¼]?", "YMD_KO"),  # 2025ë…„1ì›”12ì¼
        (r"(\d{4})[/-](\d{1,2})[/-](\d{1,2})", "YMD"),  # 2025-01-12
        (r"(\d{1,2})[/-](\d{1,2})[/-](\d{4})", "DMY"),  # 12/01/2025
        (r"(?:january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\s+(\d{1,2}),?\s+(\d{4})",
         "MDA"),  # January 12, 2025
    ]


class MetadataExtractor:
    """
    ê·œì œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° (v2: ë„ë©”ì¸ íŠ¹í™”).
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. ìë™ ê´€í• ê¶Œ ê°ì§€ (federal/state/local/national)
    2. ìë™ ê·œì œê¸°ê´€ ê°ì§€ (FDA, State Board, Local Health Dept)
    3. ìë™ ë²•ì˜ ìœ í˜• ë¶„ë¥˜ (statute/code/regulation/rule/notice)
    4. ë‹´ë°° ê·œì œ ì „ë¬¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    5. í•œêµ­ ë²•ë ¹ í˜•ì‹ ì§€ì›
    
    ì¶”ì¶œ ë©”íƒ€ë°ì´í„°:
    - title: ë¬¸ì„œ ì œëª©
    - country: êµ­ê°€ ì½”ë“œ (KR, US)
    - jurisdiction: ê´€í• ê¶Œ (federal/state/local/national)
    - regulatory_body: ê·œì œê¸°ê´€ (FDA/State Board/Local Health Dept)
    - law_type: ë²•ì˜ ìœ í˜• (statute/code/regulation/rule/notice)
    - regulation_type: ê·œì œ ì¹´í…Œê³ ë¦¬ (tobacco_control/healthcare/etc)
    - publication_date: ë°œí‘œ ë‚ ì§œ
    - effective_date: ë°œíš¨ ë‚ ì§œ
    - keywords: í‚¤ì›Œë“œ (ë‹´ë°°, nicotine ë“±)
    - confidence: ì¶”ì¶œ ì‹ ë¢°ë„ (0.0~1.0)
    """
    
    def __init__(self, use_langchain: bool = True):
        """ì´ˆê¸°í™”.
        
        Args:
            use_langchain: LangChain DocumentLoader ì‚¬ìš© ì—¬ë¶€
        """
        self.patterns = RegulationPatterns()
        self.use_langchain = use_langchain and LANGCHAIN_AVAILABLE
        
        if self.use_langchain:
            logger.info("âœ… MetadataExtractor v2 initialized (LangChain + Pattern-based)")
        else:
            logger.info("âœ… MetadataExtractor v2 initialized (Pattern-based only)")
    
    def extract_metadata(
        self,
        document_text: str,
        filename: Optional[str] = None,
        source_url: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            document_text: ê·œì œ ë¬¸ì„œ í…ìŠ¤íŠ¸
            filename: ì›ë³¸ íŒŒì¼ëª… (ê´€í• ê¶Œ ê°ì§€ì— í™œìš©)
            source_url: ì¶œì²˜ URL
        
        Returns:
            Dict[str, Any]: ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°
            {
                "title": str,
                "country": str ("KR", "US", etc),
                "jurisdiction": str ("federal", "state", "local", "national"),
                "regulatory_body": str ("FDA", "State Board", "Local Health Dept"),
                "law_type": str ("statute", "code", "regulation", "rule", "notice"),
                "regulation_type": str ("tobacco_control"),
                "publication_date": str (ISO format),
                "effective_date": Optional[str],
                "keywords": List[str],
                "summary": str (ì²« 300ì),
                "confidence": float (0.0~1.0),
                "source_url": Optional[str],
                "filename": Optional[str],
                "extracted_at": str (ISO format),
            }
        """
        if not document_text or not document_text.strip():
            raise ValueError("Document text cannot be empty")
        
        # ê¸°ë³¸ ì¶”ì¶œ
        metadata = {
            "title": self._extract_title(document_text),
            "country": self._extract_country(document_text, filename),
            "jurisdiction": self._extract_jurisdiction(document_text, filename),
            "regulatory_body": self._extract_regulatory_body(document_text),
            "law_type": self._extract_law_type(document_text),
            "regulation_type": self._extract_regulation_type(document_text),
            "publication_date": self._extract_publication_date(document_text),
            "effective_date": self._extract_effective_date(document_text),
            "keywords": self._extract_keywords(document_text),
            "summary": self._extract_summary(document_text),
            "source_url": source_url,
            "filename": filename,
            "extracted_at": datetime.utcnow().isoformat() + "Z",
            "legal_hierarchy": self._extract_legal_hierarchy(document_text),
            "external_id": self._extract_external_id(filename or ""),
            "document_hash": self._calculate_document_hash(document_text),
        }
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        metadata["confidence"] = self._calculate_confidence(metadata)
        
        logger.info(
            f"âœ… Metadata extracted: title={metadata['title'][:50]}... "
            f"country={metadata['country']}, jurisdiction={metadata['jurisdiction']}, "
            f"confidence={metadata['confidence']:.2f}"
        )
        
        return metadata
    
    # ==================== ì¶”ì¶œ ë©”ì„œë“œ ====================
    
    def _extract_title(self, text: str) -> str:
        """ì œëª© ì¶”ì¶œ (UI í…ìŠ¤íŠ¸ í•„í„°ë§ ê°œì„ )."""
        lines = text.strip().split("\n")
        
        # UI/ì›¹í˜ì´ì§€ í…ìŠ¤íŠ¸ íŒ¨í„´ (ì œì™¸í•  ê²ƒë“¤)
        ui_patterns = [
            r"^(?:code|select|search|section|up|add|to|my|favorites)\s*:?",
            r"^[\d\s\|\-\+\=]+$",  # ìˆ«ì/ê¸°í˜¸ë§Œ
            r"^\s*[\"\']?case_id[\"\']?\s*:",  # JSON í‚¤
            r"^\s*\{|^\s*\[",  # JSON ì‹œì‘
        ]
        
        for line in lines[:15]:  # ë” ë§ì€ ì¤„ ê²€ì‚¬
            line = line.strip()
            
            # ê¸°ë³¸ í•„í„°
            if not line or len(line) < 5 or len(line) > 500:
                continue
            
            # UI íŒ¨í„´ ì œì™¸
            if any(re.search(p, line, re.IGNORECASE) for p in ui_patterns):
                continue
            
            # ìˆ«ì/ê¸°í˜¸ë§Œ ìˆëŠ” ì¤„ ì œì™¸
            if not re.search(r"[ê°€-í£a-zA-Z]", line):
                continue
            
            # ë²•ë¥  ì œëª© íŒ¨í„´ ìš°ì„  (ë” ì •í™•í•œ ì œëª©)
            if re.search(r"(?:public\s+law|act|ë²•ë¥ |ê·œì •|ê³ ì‹œ)", line, re.IGNORECASE):
                return line
            
            # ì¼ë°˜ì ì¸ ì œëª© ê¸¸ì´ ì²´í¬
            if 15 < len(line) < 200:  # ë²”ìœ„ ì¡°ì •
                return line
        
        return "ì œëª© ë¯¸í™•ì¸"
    
    def _extract_country(self, text: str, filename: Optional[str]) -> str:
        """êµ­ê°€ ì½”ë“œ ì¶”ì¶œ (ë¯¸êµ­ ê·œì œ ì „ìš©)."""
        text_lower = text.lower()
        
        # ë¯¸êµ­ ì§€í‘œ (ì ìˆ˜ ì‹œìŠ¤í…œ)
        us_score = sum([
            2 if re.search(r"united\s+states", text_lower) else 0,
            2 if re.search(r"congress", text_lower) else 0,
            2 if re.search(r"public\s+law\s+\d+[-â€“]\d+", text_lower) else 0,
            1 if re.search(r"\d+\s+u\.?s\.?c\.?", text_lower) else 0,
            1 if re.search(r"california|florida|texas|new\s+york", text_lower) else 0,
        ])
        
        # íŒŒì¼ëª… ë³´ì¡° ì ìˆ˜
        if filename:
            filename_lower = filename.lower()
            if "fda" in filename_lower or "congress" in filename_lower:
                us_score += 1
        
        return "US" if us_score >= 2 else "UNKNOWN"
    
    def _extract_jurisdiction(self, text: str, filename: Optional[str]) -> str:
        """ê´€í• ê¶Œ ì¶”ì¶œ (ê°œì„ ëœ ìš°ì„ ìˆœìœ„)."""
        text_lower = text.lower()
        
        # Local í™•ì¸ (ìš°ì„ ìˆœìœ„ ë†’ì„ - êµ¬ì²´ì  íŒ¨í„´)
        local_strong_patterns = [
            r"san\s+francisco", r"los\s+angeles", r"new\s+york\s+city",
            r"municipal\s+code", r"city\s+ordinance", r"county\s+health"
        ]
        if any(re.search(p, text_lower) for p in local_strong_patterns):
            return "local"
        
        # Federal í™•ì¸ (ê°•í•œ ì§€í‘œ)
        federal_strong_patterns = [
            r"public\s+law\s+\d+[-â€“]\d+", r"congress(?:ional)?",
            r"federal\s+register", r"\d+\s+u\.?s\.?c\.?"
        ]
        if any(re.search(p, text_lower) for p in federal_strong_patterns):
            return "federal"
        
        # State í™•ì¸
        if any(re.search(p, text_lower) for p in self.patterns.STATE_INDICATORS):
            return "state"
        
        # ì•½í•œ Federal íŒ¨í„´ (ë§ˆì§€ë§‰ ì²´í¬)
        if any(re.search(p, text_lower) for p in self.patterns.FEDERAL_INDICATORS):
            return "federal"
        
        return "unknown"
    
    def _extract_regulatory_body(self, text: str) -> Optional[str]:
        """ê·œì œê¸°ê´€ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„ ìˆ˜ì •ìœ¼ë¡œ ì˜¤ë§¤ì¹­ ë°©ì§€)."""
        text_lower = text.lower()
        
        # FDA ìš°ì„  í™•ì¸ (ê°€ì¥ êµ¬ì²´ì )
        fda_patterns = self.patterns.REGULATORY_BODY_MAP["FDA"]
        if any(re.search(p, text_lower) for p in fda_patterns):
            return "FDA"
        
        # State Board í™•ì¸
        state_patterns = self.patterns.REGULATORY_BODY_MAP["State Board"]
        if any(re.search(p, text_lower) for p in state_patterns):
            return "State Board"
        
        # Local Health Dept í™•ì¸ (ë§ˆì§€ë§‰, ê°€ì¥ ì¼ë°˜ì )
        local_patterns = self.patterns.REGULATORY_BODY_MAP["Local Health Dept"]
        if any(re.search(p, text_lower) for p in local_patterns):
            # ì—°ë°©ë²• ë¬¸ì„œì—ì„œëŠ” Local Health Dept ì œì™¸
            if re.search(r"public\s+law|congress|federal\s+register", text_lower):
                return None
            return "Local Health Dept"
        
        return None
    
    def _extract_law_type(self, text: str) -> str:
        """ë²•ì˜ ìœ í˜• ì¶”ì¶œ."""
        text_lower = text.lower()
        
        for law_type, patterns in self.patterns.LAW_TYPE_MAP.items():
            if any(re.search(p, text_lower) for p in patterns):
                return law_type
        
        return "regulation"  # ê¸°ë³¸ê°’
    
    def _extract_regulation_type(self, text: str) -> str:
        """ê·œì œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ."""
        text_lower = text.lower()
        
        # ë‹´ë°° ê´€ë ¨ ì—¬ë¶€ í™•ì¸
        if any(keyword in text_lower for keyword in self.patterns.TOBACCO_KEYWORDS):
            return "tobacco_control"
        
        # ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬
        if any(keyword in text_lower for keyword in ["healthcare", "medical", "device"]):
            return "healthcare"
        if any(keyword in text_lower for keyword in ["food", "safety"]):
            return "food_safety"
        if any(keyword in text_lower for keyword in ["environmental", "pollution"]):
            return "environment"
        
        return "general"
    
    def _extract_publication_date(self, text: str) -> Optional[str]:
        """ë°œí‘œ ë‚ ì§œ ì¶”ì¶œ."""
        # íŠ¹ì • íŒ¨í„´ ê²€ìƒ‰
        patterns = [
            r"(?:published|issued|enacted|ê³µí¬|ë°œí‘œ)(?:\s+on)?\s*[:\s]*([^\n]+)",
            r"(?:public\s+law\s+\d+[-â€“]\d+|bill\s+no\.?\s*\d+)?.*?(\d{4})",
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                date_str = self._normalize_date(match.group(1))
                if date_str:
                    return date_str
        
        # ë¬¸ì„œ ì²« ë¶€ë¶„ì—ì„œ ì²« ë²ˆì§¸ ë‚ ì§œ ì¶”ì¶œ
        first_match = re.search(r"(\d{4})[ë…„-](\d{1,2})[ì›”-](\d{1,2})|(\d{4})[/-](\d{1,2})[/-](\d{1,2})", text)
        if first_match:
            return self._normalize_date(first_match.group(0))
        
        return None
    
    def _extract_effective_date(self, text: str) -> Optional[str]:
        """ë°œíš¨ ë‚ ì§œ ì¶”ì¶œ."""
        patterns = [
            r"(?:effective|ì‹œí–‰|ë°œíš¨)(?:\s+on)?\s*[:\s]*([^\n]+)",
            r"(?:effective\s+date|ì‹œí–‰ì¼)\s*[:\s]*(\d{4}[/-]?\d{1,2}[/-]?\d{1,2})",
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                date_str = self._normalize_date(match.group(1))
                if date_str:
                    return date_str
        
        return None
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ."""
        keywords = []
        text_lower = text.lower()
        
        # ë‹´ë°° ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
        tobacco_keywords = [kw for kw in self.patterns.TOBACCO_KEYWORDS 
                           if kw in text_lower]
        keywords.extend(tobacco_keywords[:5])  # ìµœëŒ€ 5ê°œ
        
        # ì¶”ê°€ ë„ë©”ì¸ í‚¤ì›Œë“œ
        additional_keywords = {
            "warning": r"warning|ê²½ê³ |ì£¼ì˜",
            "label": r"label|ë¼ë²¨|í‘œì‹œ",
            "manufacturing": r"manufactur|ì œì¡°",
            "distribution": r"distribut|ë°°í¬",
            "advertising": r"advertis|ê´‘ê³ ",
            "prohibition": r"prohibit|ê¸ˆì§€",
        }
        
        for kw, pattern in additional_keywords.items():
            if re.search(pattern, text_lower):
                keywords.append(kw)
        
        return list(set(keywords))[:10]  # ì¤‘ë³µ ì œê±°, ìµœëŒ€ 10ê°œ
    
    def _extract_summary(self, text: str, max_length: int = 300) -> str:
        """ìš”ì•½ ì¶”ì¶œ (ì²« Nì)."""
        cleaned = re.sub(r"[\s]+", " ", text.strip())
        return cleaned[:max_length] + ("..." if len(cleaned) > max_length else "")
    
    def _normalize_date(self, date_str: str) -> Optional[str]:
        """ë‚ ì§œ ë¬¸ìì—´ì„ ISO í˜•ì‹ìœ¼ë¡œ ì •ê·œí™” (ê°œì„ ëœ ê²€ì¦)."""
        if not date_str:
            return None
        
        date_str = date_str.strip()
        
        # í•œêµ­ í˜•ì‹: 2025ë…„1ì›”12ì¼ â†’ 2025-01-12
        match = re.match(r"(\d{4})[ë…„-](\d{1,2})[ì›”-](\d{1,2})[ì¼]?", date_str)
        if match:
            year, month, day = match.groups()
            if self._is_valid_date(int(year), int(month), int(day)):
                return f"{year}-{int(month):02d}-{int(day):02d}"
        
        # í‘œì¤€ í˜•ì‹: 2025-01-12 or 2025/01/12
        match = re.match(r"(\d{4})[/-](\d{1,2})[/-](\d{1,2})", date_str)
        if match:
            year, month, day = match.groups()
            if self._is_valid_date(int(year), int(month), int(day)):
                return f"{year}-{int(month):02d}-{int(day):02d}"
        
        # ì˜ë¬¸ ì›”: January 12, 2025
        match = re.search(r"(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\w*\s+(\d{1,2}),?\s+(\d{4})", 
                         date_str, re.IGNORECASE)
        if match:
            month_str, day, year = match.groups()
            months = {
                "jan": 1, "feb": 2, "mar": 3, "apr": 4, "may": 5, "jun": 6,
                "jul": 7, "aug": 8, "sep": 9, "oct": 10, "nov": 11, "dec": 12,
            }
            month_num = months.get(month_str[:3].lower(), 1)
            if self._is_valid_date(int(year), month_num, int(day)):
                return f"{year}-{month_num:02d}-{int(day):02d}"
        
        return None
    
    def _is_valid_date(self, year: int, month: int, day: int) -> bool:
        """ë‚ ì§œ ìœ íš¨ì„± ê²€ì¦."""
        if year < 1900 or year > 2100:
            return False
        if month < 1 or month > 12:
            return False
        if day < 1 or day > 31:
            return False
        # ê°„ë‹¨í•œ ì›”ë³„ ì¼ìˆ˜ ì²´í¬
        days_in_month = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]
        return day <= days_in_month[month - 1]
    

    
    def _extract_legal_hierarchy(self, text: str) -> Optional[Dict[str, str]]:
        """ë²•ë¥  ê³„ì¸µ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ê°œì„ ëœ íŒ¨í„´ ë§¤ì¹­)."""
        hierarchy = {}
        
        # CFR ìš°ì„  í™•ì¸: 21 CFR Â§ 1160.10
        cfr_match = re.search(r'(\d+)\s+CFR\s+(?:Part\s+)?(\d+)(?:\.(\d+))?', text, re.IGNORECASE)
        if cfr_match:
            hierarchy['regulation_type'] = 'CFR'
            hierarchy['title'] = cfr_match.group(1)
            hierarchy['part'] = cfr_match.group(2)
            if cfr_match.group(3):
                hierarchy['section'] = f"{cfr_match.group(2)}.{cfr_match.group(3)}"
                hierarchy['full_citation'] = f"{cfr_match.group(1)} CFR Â§ {cfr_match.group(2)}.{cfr_match.group(3)}"
            else:
                hierarchy['section'] = cfr_match.group(2)
                hierarchy['full_citation'] = f"{cfr_match.group(1)} CFR Part {cfr_match.group(2)}"
            return hierarchy
        
        # USC í™•ì¸: 21 U.S.C. Â§ 387
        usc_match = re.search(r'(\d+)\s+U\.?S\.?C\.?\s+Â§?\s*(\d+)', text, re.IGNORECASE)
        if usc_match:
            hierarchy['regulation_type'] = 'USC'
            hierarchy['title'] = usc_match.group(1)
            hierarchy['section'] = usc_match.group(2)
            hierarchy['full_citation'] = f"{usc_match.group(1)} U.S.C. Â§ {usc_match.group(2)}"
            return hierarchy
        
        # Public Law í™•ì¸: Public Law 111-31
        publaw_match = re.search(r'public\s+law\s+(\d+)[-â€“](\d+)', text, re.IGNORECASE)
        if publaw_match:
            hierarchy['regulation_type'] = 'Public Law'
            hierarchy['congress'] = publaw_match.group(1)
            hierarchy['law_number'] = publaw_match.group(2)
            hierarchy['full_citation'] = f"Public Law {publaw_match.group(1)}-{publaw_match.group(2)}"
            return hierarchy
        
        # State Code í™•ì¸: California Business and Professions Code Section 22975
        state_match = re.search(r'(california|florida|texas|new\s+york).*?(?:code|law).*?section\s+(\d+(?:\.\d+)?)', text, re.IGNORECASE)
        if state_match:
            hierarchy['regulation_type'] = 'State Law'
            hierarchy['state'] = state_match.group(1).title()
            hierarchy['section'] = state_match.group(2)
            return hierarchy
        
        return None
    
    def _extract_external_id(self, filename: str) -> Optional[str]:
        """ì™¸ë¶€ ë¬¸ì„œ ID ì¶”ì¶œ (ì˜ˆ: 2025-00397)."""
        match = re.search(r'(\d{4})-(\d{5})', filename)
        return match.group(0) if match else None
    
    def _calculate_document_hash(self, text: str) -> str:
        """ë¬¸ì„œ í•´ì‹œ ê³„ì‚° (SHA256)."""
        import hashlib
        return hashlib.sha256(text.encode('utf-8')).hexdigest()[:16]
    
    def _calculate_confidence(self, metadata: Dict[str, Any]) -> float:
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹ ë¢°ë„ ê³„ì‚° (0.0~1.0)."""
        score = 0.0
        max_score = 5.0
        
        # ì œëª©ì´ í™•ì¸ë˜ì—ˆëŠ”ê°€?
        if metadata["title"] != "ì œëª© ë¯¸í™•ì¸":
            score += 1.0
        
        # êµ­ê°€ê°€ í™•ì¸ë˜ì—ˆëŠ”ê°€?
        if metadata["country"] != "UNKNOWN":
            score += 1.0
        
        # ê·œì œê¸°ê´€ì´ í™•ì¸ë˜ì—ˆëŠ”ê°€?
        if metadata["regulatory_body"]:
            score += 1.0
        
        # ë°œí‘œ ë‚ ì§œê°€ í™•ì¸ë˜ì—ˆëŠ”ê°€?
        if metadata["publication_date"]:
            score += 1.0
        
        # í‚¤ì›Œë“œê°€ í™•ì¸ë˜ì—ˆëŠ”ê°€?
        if metadata["keywords"]:
            score += 1.0
        
        return min(score / max_score, 1.0)
    
    # ==================== LangChain í†µí•© ë©”ì„œë“œ ====================
    
    def extract_from_file(
        self, 
        file_path: Union[str, Path], 
        use_langchain: Optional[bool] = None
    ) -> Dict[str, Any]:
        """
        íŒŒì¼ì—ì„œ ì§ì ‘ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (LangChain + íŒ¨í„´ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ).
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            use_langchain: LangChain ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ ì¸ìŠ¤í„´ìŠ¤ ì„¤ì • ë”°ë¦„)
        
        Returns:
            Dict[str, Any]: ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        use_lc = use_langchain if use_langchain is not None else self.use_langchain
        
        if use_lc and LANGCHAIN_AVAILABLE:
            return self._extract_with_langchain(file_path)
        else:
            return self._extract_with_existing_processor(file_path)
    
    def _extract_with_langchain(self, file_path: Path) -> Dict[str, Any]:
        """
        LangChain DocumentLoader í‘œì¤€í™” ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ.
        
        í•µì‹¬ ê°œì„ :
        1. Document êµ¬ì¡° í‘œì¤€í™” (page_content + metadata)
        2. ë„ë©”ì¸ ë©”íƒ€ë°ì´í„° ìë™ ì¶”ê°€
        3. ë©”íƒ€ë°ì´í„° ì „íŒŒ ë³´ì¥
        4. ê¸°ì¡´ pdf_processor í™œìš©
        """
        logger.info(f"ğŸ” DocumentLoader í‘œì¤€í™” ì¶”ì¶œ: {file_path.name}")
        
        try:
            # 1. LangChain ë¡œë”ë¡œ Document êµ¬ì¡° ìƒì„±
            loader = self._get_langchain_loader(file_path)
            documents = loader.load()
            
            if not documents:
                raise ValueError("No documents loaded")
            
            # 2. ë„ë©”ì¸ ë©”íƒ€ë°ì´í„° í‘œì¤€í™”
            standardized_docs = self._standardize_documents(documents, file_path)
            
            # 3. ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•© (ë©”íƒ€ë°ì´í„° ë³´ì¡´)
            full_text = "\n\n".join([doc.page_content for doc in standardized_docs])
            
            # 4. íŒ¨í„´ ê¸°ë°˜ ì¶”ì¶œ + Document ë©”íƒ€ë°ì´í„° ê²°í•©
            pattern_metadata = self.extract_metadata(
                document_text=full_text,
                filename=file_path.name,
                source_url=standardized_docs[0].metadata.get('source')
            )
            
            # 5. Document ë©”íƒ€ë°ì´í„°ì™€ íŒ¨í„´ ë©”íƒ€ë°ì´í„° í†µí•©
            final_metadata = self._merge_document_metadata(
                documents=standardized_docs,
                pattern_meta=pattern_metadata
            )
            
            logger.info(
                f"âœ… í‘œì¤€í™” ì¶”ì¶œ ì™„ë£Œ: {len(documents)}í˜ì´ì§€, "
                f"confidence={final_metadata['confidence']:.2f}"
            )
            
            return final_metadata
            
        except Exception as e:
            logger.error(f"âŒ DocumentLoader ì‹¤íŒ¨, ê¸°ì¡´ processor ì‚¬ìš©: {e}")
            return self._extract_with_existing_processor(file_path)
    
    def _get_langchain_loader(self, file_path: Path):
        """
        íŒŒì¼ í™•ì¥ìë³„ ìµœì  LangChain ë¡œë” ì„ íƒ.
        
        ë…¼ë¦¬:
        - PDF: PyPDFLoader (êµ¬ì¡° ë³´ì¡´) vs UnstructuredPDFLoader (í…ìŠ¤íŠ¸ í’ˆì§ˆ)
        - DOCX: UnstructuredWordDocumentLoader
        - TXT: TextLoader
        - ê¸°íƒ€: í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
        """
        suffix = file_path.suffix.lower()
        
        if suffix == '.pdf':
            # PDF: êµ¬ì¡°í™”ëœ ì¶”ì¶œ ìš°ì„ 
            try:
                return UnstructuredPDFLoader(str(file_path))
            except:
                return PyPDFLoader(str(file_path))
        
        elif suffix in ['.docx', '.doc']:
            return UnstructuredWordDocumentLoader(str(file_path))
        
        elif suffix in ['.txt', '.md']:
            return TextLoader(str(file_path), encoding='utf-8')
        
        else:
            # ê¸°íƒ€ íŒŒì¼: í…ìŠ¤íŠ¸ë¡œ ì‹œë„
            logger.warning(f"Unknown file type: {suffix}, using TextLoader")
            return TextLoader(str(file_path), encoding='utf-8')
    
    def _extract_langchain_metadata(self, documents: List[Document]) -> Dict[str, Any]:
        """
        LangChain Document ê°ì²´ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ.
        
        LangChainì˜ ì¥ì :
        - íŒŒì¼ ì‹œìŠ¤í…œ ë©”íƒ€ë°ì´í„° (ìƒì„±ì¼, ìˆ˜ì •ì¼, í¬ê¸°)
        - ë¬¸ì„œ êµ¬ì¡° ì •ë³´ (í˜ì´ì§€ ìˆ˜, ì„¹ì…˜)
        - ìë™ ì–¸ì–´ ê°ì§€
        """
        if not documents:
            return {}
        
        primary_doc = documents[0]
        metadata = primary_doc.metadata.copy()
        
        # LangChain ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì •ê·œí™”
        langchain_meta = {
            'langchain_source': metadata.get('source'),
            'langchain_page_count': len(documents),
            'langchain_total_chars': sum(len(doc.page_content) for doc in documents),
            'langchain_avg_page_length': sum(len(doc.page_content) for doc in documents) // len(documents),
        }
        
        # íŒŒì¼ ì‹œìŠ¤í…œ ë©”íƒ€ë°ì´í„° (ìˆìœ¼ë©´)
        if 'file_path' in metadata:
            file_path = Path(metadata['file_path'])
            if file_path.exists():
                stat = file_path.stat()
                langchain_meta.update({
                    'file_size_bytes': stat.st_size,
                    'file_created_at': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                    'file_modified_at': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                })
        
        # PDF íŠ¹í™” ë©”íƒ€ë°ì´í„°
        if 'page' in metadata:
            langchain_meta['pdf_page_number'] = metadata['page']
        
        return langchain_meta
    
    def _merge_document_metadata(
        self,
        documents: List[Document],
        pattern_meta: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Document ë©”íƒ€ë°ì´í„°ì™€ íŒ¨í„´ ê¸°ë°˜ ë©”íƒ€ë°ì´í„° í†µí•©.
        
        í•µì‹¬: Document êµ¬ì¡°ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬
        Qdrant ì €ì¥ ì‹œ í•„í„°ë§/ê²€ìƒ‰ì— ìµœì í™”ëœ ë©”íƒ€ë°ì´í„° ìƒì„±
        """
        # íŒ¨í„´ ê¸°ë°˜ì„ ê¸°ë³¸ìœ¼ë¡œ ì‹œì‘
        merged = pattern_meta.copy()
        
        # Document ë©”íƒ€ë°ì´í„° í†µí•©
        if documents:
            primary_doc = documents[0]
            doc_meta = primary_doc.metadata
            
            merged.update({
                'extraction_method': 'document_loader_standardized',
                'page_count': len(documents),
                'source_file': doc_meta.get('source_file'),
                
                # ë„ë©”ì¸ ë©”íƒ€ë°ì´í„° ìš°ì„  ì ìš© (Documentì—ì„œ ìë™ ê°ì§€ëœ ê²ƒ)
                'jurisdiction': doc_meta.get('jurisdiction', merged.get('jurisdiction')),
                'agency': doc_meta.get('agency', merged.get('regulatory_body')),
                'regulation_type': doc_meta.get('regulation_type', merged.get('regulation_type')),
                
                # Qdrant í•„í„°ë§ìš© ë©”íƒ€ë°ì´í„°
                'meta_source_type': 'document_loader',
                'meta_page_count': len(documents),
                'meta_extraction_method': 'langchain_standardized',
            })
            
            # ì œëª© ê°œì„  (Document source í™œìš©)
            if merged['title'] == 'ì œëª© ë¯¸í™•ì¸' and doc_meta.get('source_file'):
                source_name = Path(doc_meta['source_file']).stem
                if len(source_name) > 5:
                    merged['title'] = source_name.replace('_', ' ').title()
            
            # ì‹ ë¢°ë„ ì¬ê³„ì‚° (Document êµ¬ì¡° ì •ë³´ ë°˜ì˜)
            confidence_boost = 0.0
            if len(documents) > 1:
                confidence_boost += 0.1  # ë‹¤ì¤‘ í˜ì´ì§€
            if doc_meta.get('jurisdiction'):
                confidence_boost += 0.1  # ìë™ ê°ì§€ëœ ê´€í• ê¶Œ
            if doc_meta.get('agency'):
                confidence_boost += 0.1  # ìë™ ê°ì§€ëœ ê¸°ê´€
            
            merged['confidence'] = min(merged['confidence'] + confidence_boost, 1.0)
        
        return merged
    
    def _extract_with_existing_processor(self, file_path: Path) -> Dict[str, Any]:
        """
        ê¸°ì¡´ pdf_processor.py í™œìš©í•œ ì•ˆì •ì  ì¶”ì¶œ.
        """
        logger.info(f"ğŸ“„ ê¸°ì¡´ processor í™œìš©: {file_path.name}")
        
        try:
            if file_path.suffix.lower() == '.pdf':
                # ê¸°ì¡´ PDFProcessor í™œìš©
                from app.ai_pipeline.preprocess.pdf_processor import PDFProcessor
                
                processor = PDFProcessor()
                pdf_result = processor.load_and_extract(str(file_path))
                
                if pdf_result["status"] == "success":
                    text = pdf_result["full_text"]
                    
                    # íŒ¨í„´ ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    metadata = self.extract_metadata(
                        document_text=text,
                        filename=file_path.name
                    )
                    
                    # PDFProcessor ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    metadata.update({
                        'extraction_method': 'existing_pdf_processor',
                        'pdf_metadata': pdf_result.get('metadata', {}),
                        'page_count': pdf_result.get('metadata', {}).get('num_pages', 0)
                    })
                    
                    return metadata
                else:
                    raise RuntimeError(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {pdf_result.get('error')}")
            else:
                # í…ìŠ¤íŠ¸ íŒŒì¼
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                metadata = self.extract_metadata(
                    document_text=text,
                    filename=file_path.name
                )
                metadata['extraction_method'] = 'text_file'
                return metadata
                
        except Exception as e:
            logger.error(f"âŒ ê¸°ì¡´ processor ì‹¤íŒ¨: {e}")
            raise
    
    def _standardize_documents(self, documents: List[Document], file_path: Path) -> List[Document]:
        """
        Document êµ¬ì¡° í‘œì¤€í™” ë° ë„ë©”ì¸ ë©”íƒ€ë°ì´í„° ì¶”ê°€.
        
        í•µì‹¬: ê·œì œ ë„ë©”ì¸ íŠ¹í™” ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë”© ì§í›„ ì¶”ê°€í•˜ì—¬
        ì²­í‚¹/ì„ë² ë”©/ê²€ìƒ‰ ë‹¨ê³„ì—ì„œ ë©”íƒ€ë°ì´í„°ê°€ ì „íŒŒë˜ë„ë¡ ë³´ì¥
        """
        standardized = []
        
        for idx, doc in enumerate(documents):
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ë³´ê°•
            enhanced_metadata = doc.metadata.copy()
            enhanced_metadata.update({
                # íŒŒì¼ ì •ë³´
                'source_file': file_path.name,
                'source_path': str(file_path),
                'page_number': enhanced_metadata.get('page', idx + 1),
                
                # ë„ë©”ì¸ ë©”íƒ€ë°ì´í„° (ê·œì œ íŠ¹í™”)
                'document_type': 'regulation',
                'extraction_timestamp': datetime.utcnow().isoformat() + 'Z',
                
                # ì²­í‚¹/ê²€ìƒ‰ìš© ë©”íƒ€ë°ì´í„°
                'chunk_source': 'document_loader',
                'parent_document': file_path.stem,
            })
            
            # ê·œì œ ë„ë©”ì¸ ë©”íƒ€ë°ì´í„° ìë™ ê°ì§€
            domain_meta = self._extract_domain_metadata_from_content(doc.page_content)
            enhanced_metadata.update(domain_meta)
            
            # ìƒˆ Document ìƒì„± (ë©”íƒ€ë°ì´í„° ì „íŒŒ ë³´ì¥)
            standardized_doc = Document(
                page_content=doc.page_content,
                metadata=enhanced_metadata
            )
            standardized.append(standardized_doc)
        
        return standardized
    
    def _extract_domain_metadata_from_content(self, content: str) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ ë‚´ìš©ì—ì„œ ê·œì œ ë„ë©”ì¸ ë©”íƒ€ë°ì´í„° ìë™ ê°ì§€.
        """
        domain_meta = {}
        content_lower = content.lower()
        
        # ê´€í• ê¶Œ ìë™ ê°ì§€
        if any(pattern in content_lower for pattern in ['congress', 'federal', 'u.s.c']):
            domain_meta['jurisdiction'] = 'federal'
        elif any(pattern in content_lower for pattern in ['california', 'state', 'division']):
            domain_meta['jurisdiction'] = 'state'
        elif any(pattern in content_lower for pattern in ['city', 'county', 'municipal']):
            domain_meta['jurisdiction'] = 'local'
        
        # ê·œì œ ê¸°ê´€ ìë™ ê°ì§€
        if 'fda' in content_lower or 'food and drug' in content_lower:
            domain_meta['agency'] = 'FDA'
        elif 'state board' in content_lower:
            domain_meta['agency'] = 'State Board'
        
        # ê·œì œ íƒ€ì… ìë™ ê°ì§€
        if any(kw in content_lower for kw in ['tobacco', 'cigarette', 'nicotine']):
            domain_meta['regulation_type'] = 'tobacco_control'
        
        # ì¡°í•­ ID ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
        section_match = re.search(r'section\s+(\d+)', content_lower)
        if section_match:
            domain_meta['clause_id'] = f"sec_{section_match.group(1)}"
        
        return domain_meta
    
    def batch_extract_metadata(
        self,
        documents: List[Dict[str, str]],
        show_progress: bool = True
    ) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ë°°ì¹˜ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            documents: [{"text": "...", "filename": "..."}, ...]
            show_progress: ì§„í–‰ ìƒí™© ë¡œê¹… ì—¬ë¶€
        
        Returns:
            List[Dict[str, Any]]: ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        results = []
        total = len(documents)
        
        for i, doc in enumerate(documents, 1):
            try:
                metadata = self.extract_metadata(
                    document_text=doc.get("text", ""),
                    filename=doc.get("filename"),
                    source_url=doc.get("source_url")
                )
                results.append(metadata)
                
                if show_progress:
                    logger.info(f"[{i}/{total}] âœ… Extracted: {metadata['title'][:40]}")
            
            except Exception as e:
                logger.error(f"[{i}/{total}] âŒ Error: {str(e)}")
                results.append({"error": str(e), "filename": doc.get("filename")})
        
        logger.info(f"âœ… Batch extraction complete: {total} documents processed")
        return results


# ==================== í…ŒìŠ¤íŠ¸ í—¬í¼ í•¨ìˆ˜ ====================

def demo_extract():
    """ë°ëª¨ìš© ë©”íƒ€ë°ì´í„° ì¶”ì¶œ."""
    extractor = MetadataExtractor()
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ (ë¯¸êµ­ ë‹´ë°° ê·œì œ)
    test_doc = """
    PUBLIC LAW 111â€“31â€”JUNE 22, 2009
    
    FAMILY SMOKING PREVENTION AND TOBACCO CONTROL
    AND FEDERAL RETIREMENT REFORM
    
    An Act
    To protect the public health by providing the Food and Drug Administration with 
    certain authority to regulate tobacco products...
    
    Be it enacted by the Senate and House of Representatives of 
    the United States of America in Congress assembled,
    """
    
    metadata = extractor.extract_metadata(
        document_text=test_doc,
        filename="family_smoking_prevention_act.txt",
        source_url="https://example.com/law"
    )
    
    print(json.dumps(metadata, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    demo_extract()
