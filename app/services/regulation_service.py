"""
module: regulation_service.py
description: ê·œì œ ë¬¸ì„œ ì¡°íšŒ ë° í¬ë¡¤ë§ ë°ì´í„° ì²˜ë¦¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
author: ì¡°ì˜ìš°
created: 2025-11-12
updated: 2025-11-27
dependencies:
    - sqlalchemy.ext.asyncio
    - aiofiles
    - bs4
    - core.repositories.regulation_repository
    - core.models.regulation_model
"""

import os
import aiofiles
from bs4 import BeautifulSoup
from datetime import datetime

from sqlalchemy.ext.asyncio import AsyncSession
from app.core.repositories.regulation_keynote_repository import RegulationKeynoteRepository

# logger = logging.getLogger(__name__)
from sqlalchemy import select, desc

from app.config.logger import logger
from app.core.repositories.regulation_keynote_repository import RegulationKeynoteRepository
from app.core.models.regulation_model import Regulation, RegulationVersion, RegulationChangeHistory
from app.ai_pipeline.preprocess.preprocess_agent import PreprocessAgent

class RegulationService:
    """ê·œì œ ë¬¸ì„œ ê´€ë ¨ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§(ì¡°íšŒ ë° í¬ë¡¤ë§ ì²˜ë¦¬)ì„ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self, db: AsyncSession = None):
        """
        Args:
            db (AsyncSession, optional): í¬ë¡¤ë§ ë¡œì§ì—ì„œ ì‚¬ìš©ë˜ëŠ” DB ì„¸ì…˜. 
                                         ë‹¨ìˆœ ì¡°íšŒ(get_regulations) ì‹œì—ëŠ” í•„ìš”í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ.
        """
        # [ê¸°ì¡´] ì¡°íšŒìš© ë¦¬í¬ì§€í† ë¦¬
        self.repo = RegulationKeynoteRepository()

        # [ì¶”ê°€] í¬ë¡¤ë§ ë° ì „ì²˜ë¦¬ìš© ì„¤ì •
        self.db = db
        self.preprocess_agent = PreprocessAgent()
        self.save_dir = os.path.join("db", "regulation")
        os.makedirs(self.save_dir, exist_ok=True)

    # ==========================================
    # [ê¸°ì¡´ ê¸°ëŠ¥] ê·œì œ ë¬¸ì„œ ì¡°íšŒ ë¡œì§
    # ==========================================
    async def get_regulations(self, db: AsyncSession) -> dict:
        """
        ê·œì œ ë¬¸ì„œ ëª©ë¡ì„ ì¡°íšŒí•œë‹¤.

        Args:
            db (AsyncSession): ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜.

        Returns:
            dict: ê·œì œ ë¬¸ì„œ ëª©ë¡ (í”„ë¡ íŠ¸ í˜•ì‹).
        """
        
        # risk_level í•œê¸€ ë³€í™˜ ë§µ
        RISK_LEVEL_MAP = {
            "Low": "ë‚®ìŒ",
            "Medium": "ë³´í†µ",
            "High": "ë†’ìŒ"
        }
        
        try:
            # keynoteì™€ impact_scoreë¥¼ í¬í•¨í•˜ì—¬ ì¡°íšŒ
            regulations = await self.repo.get_all_keynotes(db)
            result = []
            for keynote in regulations:
                # keynote_textëŠ” ["country: US", "category: demo", ...] í˜•íƒœ
                keynote_data = {}
                for item in keynote.keynote_text:
                    if ": " in item:
                        key, value = item.split(": ", 1)
                        keynote_data[key] = value
                
                # í”„ë¡ íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                result.append({
                    "id": keynote.keynote_id,
                    "country": keynote_data.get("country", ""),
                    "category": keynote_data.get("category", ""),
                    "summary": keynote_data.get("summary", ""),
                    "impact": RISK_LEVEL_MAP.get(keynote_data.get("impact", ""), keynote_data.get("impact", ""))
                })
            
            logger.info(f"Found {len(result)} regulations")
            return {
                "today_count": len(result),
                "regulations": result #dbì—ì„œ ê°€ì ¸ì˜¨ json êµ¬ì¡°?
            }
            
        except Exception as e:
            logger.error(f"Error fetching regulations: {e}", exc_info=True)
            # ì—ëŸ¬ ë°œìƒí•´ë„ ë¹ˆ ë°°ì—´ ë°˜í™˜
            return {
                "today_count": 0,
                "regulations": []
            }

    # ==========================================
    # [ì¶”ê°€ ê¸°ëŠ¥] í¬ë¡¤ë§ ë°ì´í„° ì²˜ë¦¬ ë¡œì§
    # ==========================================
    async def process_crawled_data(self, data: dict, crawler=None):
        if not self.db:
            logger.error("DB session is not initialized for crawling process.")
            return "error: no_db_session"

        url = data["url"]
        
        stmt = (
            select(Regulation)
            .join(RegulationVersion, Regulation.regulation_id == RegulationVersion.regulation_id)
            .where(RegulationVersion.original_uri == url)
            .limit(1)
        )
        result = await self.db.execute(stmt)
        existing_reg = result.scalar_one_or_none()

        if not existing_reg:
            return await self._create_new_regulation(data, crawler)
        else:
            return await self._handle_existing_regulation(existing_reg, data, crawler)

    async def _save_file_locally(self, url: str, hash_value: str, crawler) -> str:
        if not crawler:
            return None

        # 1. PDFì¸ ê²½ìš°: ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ ë°”ì´ë„ˆë¦¬ ì €ì¥
        if url.lower().endswith(".pdf"):
            filename = f"{hash_value}.pdf"
            file_path = os.path.join(self.save_dir, filename)
            
            if os.path.exists(file_path):
                return file_path

            content = await crawler.fetch_binary(url)
            if content:
                async with aiofiles.open(file_path, "wb") as f:
                    await f.write(content)
                print(f"ğŸ’¾ PDF ì €ì¥ ì™„ë£Œ: {file_path}")
                return file_path
        
        # 2. HTML(ì›¹í˜ì´ì§€)ì¸ ê²½ìš°: í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ .txtë¡œ ì €ì¥
        else:
            filename = f"{hash_value}.txt" # í™•ì¥ìë¥¼ .txtë¡œ ë³€ê²½
            file_path = os.path.join(self.save_dir, filename)

            if os.path.exists(file_path):
                return file_path

            # fetch()ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸(HTML) ê°€ì ¸ì˜¤ê¸°
            html_content = await crawler.fetch(url)
            if html_content:
                # BeautifulSoupìœ¼ë¡œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                soup = BeautifulSoup(html_content, "lxml")
                
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ë„¤ë¹„ê²Œì´ì…˜ ë“±)
                for script in soup(["script", "style", "header", "footer", "nav", "iframe"]):
                    script.extract()
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê³µë°± ì •ë¦¬)
                clean_text = soup.get_text(separator="\n", strip=True)

                # .txt íŒŒì¼ë¡œ ì €ì¥
                async with aiofiles.open(file_path, "w", encoding="utf-8") as f:
                    await f.write(clean_text)
                
                print(f"ğŸ’¾ í…ìŠ¤íŠ¸ ë³€í™˜ ë° ì €ì¥ ì™„ë£Œ: {file_path}")
                return file_path

        return None

    async def _create_new_regulation(self, data: dict, crawler):
        file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

        new_reg = Regulation(
            source_id=1,
            country_code=data["country_code"],
            title=data["title"],
            proclaimed_date=datetime.strptime(data["proclaimed_date"], "%Y-%m-%d").date() if data.get("proclaimed_date") else None,
            status="active"
        )
        self.db.add(new_reg)
        await self.db.flush()

        new_version = RegulationVersion(
            regulation_id=new_reg.regulation_id,
            version_number=1,
            original_uri=data["url"],
            hash_value=data["hash_value"]
        )
        self.db.add(new_version)
        
        history = RegulationChangeHistory(
            version=new_version,
            change_type="new", 
            change_summary="ìµœì´ˆ ìˆ˜ì§‘ë¨"
        )
        self.db.add(history)
        
        await self.db.commit()
        print(f"âœ¨ [New] ì‹ ê·œ ê·œì œ ë“±ë¡: {data['title'][:30]}...")

        if file_path:
            await self.preprocess_agent.run(file_path, data)
            
        return "created"

    async def _handle_existing_regulation(self, regulation: Regulation, data: dict, crawler):
        stmt = select(RegulationVersion).where(
            RegulationVersion.regulation_id == regulation.regulation_id
        ).order_by(desc(RegulationVersion.version_number)).limit(1)
        
        result = await self.db.execute(stmt)
        latest_version = result.scalar_one_or_none()

        if latest_version and latest_version.hash_value == data["hash_value"]:
            return "skipped"

        print(f"ğŸ”„ ë³€ê²½ ê°ì§€ë¨! íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        file_path = await self._save_file_locally(data["url"], data["hash_value"], crawler)

        new_v_num = latest_version.version_number + 1 if latest_version else 1
        
        new_version = RegulationVersion(
            regulation_id=regulation.regulation_id,
            version_number=new_v_num,
            original_uri=data["url"],
            hash_value=data["hash_value"]
        )
        self.db.add(new_version)

        history = RegulationChangeHistory(
            version=new_version,
            change_type="append",
            change_summary=f"ë²„ì „ {new_v_num}ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨"
        )
        self.db.add(history)
        
        await self.db.commit()
        print(f"ğŸ”„ [Update] ê·œì œ ì—…ë°ì´íŠ¸ ì™„ë£Œ (v{new_v_num})")

        if file_path:
            await self.preprocess_agent.run(file_path, data)

        return "updated"